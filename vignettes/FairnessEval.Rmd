---
title: "Binary Protected Attributes"
authors:
  - name: "Jianhui Gao"
    orcid: "0000-0003-0915-1473"
    affiliation: 1
  - name: "Benjamin Smith"
    orcid: "0009-0007-2206-0177"
    affiliation: 1
  - name: "Benson Chou"
    orcid: "0009-0007-0265-033X"
    affiliation: 1
  - name: "Jessica Gronsbell"
    orcid: "0000-0002-5360-5869"
    affiliation: 1
affiliations:
  - name: "University of Toronto"
    index: 1
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Binary Protected Attributes}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Introduction

We illustrate the usage of the `FairnessEval` package through a case study using a publicly available dataset of 1,776 ICU patients from the MIMIC-II clinical database, focusing on predicting 28-day mortality and evaluating disparities in model performance across sex.

The following packages are used for the analysis along with the `FairnessEval` package:

```{r setup, message=FALSE, warning=FALSE}
# Packages we are using for the analysis
library(dplyr)
library(corrplot)
library(randomForest)
library(pROC)
library(SpecsVerification)
library(kableExtra)
library(naniar)
# Our package
library(FairnessEval)
```

The "Data Preprocessing" section discusses the dataset, the handling of missing data, model construction and standard predictive model evaluation through train-test splitting for binary classification. The "Fairness Evaluation" section shows how to evaluate the model's fairness toward binary protected attributes with the `FairnessEval` package. Finally, the “Practical Considerations” section discusses the inherent trade-offs between different fairness definitions in clinical settings and highlights the challenges of enforcing fairness when outcome prevalence differs across groups. Details regarding the construction of 95% confidence intervals are provided in the appendix.

# Data Preprocessing

The dataset used in this analysis is the MIMIC II clinical database, which has been previously studied to explore the relationship between indwelling arterial catheters in hemodynamically stable patients and respiratory failure in relation to mortality outcomes. It includes 46 variables which cover demographics and clinical characteristics (including white blood cell count, heart rate during ICU stays and others) along with a 28-day mortality indicator (`day_28_flg`) for 1,776 patients. The data has been made publicly available by [PhysioNet](https://physionet.org/content/mimic2-iaccd/1.0/) and is available in the `FairnessEval` package as the `mimic` dataset.

## Handling Missing Data

We first assess the extent of missingness in the dataset. For each variable, we calculate both the total number and the percentage of missing values with `naniar::miss_var_summary()`.

```{r}
# Loading mimic dataset 
# (available in FairnessEval)
data("mimic") 

missing_data_summary<- naniar::miss_var_summary(mimic)

kableExtra::kable(missing_data_summary, booktabs = TRUE, escape = FALSE) %>%
  kableExtra::kable_styling(
    latex_options = "hold_position"
  )

```

To ensure data quality, the following procedure is applied to handle missing data:

-   Removal of that variables which had more than 10% missing values. Three variables had more than 10% of missing values: body mass index (`bmi`; 26.2%), first partial pressure of oxygen (`po2_first`; 10.5%), and first partial pressure of carbon dioxide (`pco2_first`; 10.5%).

-   Remaining missing values are imputed using the median value of the variable which they belong to.

```{r}
# Remove columns with more than 10% missing values
columns_to_remove <- missing_data_summary %>%
  dplyr::filter(pct_miss > 10) %>%
  dplyr::pull(variable)
  
mimic <- dplyr::select(mimic, 
                       -dplyr::one_of(columns_to_remove)
                       )

# Impute remaining missing values with median
mimic <- mimic %>% 
  dplyr::mutate(
    dplyr::across(
      dplyr::where(~any(is.na(.))), 
                  ~ifelse(is.na(.), median(., na.rm = TRUE), .)
                  )
    )
```

We additionally remove `sepsis_flg` column from the dataset as it contains a single unique value across all observations. Since this does not provide any useful information for model training, it is excluded.

```{r}
# Identify columns that have only one unique value
cols_with_one_value <- sapply(mimic, function(x) length(unique(x)) == 1)
# Subset the dataframe to remove these columns
mimic <- mimic[, !cols_with_one_value]
```

## Model Building

Before training the model, we further remove variables that are directly correlated with patient outcomes to prevent [data leakage](https://en.wikipedia.org/wiki/Leakage_%28machine_learning%29). In particular, we inspect the correlation matrix of the numeric features and exclude variables such as the hospital expiration flag (`hosp_exp_flg`), the ICU expiration flag (`icu_exp_flg`), the mortality censoring day (`mort_day_censored`), and the censoring flag (`censor_flg`), which are strongly associated patient outcomes.

```{r}
# Remove columns that are highly correlated with the outcome variable
corrplot::corrplot(cor(select_if(mimic, is.numeric)), method = "color", tl.cex = 0.5)

mimic <- mimic %>% 
  dplyr::select(-c("hosp_exp_flg", "icu_exp_flg", "mort_day_censored", "censor_flg"))
```

We split the dataset into a training and testing sets. The first 700 patients are used as the training set and the remaining patients are used as the testing set. The hyperparameters for the random forest (RF) model are set to use 1000 trees and a random sampling of 6 variables at each split, determined by the square root of the number of predictors. After training, the overall area under the receiver operating characteristic curve (AUC) for the model on the test set is 0.90 and the overall accuracy of the model on the test set is 0.88.

```{r}
# Use 700 labels to train the mimic
train_data <- mimic %>% 
  dplyr::filter(
    dplyr::row_number() <= 700
    )

# Fit a random forest model
set.seed(123)
rf_model <- randomForest::randomForest(factor(day_28_flg) ~ ., data = train_data, ntree = 1000)

# Test the model on the remaining data
test_data <- mimic %>% 
  dplyr::filter(
    dplyr::row_number() > 700
    )

test_data$pred <- predict(rf_model, newdata = test_data, type = "prob")[,2]

# Check the AUC
roc_obj <- pROC::roc(test_data$day_28_flg, test_data$pred)
roc_auc <- pROC::auc(roc_obj)
roc_auc
```

# Fairness Evaluation

For fairness evaluation, use the results of the testing set and focus on patient gender as the binary protected attribute and its relationship with 28-day mortality (`day_28_flg`) as the outcome.

```{r}
# Recode gender variable explicitly for readability: 

test_data <- test_data %>%
  dplyr::mutate(gender = ifelse(gender_num == 1, "Male", "Female"))
```

Since many fairness metrics require binary predictions, we threshold the predicted probabilities using a fixed cutoff. We set a threshold of 0.41 to maintain the overall false positive rate (FPR) at approximately 5%

```{r}
# control the overall false positive rate (FPR) at 5% by setting a threshold.

cut_off <- 0.41

test_data %>%
  dplyr::mutate(pred = ifelse(pred > cut_off, 1, 0)) %>%
  dplyr::filter(day_28_flg == 0) %>%
  dplyr::summarise(fpr = mean(pred))
```

To calculate various fairness metrics for the model, we pass our test data with its predicted results into the `get_fairness_metrics` function.

```{r}
fairness_result <- FairnessEval::get_fairness_metrics(
  data = test_data,
  outcome = "day_28_flg",
  group = "gender",
  group2 = "age",
  condition = ">=60",
  probs = "pred",
  cutoff = cut_off
 )

kableExtra::kable(fairness_result, booktabs = TRUE, escape = FALSE) %>%
  kableExtra::kable_styling(full_width = FALSE) %>%
  kableExtra::pack_rows("Independence-based criteria", 1, 2) %>%
  kableExtra::pack_rows("Separation-based criteria", 3, 6) %>%
  kableExtra::pack_rows("Sufficiency-based criteria", 7, 7) %>%
  kableExtra::pack_rows("Other criteria", 8, 10) %>%
  kableExtra::kable_styling(
    full_width = FALSE,
    font_size = 10,         # control font size manually
    latex_options = "hold_position"
  )
```

From the outputted fairness metrics we note:

-   Independence is likely violated, as evidenced by the statistical parity metric which shows a 9% difference (95% CI: [5%, 13%]) or a ratio of 2.12 (95% CI: [1.49, 3.04]) between females and males. The measures in the independence category indicate that the model predicts a significantly higher mortality rate for females, even after conditioning on age.

-   With respect to separation, we observe that all metrics show significant disparities. For instance, equal opportunity shows a -24% difference (95% CI: [-39%, -9%]) in false negative rate (FNR) between females and males. This indicates our model is less likely to detect males at risk of mortality compared to females.

-   On the other hand, the sufficiency criterion is satisfied as predictive parity shows no significant difference between males and females (difference: -4%, 95% CI: [-21%, 13%]; ratio: 0.94, 95% CI: [0.72, 1.23]). This suggests that given the same prediction, the actual mortality rates are similar for both males and females.

-   Among the additional metrics that assess calibration and discrimination, Brier Score Parity and Overall Accuracy Equality do not show significant disparities. However, Treatment Equality shows a statistically significant difference (difference: –2.21, 95% CI: [–4.35, –0.07]; ratio: 0.32, 95% CI: [0.15, 0.68]), indicating that males have a substantially higher false negative to false positive ratio compared to females. This suggests that male patients are more likely to be missed by the model relative to being incorrectly flagged.

# Practical Considerations

In the above example, both separation and sufficiency-based metrics are important. Separation ensures that patients at a true risk of mortality are accurately identified across protected groups. This reduces the risk of systematically under-detecting individuals who may require immediate intervention in a particular subpopulation. On the other hand, sufficiency is also valuable, as it ensures immediate intervention is based solely on predicted medical needs rather than other protected attributes.

However, 28-day-mortality is unlikely to be marginally independent of sex as the estimated mortality rates differ between females (19%) and males (14%). As a consequence, it is not possible for the model to simultaneously satisfy more than one fairness category due to the previously discussed incompatibilities. This points to the complexity of fairness considerations in clinical settings, where one must prioritize which criteria are most relevant. Given the different mortality rates between males and females, enforcing independence is likely not advisable, as it could blind the model to true mortality rate differences. The model used in this example violates separation criteria, which could result in higher rates of undetected mortality risk among male patients and potential delays in interventions to their care. In the main text, we refer user to a recent [survey](https://arxiv.org/abs/2207.07068) for bias mitigation strategies that can potentially help reduce the observed disparities in separation-based metrics.

# Appendix

We illustrate the procedure for constructing confidence intervals (CIs) using the false positive rate (FPR) as an example. Let $\widehat{\textrm{FPR}}_a$ and $\textrm{FPR}_a$ represent the estimated and true FPRs in group $A = a$. We construct Wald-type CIs using the following result: $\sqrt{n} \left(\widehat{\textrm{FPR}}_a - \textrm{FPR}_a \right)$ is asymptotic normal with mean 0 in each $A = a$ (e.g., [Gronsbell et al., 2018](https://academic.oup.com/jrsssb/article/80/3/579/7048446)). Let $\widehat{\Delta}_{\textrm{FPR}} = \widehat{\textrm{FPR}}_{a_1} - \widehat{\textrm{FPR}}_{a_0}$ represent the estimated difference between group $a_1$ and $a_0$. It follows that $\sqrt{n}\left(\widehat{\Delta}_{\textrm{FPR}} - \Delta_{\textrm{FPR}}\right)$ is asymptotic normal with mean 0. To estimate the standard error of $\widehat{\Delta}_{\textrm{FPR}}$, we utilize the non-parametric bootstrap ([Efron and Tibshirani, 1986](https://projecteuclid.org/journals/statistical-science/volume-1/issue-1/Bootstrap-Methods-for-Standard-Errors-Confidence-Intervals-and-Other-Measures/10.1214/ss/1177013815.full)) . For each bootstrap iteration $b$ in {1,..., B}, we sample the data with replacement within each group and then calculate $\widehat{\Delta}_{\textrm{FPR}}^{(b)}$. Note that resampling within groups assumes that the group sizes are fixed. The estimated standard error is the empirical standard error of the difference:

$$
\widehat{\textrm{se}}\left[\widehat{\Delta}_{\textrm{FPR}}\right] = \sqrt{\frac{1}{B-1}\sum_{b=1}^B \left(\widehat{\Delta}_{\textrm{FPR}}^{(b)} - \frac{1}{B} \sum_{b=1}^B \widehat{\Delta}_{\textrm{FPR}}^{(b)}\right)^2}.
$$ The corresponding Wald-based $100(1-\alpha)\%$ CI is given by $\widehat{\Delta}_{\textrm{FPR}} \pm z_{1-\alpha/2} * \widehat{\textrm{se}}\left[\widehat{\Delta}_{\textrm{FPR}}\right]$, where $z_{1-\alpha/2}$ is the $100(1-\alpha/2)$ percentile of the standard normal distribution.

To construct a $100(1-\alpha)\%$ Wald-type CI for the ratio $\widehat{\rho}_{\textrm{FPR}} = \frac{\widehat{\textrm{FPR}}_{a_1}}{\widehat{\textrm{FPR}}_{a_0}}$, we note that $\sqrt{n}\left[\log\left(\widehat{\rho}_{\textrm{FPR}}\right)- \log\left(\rho_{\textrm{FPR}}\right)\right]$ is asymptotic normal with mean 0. For each bootstrap iteration $b$ in {1,..., B}, we sample the data with replacement within each group and then calculate $\log\left[\widehat{\rho}_{\textrm{FPR}}^{(b)}\right]$. The estimated standard error of the log ratio is:

$$
\widehat{\textrm{se}}\left[\log\left(\widehat{\rho}_{\textrm{FPR}}\right)\right] = \sqrt{\frac{1}{B-1}\sum_{b=1}^B \left\{\log\left[\widehat{\rho}_{\textrm{FPR}}^{(b)}\right] - \frac{1}{B} \sum_{b=1}^B \log\left[\widehat{\rho}_{\textrm{FPR}}^{(b)}\right]\right\}^2}.
$$

The corresponding $100(1-\alpha)\%$ CI is given by $\exp \left\{\log\left[\widehat{\rho}_{\textrm{FPR}}\right] \pm z_{1-\alpha/2} * \widehat{\textrm{se}}\left[\log\left(\widehat{\rho}_{\textrm{FPR}}\right)\right]\right\}$, where $z_{1-\alpha/2}$ is the $100(1-\alpha/2)$ percentile of the standard normal distribution.
