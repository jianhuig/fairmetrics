[{"path":"/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 jianhuig Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":[]},{"path":"/articles/TwoGroup.html","id":"missing-data","dir":"Articles","previous_headings":"Data Preprocessing","what":"Missing Data","title":"Binary Protected Attributes","text":"","code":"# Calculate the number of missing values per column missing_values <- sapply(mimic, function(x) sum(is.na(x)))  # Calculate the percentage of missing values per column missing_values_percentage <- sapply(mimic, function(x) sum(is.na(x)) / length(x) * 100)  # Combine the results into a data frame for easy viewing missing_data_summary <- data.frame(Number_of_Missing_Values = missing_values,                                     Percentage = missing_values_percentage)  # Print the summary print(missing_data_summary) #>                    Number_of_Missing_Values  Percentage #> aline_flg                                 0  0.00000000 #> icu_los_day                               0  0.00000000 #> hospital_los_day                          0  0.00000000 #> age                                       0  0.00000000 #> gender_num                                1  0.05630631 #> weight_first                            110  6.19369369 #> bmi                                     466 26.23873874 #> sapsi_first                              85  4.78603604 #> sofa_first                                6  0.33783784 #> service_unit                              0  0.00000000 #> service_num                               0  0.00000000 #> day_icu_intime                            0  0.00000000 #> day_icu_intime_num                        0  0.00000000 #> hour_icu_intime                           0  0.00000000 #> hosp_exp_flg                              0  0.00000000 #> icu_exp_flg                               0  0.00000000 #> day_28_flg                                0  0.00000000 #> mort_day_censored                         0  0.00000000 #> censor_flg                                0  0.00000000 #> sepsis_flg                                0  0.00000000 #> chf_flg                                   0  0.00000000 #> afib_flg                                  0  0.00000000 #> renal_flg                                 0  0.00000000 #> liver_flg                                 0  0.00000000 #> copd_flg                                  0  0.00000000 #> cad_flg                                   0  0.00000000 #> stroke_flg                                0  0.00000000 #> mal_flg                                   0  0.00000000 #> resp_flg                                  0  0.00000000 #> map_1st                                   0  0.00000000 #> hr_1st                                    0  0.00000000 #> temp_1st                                  3  0.16891892 #> spo2_1st                                  0  0.00000000 #> abg_count                                 0  0.00000000 #> wbc_first                                 8  0.45045045 #> hgb_first                                 8  0.45045045 #> platelet_first                            8  0.45045045 #> sodium_first                              5  0.28153153 #> potassium_first                           5  0.28153153 #> tco2_first                                5  0.28153153 #> chloride_first                            5  0.28153153 #> bun_first                                 5  0.28153153 #> creatinine_first                          6  0.33783784 #> po2_first                               186 10.47297297 #> pco2_first                              186 10.47297297 #> iv_day_1                                143  8.05180180  # remove columns with more than 10% missing data and impute the rest with median # Identify columns with more than 10% missing values columns_to_remove <- names(missing_values_percentage[missing_values_percentage > 10])  # Remove these columns mimic <- select(mimic, -one_of(columns_to_remove))  # Impute remaining missing values with median mimic<- mimic %>% mutate(across(where(~any(is.na(.))), ~ifelse(is.na(.), median(., na.rm = TRUE), .)))  # Check if there are any missing values left remaining_missing_values <- sum(sapply(mimic, function(x) sum(is.na(x)))) remaining_missing_values #> [1] 0  # Identify columns that have only one unique value cols_with_one_value <- sapply(mimic, function(x) length(unique(x)) == 1)  # Subset the dataframe to remove these columns mimic <- mimic[, !cols_with_one_value]"},{"path":"/articles/TwoGroup.html","id":"model-building","dir":"Articles","previous_headings":"Data Preprocessing","what":"Model Building","title":"Binary Protected Attributes","text":"","code":"# Remove columns that are highly correlated with the outcome variable corrplot(cor(select_if(mimic, is.numeric)), method = \"color\", tl.cex = 0.5) mimic <- mimic %>%    select(-c(\"hosp_exp_flg\", \"icu_exp_flg\", \"mort_day_censored\", \"censor_flg\"))  # Use 700 labels to train the mimic train_data <- mimic %>% filter(row_number() <= 700) # Fit a random forest model set.seed(123) rf_model <- randomForest(factor(day_28_flg) ~ ., data = train_data, ntree = 1000)  # Test the model on the remaining data test_data <- mimic %>% filter(row_number() > 700) test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[,2]"},{"path":"/articles/TwoGroup.html","id":"fairness-evaluation","dir":"Articles","previous_headings":"","what":"Fairness Evaluation","title":"Binary Protected Attributes","text":"use sex sensitive attribute day_28_flg outcome. choose threshold = 0.41 overall FPR around 5%.","code":"test_data <- test_data %>%   mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\")) cut_off <- 0.41  test_data %>%   mutate(pred = ifelse(pred > cut_off, 1, 0)) %>%   filter(day_28_flg == 0) %>%   summarise(fpr = mean(pred)) #>          fpr #> 1 0.05054945"},{"path":[]},{"path":"/articles/TwoGroup.html","id":"statistical-parity","dir":"Articles","previous_headings":"Fairness Evaluation > Independence","what":"Statistical Parity","title":"Binary Protected Attributes","text":"","code":"eval_stats_parity(   dat = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = cut_off ) #> There is evidence that model does not satisfy statistical parity. #>   Metric GroupFemale GroupMale Difference  95% Diff CI Ratio 95% Ratio CI #> 1    PPR        0.17      0.08       0.09 [0.05, 0.13]  2.12 [1.48, 3.05]"},{"path":"/articles/TwoGroup.html","id":"conditional-statistical-parity","dir":"Articles","previous_headings":"Fairness Evaluation > Independence","what":"Conditional Statistical Parity","title":"Binary Protected Attributes","text":"conditional age >= 60. can also condition categorical variable. example, can condition service unit = MICU.","code":"eval_cond_stats_parity(dat = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = cut_off,   group2 = \"age\",   condition = \">= 60\") #> There is evidence that model does not satisfy statistical parity. #>   Metric GroupFemale GroupMale Difference  95% Diff CI Ratio 95% Ratio CI #> 1    PPR        0.34      0.21       0.13 [0.05, 0.21]  1.62 [1.18, 2.22] eval_cond_stats_parity(dat = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = cut_off,   group2 = \"service_unit\",   condition = \"MICU\") #> There is not enough evidence that the model does not satisfy #>             statistical parity. #>   Metric GroupFemale GroupMale Difference   95% Diff CI Ratio 95% Ratio CI #> 1    PPR        0.15       0.1       0.05 [-0.01, 0.11]   1.5 [0.86, 2.61]"},{"path":[]},{"path":"/articles/TwoGroup.html","id":"equal-opportunity","dir":"Articles","previous_headings":"Fairness Evaluation > Separation","what":"Equal Opportunity","title":"Binary Protected Attributes","text":"","code":"eval_eq_opp(   dat = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = cut_off ) #> There is evidence that model does not satisfy equal opportunity. #>   Metric GroupFemale GroupMale Difference    95% Diff CI Ratio 95% Ratio CI #> 1    FNR        0.38      0.62      -0.24 [-0.39, -0.09]  0.61 [0.44, 0.85]"},{"path":"/articles/TwoGroup.html","id":"predictive-equality","dir":"Articles","previous_headings":"Fairness Evaluation > Separation","what":"Predictive Equality","title":"Binary Protected Attributes","text":"","code":"eval_pred_equality(   dat = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = cut_off ) #> There is evidence that model does not satisfy predictive #>             equality. #>   Metric GroupFemale GroupMale Difference  95% Diff CI Ratio 95% Ratio CI #> 1    FPR        0.08      0.03       0.05 [0.02, 0.08]  2.67 [1.38, 5.14]"},{"path":"/articles/TwoGroup.html","id":"balance-for-positive-class","dir":"Articles","previous_headings":"Fairness Evaluation > Separation","what":"Balance for Positive Class","title":"Binary Protected Attributes","text":"","code":"eval_pos_class_bal(   dat = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\" ) #> There is evidence that the model does not satisfy #>             balance for positive class. #>                 Metric GroupFemale GroupMale Difference  95% Diff CI Ratio #> 1 Avg. Predicted Prob.        0.46      0.37       0.09 [0.04, 0.14]  1.24 #>   95% Ratio CI #> 1 [1.09, 1.41]"},{"path":"/articles/TwoGroup.html","id":"balance-for-negative-class","dir":"Articles","previous_headings":"Fairness Evaluation > Separation","what":"Balance for Negative Class","title":"Binary Protected Attributes","text":"","code":"eval_neg_class_bal(   dat = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\" ) #> There is enough evidence that the model does not satisfy #>             balance for negative class. #>                 Metric GroupFemale GroupMale Difference  95% Diff CI Ratio #> 1 Avg. Predicted Prob.        0.15       0.1       0.05 [0.03, 0.07]   1.5 #>   95% Ratio CI #> 1 [1.29, 1.74]"},{"path":[]},{"path":"/articles/TwoGroup.html","id":"predictive-parity","dir":"Articles","previous_headings":"Fairness Evaluation > Sufficiency","what":"Predictive Parity","title":"Binary Protected Attributes","text":"","code":"eval_pred_parity(   dat = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = cut_off ) #> There is not enough evidence that the model does not satisfy #>             predictive parity. #>   Metric GroupFemale GroupMale Difference   95% Diff CI Ratio 95% Ratio CI #> 1    PPV        0.62      0.66      -0.04 [-0.21, 0.13]  0.94 [0.72, 1.23]"},{"path":[]},{"path":"/articles/TwoGroup.html","id":"brier-score-parity","dir":"Articles","previous_headings":"Fairness Evaluation > Other Fairness Metrics","what":"Brier Score Parity","title":"Binary Protected Attributes","text":"","code":"eval_bs_parity(   dat = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\" ) #> There is not enough evidence that the model does not satisfy #>             Brier Score parity. #>        Metric GroupFemale GroupMale Difference   95% Diff CI Ratio 95% Ratio CI #> 1 Brier Score        0.09      0.08       0.01 [-0.01, 0.03]  1.12 [0.89, 1.43]"},{"path":"/articles/TwoGroup.html","id":"accuracy-parity","dir":"Articles","previous_headings":"Fairness Evaluation > Other Fairness Metrics","what":"Accuracy Parity","title":"Binary Protected Attributes","text":"","code":"eval_acc_parity(   dat = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = cut_off ) #> There is not enough evidence that the model does not satisfy #>             accuracy parity. #>     Metric GroupFemale GroupMale Difference   95% Diff CI Ratio 95% Ratio CI #> 1 Accuracy        0.87      0.88      -0.01 [-0.05, 0.03]  0.99 [0.94, 1.04]"},{"path":"/articles/TwoGroup.html","id":"treatment-equality","dir":"Articles","previous_headings":"Fairness Evaluation > Other Fairness Metrics","what":"Treatment Equality","title":"Binary Protected Attributes","text":"","code":"eval_treatment_equality(   dat = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\" ) #> There is not enough evidence that the model does not satisfy #>             treatment equality. #>        Metric GroupFemale GroupMale Difference     95% Diff CI Ratio #> 1 FN/FP Ratio        5.11      13.6      -8.49 [-33.33, 16.35]  0.38 #>   95% Ratio CI #> 1  [0.1, 1.36]"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Jianhui Gao. Author, maintainer. Benson Chou. Author. Benjamin Smith. Author. Jessica Gronsbell. Author.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Gao J, Chou B, Smith B, Gronsbell J (2025). FairnessEval: Fairness evaluation metrics confidence intervals. R package version 1.0.0, https://jianhuig.github.io/FairnessTutorial/.","code":"@Manual{,   title = {FairnessEval: Fairness evaluation metrics with confidence intervals},   author = {Jianhui Gao and Benson Chou and Benjamin Smith and Jessica Gronsbell},   year = {2025},   note = {R package version 1.0.0},   url = {https://jianhuig.github.io/FairnessTutorial/}, }"},{"path":"/index.html","id":"fairnesseval-fairness-evaluation-metrics-with-confidence-intervals-","dir":"","previous_headings":"","what":"Fairness evaluation metrics with confidence intervals","title":"Fairness evaluation metrics with confidence intervals","text":"R package reproducible real-world data examples tutorial fairness machine learning healthcare. Link online tutorial. Link preprint.","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Fairness evaluation metrics with confidence intervals","text":"","code":"devtools::install_github(repo = \"https://github.com/jianhuig/FairnessTutorial\")"},{"path":"/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Fairness evaluation metrics with confidence intervals","text":"cite package ‘FairnessEval’ publications use: Gao J, Chou B, Gronsbell J, Smith B (2025). FairnessEval: Fairness evaluation metrics confidence intervals. R package version 1.0.0, https://jianhuig.github.io/FairnessEval/. BibTeX entry LaTeX users ","code":"@Manual{,     title = {FairnessEval: Fairness evaluation metrics with confidence intervals},     author = {Jianhui Gao and Benson Chou and Jessica Gronsbell and Benjamin Smith},     year = {2025},     note = {R package version 1.0.0},     url = {https://jianhuig.github.io/FairnessTutorial/},   }"},{"path":"/index.html","id":"similar-works","dir":"","previous_headings":"","what":"Similar Works","title":"Fairness evaluation metrics with confidence intervals","text":"fairness R package","code":""},{"path":"/index.html","id":"references","dir":"","previous_headings":"","what":"References","title":"Fairness evaluation metrics with confidence intervals","text":"Gao, J. et al. Fair? Defining Fairness Machine Learning Health. arXiv.org https://arxiv.org/abs/2406.09307 (2024).","code":""},{"path":"/reference/eval_acc_parity.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine Accuracy Parity of a Model — eval_acc_parity","title":"Examine Accuracy Parity of a Model — eval_acc_parity","text":"function assesses Accuracy Parity, fairness criterion evaluates whether overall accuracy predictive model consistent across different groups.","code":""},{"path":"/reference/eval_acc_parity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine Accuracy Parity of a Model — eval_acc_parity","text":"","code":"eval_acc_parity(   data,   outcome,   group,   probs,   cutoff = 0.5,   confint = TRUE,   alpha = 0.05,   bootstraps = 2500,   digits = 2,   message = TRUE )"},{"path":"/reference/eval_acc_parity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine Accuracy Parity of a Model — eval_acc_parity","text":"data Data frame containing outcome, predicted outcome, sensitive attribute outcome Name outcome variable group Name sensitive attribute probs Predicted probabilities cutoff Cutoff value predicted probabilities confint Logical indicating whether calculate confidence intervals alpha 1 - significance level confidence interval, default 0.05 bootstraps Number bootstraps use confidence intervals digits Number digits round results , default 2 message Whether print results, default TRUE","code":""},{"path":"/reference/eval_acc_parity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine Accuracy Parity of a Model — eval_acc_parity","text":"list containing following elements: Accuracy Group 1 Accuracy Group 2 Difference accuracy Ratio accuracy confidence intervals computed (confint = TRUE): vector length 2 containing lower upper bounds 95% confidence interval difference accuracy vector length 2 containing lower upper bounds 95% confidence interval ratio accurac","code":""},{"path":[]},{"path":"/reference/eval_acc_parity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Examine Accuracy Parity of a Model — eval_acc_parity","text":"","code":"# \\donttest{ library(FairnessEval) library(dplyr) #>  #> Attaching package: ‘dplyr’ #> The following objects are masked from ‘package:stats’: #>  #>     filter, lag #> The following objects are masked from ‘package:base’: #>  #>     intersect, setdiff, setequal, union library(randomForest) #> randomForest 4.7-1.2 #> Type rfNews() to see new features/changes/bug fixes. #>  #> Attaching package: ‘randomForest’ #> The following object is masked from ‘package:dplyr’: #>  #>     combine data(\"mimic_preprocessed\") set.seed(123) train_data <- mimic_preprocessed |>   dplyr::filter(dplyr::row_number() <= 700) # Fit a random forest model rf_model <- randomForest::randomForest(factor(day_28_flg) ~ ., data = train_data, ntree = 1000) # Test the model on the remaining data test_data <- mimic_preprocessed |>   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\")) |>   dplyr::filter(dplyr::row_number() > 700)  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]  # Fairness evaluation # We will use sex as the sensitive attribute and day_28_flg as the outcome. # We choose threshold = 0.41 so that the overall FPR is around 5%.  # Evaluate Accuracy Parity eval_acc_parity(   data = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = 0.41 ) #> There is not enough evidence that the model does not satisfy #>             accuracy parity. #>     Metric GroupFemale GroupMale Difference   95% Diff CI Ratio 95% Ratio CI #> 1 Accuracy        0.87      0.88      -0.01 [-0.05, 0.03]  0.99 [0.94, 1.04] # }"},{"path":"/reference/eval_bs_parity.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine Brier Score Parity of a Model This function evaluates Brier Score Parity, a fairness measure that checks whether the Brier score (a metric that quantifies the accuracy of probabilistic predictions) is balanced across different groups. Brier score parity ensures that the model's predicted probabilities have similar accuracy for different groups,such as different genders or ethnicities. — eval_bs_parity","title":"Examine Brier Score Parity of a Model This function evaluates Brier Score Parity, a fairness measure that checks whether the Brier score (a metric that quantifies the accuracy of probabilistic predictions) is balanced across different groups. Brier score parity ensures that the model's predicted probabilities have similar accuracy for different groups,such as different genders or ethnicities. — eval_bs_parity","text":"Examine Brier Score Parity Model function evaluates Brier Score Parity, fairness measure checks whether Brier score (metric quantifies accuracy probabilistic predictions) balanced across different groups. Brier score parity ensures model's predicted probabilities similar accuracy different groups,different genders ethnicities.","code":""},{"path":"/reference/eval_bs_parity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine Brier Score Parity of a Model This function evaluates Brier Score Parity, a fairness measure that checks whether the Brier score (a metric that quantifies the accuracy of probabilistic predictions) is balanced across different groups. Brier score parity ensures that the model's predicted probabilities have similar accuracy for different groups,such as different genders or ethnicities. — eval_bs_parity","text":"","code":"eval_bs_parity(   data,   outcome,   group,   probs,   confint = TRUE,   alpha = 0.05,   bootstraps = 2500,   digits = 2,   message = TRUE )"},{"path":"/reference/eval_bs_parity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine Brier Score Parity of a Model This function evaluates Brier Score Parity, a fairness measure that checks whether the Brier score (a metric that quantifies the accuracy of probabilistic predictions) is balanced across different groups. Brier score parity ensures that the model's predicted probabilities have similar accuracy for different groups,such as different genders or ethnicities. — eval_bs_parity","text":"data Data frame containing outcome, predicted outcome, sensitive attribute outcome Name outcome variable group Name sensitive attribute probs Predicted probabilities confint Logical indicating whether calculate confidence intervals alpha 1 - significance level confidence interval, default 0.05 bootstraps Number bootstraps use confidence intervals digits Number digits round results , default 2 message Whether print results, default TRUE","code":""},{"path":"/reference/eval_bs_parity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine Brier Score Parity of a Model This function evaluates Brier Score Parity, a fairness measure that checks whether the Brier score (a metric that quantifies the accuracy of probabilistic predictions) is balanced across different groups. Brier score parity ensures that the model's predicted probabilities have similar accuracy for different groups,such as different genders or ethnicities. — eval_bs_parity","text":"list containing following elements: Brier Score Group 1 Brier Score Group 2 Difference Brier Score Ratio Brier Score confidence intervals computed (confint = TRUE): vector length 2 containing lower upper bounds 95% confidence interval difference Brier Score vector length 2 containing lower upper bounds 95% confidence interval ratio Brier Score","code":""},{"path":[]},{"path":"/reference/eval_bs_parity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Examine Brier Score Parity of a Model This function evaluates Brier Score Parity, a fairness measure that checks whether the Brier score (a metric that quantifies the accuracy of probabilistic predictions) is balanced across different groups. Brier score parity ensures that the model's predicted probabilities have similar accuracy for different groups,such as different genders or ethnicities. — eval_bs_parity","text":"","code":"# \\donttest{ library(FairnessEval) library(dplyr) library(randomForest) data(\"mimic_preprocessed\") set.seed(123) train_data <- mimic_preprocessed |>   dplyr::filter(dplyr::row_number() <= 700) # Fit a random forest model rf_model <- randomForest::randomForest(factor(day_28_flg) ~ ., data = train_data, ntree = 1000) # Test the model on the remaining data test_data <- mimic_preprocessed |>   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\")) |>   dplyr::filter(dplyr::row_number() > 700)  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]  # Fairness evaluation # We will use sex as the sensitive attribute and day_28_flg as the outcome.  # Evaluate Brier Score Parity eval_bs_parity(   data = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\" ) #> There is not enough evidence that the model does not satisfy #>             Brier Score parity. #>        Metric GroupFemale GroupMale Difference   95% Diff CI Ratio 95% Ratio CI #> 1 Brier Score        0.09      0.08       0.01 [-0.01, 0.03]  1.12 [0.89, 1.43] # }"},{"path":"/reference/eval_cond_acc_equality.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine Conditional Use Accuracy Equality of a Model — eval_cond_acc_equality","title":"Examine Conditional Use Accuracy Equality of a Model — eval_cond_acc_equality","text":"function evaluates Conditional Use Accuracy Equality, fairness criterion requires predictive performance similar across groups model makes positive negative predictions.","code":""},{"path":"/reference/eval_cond_acc_equality.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine Conditional Use Accuracy Equality of a Model — eval_cond_acc_equality","text":"","code":"eval_cond_acc_equality(   data,   outcome,   group,   probs,   cutoff = 0.5,   confint = TRUE,   alpha = 0.05,   bootstraps = 2500,   digits = 2,   message = TRUE )"},{"path":"/reference/eval_cond_acc_equality.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine Conditional Use Accuracy Equality of a Model — eval_cond_acc_equality","text":"data Data frame containing outcome, predicted outcome, sensitive attribute outcome Name outcome variable, must binary group Name sensitive attribute probs Name predicted outcome variable cutoff Threshold predicted outcome, default 0.5 confint Whether compute 95% confidence interval, default TRUE alpha 1 - significance level confidence interval, default 0.05 bootstraps Number bootstrap samples, default 2500 digits Number digits round results , default 2 message Whether print results, default TRUE","code":""},{"path":"/reference/eval_cond_acc_equality.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine Conditional Use Accuracy Equality of a Model — eval_cond_acc_equality","text":"list containing following elements: PPV_Group1: Positive Predictive Value first group PPV_Group2: Positive Predictive Value second group PPV_Diff: Difference Positive Predictive Value NPV_Group1: Negative Predictive Value first group NPV_Group2: Negative Predictive Value second group NPV_Diff: Difference Negative Predictive Value confidence intervals computed (confint = TRUE): PPV_Diff_CI: vector length 2 containing lower upper bounds 95% confidence interval difference Positive Predictive Value NPV_Diff_CI: vector length 2 containing lower upper bounds 95% confidence interval difference Negative Predictive Value","code":""},{"path":[]},{"path":"/reference/eval_cond_acc_equality.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Examine Conditional Use Accuracy Equality of a Model — eval_cond_acc_equality","text":"","code":"# \\donttest{ library(FairnessEval) library(dplyr) library(randomForest) data(\"mimic_preprocessed\") set.seed(123) train_data <- mimic_preprocessed |>   dplyr::filter(dplyr::row_number() <= 700) # Fit a random forest model rf_model <- randomForest::randomForest(factor(day_28_flg) ~ ., data = train_data, ntree = 1000) # Test the model on the remaining data test_data <- mimic_preprocessed |>   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\")) |>   dplyr::filter(dplyr::row_number() > 700)  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]  # Fairness evaluation # We will use sex as the sensitive attribute and day_28_flg as the outcome. # We choose threshold = 0.41 so that the overall FPR is around 5%.  # Evaluate Conditional Use Accuracy Equality eval_cond_acc_equality(   data = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = 0.41 ) #> There is not enough evidence that the model does not satisfy #>             conditional use accuracy equality. #>     Metric GroupFemale GroupMale  Difference                       95% CI #> 1 PPV; NPV  0.62; 0.92 0.66; 0.9 -0.04; 0.02 [-0.24, 0.16]; [-0.02, 0.06] # }"},{"path":"/reference/eval_cond_stats_parity.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine Conditional Statistical Parity of a Model — eval_cond_stats_parity","title":"Examine Conditional Statistical Parity of a Model — eval_cond_stats_parity","text":"function evaluates conditional statistical parity, measures fairness comparing positive prediction rates across sensitive groups within defined subgroup population. useful scenarios fairness evaluated context-specific way—e.g., within particular hospital unit age bracket. Conditional statistical parity refinement standard statistical parity. Instead comparing prediction rates across groups entire dataset, restricts comparison specified subset population, defined conditioning variable.","code":""},{"path":"/reference/eval_cond_stats_parity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine Conditional Statistical Parity of a Model — eval_cond_stats_parity","text":"","code":"eval_cond_stats_parity(   data,   outcome,   group,   group2,   condition,   probs,   cutoff = 0.5,   bootstraps = 2500,   alpha = 0.05,   message = TRUE,   digits = 2 )"},{"path":"/reference/eval_cond_stats_parity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine Conditional Statistical Parity of a Model — eval_cond_stats_parity","text":"data Data frame containing outcome, predicted outcome, sensitive attribute outcome Name outcome variable, must binary group Name sensitive attribute group2 Name group condition condition conditional group categorical, condition supplied must character levels condition . conditional group continuous, conditions supplied must character containing sign condition value threshold continuous variable (e.g. \"<50\", \">50\", \"<=50\", \">=50\"). probs Name predicted outcome variable cutoff Threshold predicted outcome, default 0.5 bootstraps Number bootstrap samples, default 2500 alpha 1 - significance level confidence interval, default 0.05 message Whether print results, default TRUE digits Number digits round results , default 2","code":""},{"path":"/reference/eval_cond_stats_parity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine Conditional Statistical Parity of a Model — eval_cond_stats_parity","text":"list containing following elements: Conditions: conditions used calculate conditional PPR PPR_Group1: Positive Prediction Rate first group PPR_Group2: Positive Prediction Rate second group PPR_Diff: Difference Positive Prediction Rate PPR_Ratio: Ratio Positive Prediction Rate confidence intervals computed (confint = TRUE): PPR_Diff_CI: vector length 2 containing lower upper bounds 95% confidence interval difference Positive Prediction Rate PPR_Ratio_CI: vector length 2 containing lower upper bounds 95% confidence interval ratio Positive Prediction Rate","code":""},{"path":"/reference/eval_cond_stats_parity.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Examine Conditional Statistical Parity of a Model — eval_cond_stats_parity","text":"function supports categorical continuous conditioning variables. continuous variables, can supply threshold expression like \"<50\" \">=75\" condition parameter.","code":""},{"path":[]},{"path":"/reference/eval_cond_stats_parity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Examine Conditional Statistical Parity of a Model — eval_cond_stats_parity","text":"","code":"# \\donttest{ #' library(FairnessEval) library(dplyr) library(randomForest) data(\"mimic_preprocessed\") set.seed(123) train_data <- mimic_preprocessed |>   dplyr::filter(dplyr::row_number() <= 700) # Fit a random forest model rf_model <- randomForest::randomForest(factor(day_28_flg) ~ ., data = train_data, ntree = 1000) # Test the model on the remaining data test_data <- mimic_preprocessed |>   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\")) |>   dplyr::filter(dplyr::row_number() > 700)  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]  # Fairness evaluation # We will use sex as the sensitive attribute and day_28_flg as the outcome. # We choose threshold = 0.41 so that the overall FPR is around 5%.  # Evaluate Conditional Statistical Parity  eval_cond_stats_parity(   data = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   group2 = \"service_unit\",   condition = \"MICU\",   probs = \"pred\",   cutoff = 0.41 ) #> There is not enough evidence that the model does not satisfy #>             statistical parity. #>   Metric GroupFemale GroupMale Difference   95% Diff CI Ratio 95% Ratio CI #> 1    PPR        0.15       0.1       0.05 [-0.01, 0.11]   1.5  [0.87, 2.6] # }"},{"path":"/reference/eval_eq_odds.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine Equalized Odds of a Predictive Model — eval_eq_odds","title":"Examine Equalized Odds of a Predictive Model — eval_eq_odds","text":"function evaluates whether predictive model satisfies Equalized Odds criterion comparing False Negative Rates (FNR) False Positive Rates (FPR) across two groups defined binary sensitive attribute. reports rate group, differences, ratios, bootstrap-based confidence regions. Bonferroni-corrected union test used test whether model violates Equalized Odds criterion.","code":""},{"path":"/reference/eval_eq_odds.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine Equalized Odds of a Predictive Model — eval_eq_odds","text":"","code":"eval_eq_odds(   data,   outcome,   group,   probs,   cutoff = 0.5,   bootstraps = 2500,   alpha = 0.05,   digits = 2,   message = TRUE )"},{"path":"/reference/eval_eq_odds.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine Equalized Odds of a Predictive Model — eval_eq_odds","text":"data data frame containing true binary outcomes, predicted probabilities, sensitive group membership. outcome string specifying name binary outcome variable data. group string specifying name binary sensitive attribute variable (e.g., race, gender) used define comparison groups. probs string specifying name variable containing predicted probabilities risk scores. cutoff numeric value used threshold predicted probabilities binary predictions; defaults 0.5. bootstraps integer specifying number bootstrap resamples constructing confidence intervals; vdefaults 2500. alpha Significance level (1 - alpha) confidence interval; defaults 0.05. digits Number decimal places round numeric results; defaults 2. message Logical; TRUE (default), prints textual summary fairness evaluation.","code":""},{"path":"/reference/eval_eq_odds.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine Equalized Odds of a Predictive Model — eval_eq_odds","text":"data frame summarizing group disparities FNR FPR following columns: Metric: reported metrics (\"FNR; FPR\"). Group1: Estimated FNR FPR first group. Group2: Estimated FNR FPR second group. Difference: Differences FNR FPR, computed Group1 - Group2. 95% CR: Bonferroni-adjusted confidence regions differences. Ratio: Ratios FNR FPR, computed Group1 / Group2. 95% CR: Bonferroni-adjusted confidence regions ratios.","code":""},{"path":"/reference/eval_eq_odds.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Examine Equalized Odds of a Predictive Model — eval_eq_odds","text":"","code":"# \\donttest{ library(FairnessEval) library(dplyr) library(randomForest) data(\"mimic_preprocessed\") set.seed(123) train_data <- mimic_preprocessed |>   dplyr::filter(dplyr::row_number() <= 700) # Fit a random forest model rf_model <- randomForest::randomForest(factor(day_28_flg) ~ .,   data = train_data, ntree = 1000 ) # Test the model on the remaining data test_data <- mimic_preprocessed |>   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\")) |>   dplyr::filter(dplyr::row_number() > 700)  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]  # Fairness evaluation # We will use sex as the sensitive attribute and day_28_flg as the outcome. # We choose threshold = 0.41 so that the overall FPR is around 5%.  # Evaluate Equalized Odds eval_eq_odds(   data = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = 0.41 ) #> There is evidence that model does not satisfy equalized odds. #>     Metric Group Female Group Male  Difference                       95% CR #> 1 FNR; FPR   0.38; 0.08 0.62; 0.03 -0.24; 0.05 [-0.41, -0.07]; [0.01, 0.09] #>        Ratio                    95% CR #> 1 0.61; 2.67 [0.42, 0.9]; [1.26, 5.66] # }"},{"path":"/reference/eval_eq_opp.html","id":null,"dir":"Reference","previous_headings":"","what":"Evaluate Equal Opportunity Compliance of a Predictive Model — eval_eq_opp","title":"Evaluate Equal Opportunity Compliance of a Predictive Model — eval_eq_opp","text":"function evaluates fairness predictive model respect Equal Opportunity criterion, requires False Negative Rate (FNR) comparable across groups defined sensitive attribute. function quantifies disparities FNR two groups provides absolute difference ratio, along confidence intervals obtained via bootstrapping.","code":""},{"path":"/reference/eval_eq_opp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Evaluate Equal Opportunity Compliance of a Predictive Model — eval_eq_opp","text":"","code":"eval_eq_opp(   data,   outcome,   group,   probs,   cutoff = 0.5,   bootstraps = 2500,   alpha = 0.05,   digits = 2,   message = TRUE )"},{"path":"/reference/eval_eq_opp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Evaluate Equal Opportunity Compliance of a Predictive Model — eval_eq_opp","text":"data data frame containing true binary outcomes, predicted probabilities, sensitive group membership. outcome string specifying name binary outcome variable data. group string specifying name sensitive attribute variable (e.g., race, gender). probs string specifying name variable containing predicted probabilities risk scores. cutoff numeric value used threshold predicted probabilities binary decisions; defaults 0.5. bootstraps integer specifying number bootstrap resamples constructing confidence intervals; defaults 2500. alpha Significance level constructing (1 - alpha) confidence interval; defaults 0.05. digits Integer indicating number decimal places round results ; defaults 2. message Logical; TRUE (default), prints textual summary fairness evaluation.","code":""},{"path":"/reference/eval_eq_opp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Evaluate Equal Opportunity Compliance of a Predictive Model — eval_eq_opp","text":"data frame summarizing FNR-based group disparity metrics following columns: Metric label indicating reported fairness criterion. Group1 Estimated FNR FPR first group. Group2 Estimated FNR FPR second group. Difference difference FNR two groups, computed FNR Group1 minus FNR Group2. 95% Diff CI (1 - alpha) confidence interval FNR difference. Ratio ratio FNRs Group1 Group2, computed FNR Group1 divided FNR Group2. 95% Ratio CI corresponding confidence interval FNR ratio.","code":""},{"path":"/reference/eval_eq_opp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Evaluate Equal Opportunity Compliance of a Predictive Model — eval_eq_opp","text":"","code":"# \\donttest{ library(FairnessEval) library(dplyr) library(randomForest) data(\"mimic_preprocessed\") set.seed(123) train_data <- mimic_preprocessed |>   dplyr::filter(dplyr::row_number() <= 700) # Fit a random forest model rf_model <- randomForest::randomForest(factor(day_28_flg) ~ .,   data =     train_data, ntree = 1000 ) # Test the model on the remaining data test_data <- mimic_preprocessed |>   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\")) |>   dplyr::filter(dplyr::row_number() > 700)  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]  # Fairness evaluation # We will use sex as the sensitive attribute and day_28_flg as the outcome. # We choose threshold = 0.41 so that the overall FPR is around 5%.  # Evaluate Equal Opportunity Compliance eval_eq_opp(   data = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = 0.41 ) #> There is evidence that model does not satisfy equal opportunity. #>   Metric GroupFemale GroupMale Difference    95% Diff CI Ratio 95% Ratio CI #> 1    FNR        0.38      0.62      -0.24 [-0.39, -0.09]  0.61 [0.44, 0.86] # }"},{"path":"/reference/eval_neg_class_bal.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine Balance for Negative Class of a Model — eval_neg_class_bal","title":"Examine Balance for Negative Class of a Model — eval_neg_class_bal","text":"function evaluates Balance Negative Class, fairness metric checks whether predicted probabilities negative class (.e., outcome = 0) balanced across groups defined sensitive attribute (e.g., gender, race).","code":""},{"path":"/reference/eval_neg_class_bal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine Balance for Negative Class of a Model — eval_neg_class_bal","text":"","code":"eval_neg_class_bal(   data,   outcome,   group,   probs,   confint = TRUE,   alpha = 0.05,   bootstraps = 2500,   digits = 2,   message = TRUE )"},{"path":"/reference/eval_neg_class_bal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine Balance for Negative Class of a Model — eval_neg_class_bal","text":"data Data frame containing outcome, predicted outcome, sensitive attribute outcome Name outcome variable group Name sensitive attribute probs Predicted probabilities confint Logical indicating whether calculate confidence intervals alpha 1 - significance level confidence interval, default 0.05 bootstraps Number bootstraps use confidence intervals digits Number digits round results , default 2 message Whether print results, default TRUE","code":""},{"path":"/reference/eval_neg_class_bal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine Balance for Negative Class of a Model — eval_neg_class_bal","text":"list containing following elements: Average predicted probability Group 1 Average predicted probability Group 2 Difference average predicted probability Ratio average predicted probability confidence intervals computed (confint = TRUE): vector length 2 containing lower upper bounds 95% confidence interval difference average predicted probability vector length 2 containing lower upper bounds 95% confidence interval ratio average predicted probability","code":""},{"path":[]},{"path":"/reference/eval_neg_class_bal.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Examine Balance for Negative Class of a Model — eval_neg_class_bal","text":"","code":"# \\donttest{ library(FairnessEval) library(dplyr) library(randomForest) data(\"mimic_preprocessed\") set.seed(123) train_data <- mimic_preprocessed |>   dplyr::filter(dplyr::row_number() <= 700) # Fit a random forest model rf_model <- randomForest::randomForest(factor(day_28_flg) ~ ., data = train_data, ntree = 1000) # Test the model on the remaining data test_data <- mimic_preprocessed |>   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\")) |>   dplyr::filter(dplyr::row_number() > 700)  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]  # Fairness evaluation # We will use sex as the sensitive attribute and day_28_flg as the outcome.  # Evaluate Balance for Negative Class eval_neg_class_bal(   data = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\" ) #> There is enough evidence that the model does not satisfy #>             balance for negative class. #>                 Metric GroupFemale GroupMale Difference  95% Diff CI Ratio #> 1 Avg. Predicted Prob.        0.15       0.1       0.05 [0.03, 0.07]   1.5 #>   95% Ratio CI #> 1 [1.29, 1.75] # }"},{"path":"/reference/eval_pos_class_bal.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine Balance for Positive Class of a Model — eval_pos_class_bal","title":"Examine Balance for Positive Class of a Model — eval_pos_class_bal","text":"function evaluates Balance Positive Class, fairness metric checks whether predicted probabilities positive class (.e., outcome = 1) balanced across groups defined sensitive attribute (e.g., gender, race).","code":""},{"path":"/reference/eval_pos_class_bal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine Balance for Positive Class of a Model — eval_pos_class_bal","text":"","code":"eval_pos_class_bal(   data,   outcome,   group,   probs,   confint = TRUE,   alpha = 0.05,   bootstraps = 2500,   digits = 2,   message = TRUE )"},{"path":"/reference/eval_pos_class_bal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine Balance for Positive Class of a Model — eval_pos_class_bal","text":"data Data frame containing outcome, predicted outcome, sensitive attribute outcome Name outcome variable group Name sensitive attribute probs Predicted probabilities confint Logical indicating whether calculate confidence intervals alpha 1 - significance level confidence interval, default 0.05 bootstraps Number bootstraps use confidence intervals digits Number digits round results , default 2 message Whether print results, default TRUE","code":""},{"path":"/reference/eval_pos_class_bal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine Balance for Positive Class of a Model — eval_pos_class_bal","text":"list containing following elements: Average predicted probability Group 1 Average predicted probability Group 2 Difference average predicted probability Ratio average predicted probability confidence intervals computed (confint = TRUE): vector length 2 containing lower upper bounds 95% confidence interval difference average predicted probability vector length 2 containing lower upper bounds 95% confidence interval ratio average predicted probability","code":""},{"path":[]},{"path":"/reference/eval_pos_class_bal.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Examine Balance for Positive Class of a Model — eval_pos_class_bal","text":"","code":"# \\donttest{ library(FairnessEval) library(dplyr) library(randomForest) data(\"mimic_preprocessed\") set.seed(123) train_data <- mimic_preprocessed |>   dplyr::filter(dplyr::row_number() <= 700) # Fit a random forest model rf_model <- randomForest::randomForest(factor(day_28_flg) ~ ., data = train_data, ntree = 1000) # Test the model on the remaining data test_data <- mimic_preprocessed |>   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\")) |>   dplyr::filter(dplyr::row_number() > 700)  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]  # Fairness evaluation # We will use sex as the sensitive attribute and day_28_flg as the outcome.  # Evaluate Balance for Positive Class eval_pos_class_bal(   data = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\" ) #> There is evidence that the model does not satisfy #>             balance for positive class. #>                 Metric GroupFemale GroupMale Difference  95% Diff CI Ratio #> 1 Avg. Predicted Prob.        0.46      0.37       0.09 [0.04, 0.14]  1.24 #>   95% Ratio CI #> 1 [1.09, 1.42] # }"},{"path":"/reference/eval_pred_equality.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine Predictive Equality of a Model — eval_pred_equality","title":"Examine Predictive Equality of a Model — eval_pred_equality","text":"function evaluates predictive equality, fairness metric compares False Positive Rate (FPR) groups defined sensitive attribute. assesses whether individuals different groups equally likely incorrectly flagged positive , fact, negative.","code":""},{"path":"/reference/eval_pred_equality.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine Predictive Equality of a Model — eval_pred_equality","text":"","code":"eval_pred_equality(   data,   outcome,   group,   probs,   cutoff = 0.5,   confint = TRUE,   alpha = 0.05,   bootstraps = 2500,   digits = 2,   message = TRUE )"},{"path":"/reference/eval_pred_equality.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine Predictive Equality of a Model — eval_pred_equality","text":"data Data frame containing outcome, predicted outcome, sensitive attribute outcome Name outcome variable, must binary group Name sensitive attribute probs Name predicted outcome variable cutoff Threshold predicted outcome, default 0.5 confint Whether compute 95% confidence interval, default TRUE alpha 1 - significance level confidence interval, default 0.05 bootstraps Number bootstrap samples, default 2500 digits Number digits round results , default 2 message Whether print results, default TRUE","code":""},{"path":"/reference/eval_pred_equality.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine Predictive Equality of a Model — eval_pred_equality","text":"list containing following elements: FPR_Group1: False Positive Rate first group FPR_Group2: False Positive Rate second group FPR_Diff: Difference False Positive Rate FPR_Ratio: Ratio False Positive Rate confidence intervals computed (confint = TRUE): FPR_Diff_CI: vector length 2 containing lower upper bounds 95% confidence interval difference False Positive Rate FPR_Ratio_CI: vector length 2 containing lower upper bounds 95% confidence interval ratio False Positive Rate","code":""},{"path":[]},{"path":"/reference/eval_pred_equality.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Examine Predictive Equality of a Model — eval_pred_equality","text":"","code":"# \\donttest{ library(FairnessEval) library(dplyr) library(randomForest) data(\"mimic_preprocessed\") set.seed(123) train_data <- mimic_preprocessed |>   dplyr::filter(dplyr::row_number() <= 700) # Fit a random forest model rf_model <- randomForest::randomForest(factor(day_28_flg) ~ ., data = train_data, ntree = 1000) # Test the model on the remaining data test_data <- mimic_preprocessed |>   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\")) |>   dplyr::filter(dplyr::row_number() > 700)  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]  # Fairness evaluation # We will use sex as the sensitive attribute and day_28_flg as the outcome. # We choose threshold = 0.41 so that the overall FPR is around 5%.  # Evaluate Predictive Equality eval_pred_equality(   data = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = 0.41 ) #> There is evidence that model does not satisfy predictive #>             equality. #>   Metric GroupFemale GroupMale Difference  95% Diff CI Ratio 95% Ratio CI #> 1    FPR        0.08      0.03       0.05 [0.02, 0.08]  2.67 [1.38, 5.15] # }"},{"path":"/reference/eval_pred_parity.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine Predictive Parity of a Model — eval_pred_parity","title":"Examine Predictive Parity of a Model — eval_pred_parity","text":"function evaluates predictive parity (PP), key fairness criterion compares Positive Predictive Value (PPV) groups defined sensitive attribute. words, assesses whether predicted positives equally likely correct (.e., true positives) across different subgroups.","code":""},{"path":"/reference/eval_pred_parity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine Predictive Parity of a Model — eval_pred_parity","text":"","code":"eval_pred_parity(   data,   outcome,   group,   probs,   cutoff = 0.5,   confint = TRUE,   bootstraps = 2500,   alpha = 0.05,   digits = 2,   message = TRUE )"},{"path":"/reference/eval_pred_parity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine Predictive Parity of a Model — eval_pred_parity","text":"data Data frame containing outcome, predicted outcome, sensitive attribute outcome Name outcome variable, must binary group Name sensitive attribute probs Name predicted outcome variable cutoff Threshold predicted outcome, default 0.5 confint Whether compute 95% confidence interval, default TRUE bootstraps Number bootstrap samples, default 2500 alpha 1 - significance level confidence interval, default 0.05 digits Number digits round results , default 2 message Whether print results, default TRUE","code":""},{"path":"/reference/eval_pred_parity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine Predictive Parity of a Model — eval_pred_parity","text":"list containing following elements: PPV_Group1: Positive Predictive Value first group PPV_Group2: Positive Predictive Value second group PPV_Diff: Difference Positive Predictive Value PPV_Ratio: Ratio Positive Predictive Value confidence intervals computed (confint = TRUE): PPV_Diff_CI: vector length 2 containing lower upper bounds 95% confidence interval difference Positive Predictive Value PPV_Ratio_CI: vector length 2 containing lower upper bounds 95% confidence interval ratio Positive Predictive Value","code":""},{"path":"/reference/eval_pred_parity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Examine Predictive Parity of a Model — eval_pred_parity","text":"","code":"# \\donttest{ library(FairnessEval) library(dplyr) library(randomForest) data(\"mimic_preprocessed\") set.seed(123) train_data <- mimic_preprocessed |>   dplyr::filter(dplyr::row_number() <= 700) # Fit a random forest model rf_model <- randomForest::randomForest(factor(day_28_flg) ~ ., data = train_data, ntree = 1000) # Test the model on the remaining data test_data <- mimic_preprocessed |>   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\")) |>   dplyr::filter(dplyr::row_number() > 700)  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]  # Fairness evaluation # We will use sex as the sensitive attribute and day_28_flg as the outcome. # We choose threshold = 0.41 so that the overall FPR is around 5%.  # Evaluate Predictive Parity eval_pred_parity(   data = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = 0.41 ) #> There is not enough evidence that the model does not satisfy #>             predictive parity. #>   Metric GroupFemale GroupMale Difference   95% Diff CI Ratio 95% Ratio CI #> 1    PPV        0.62      0.66      -0.04 [-0.21, 0.13]  0.94 [0.72, 1.23] # }"},{"path":"/reference/eval_stats_parity.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine Statistical Parity of a Model — eval_stats_parity","title":"Examine Statistical Parity of a Model — eval_stats_parity","text":"function assesses statistical parity - also known demographic parity - predictions binary classifier across two groups defined sensitive attribute. Statistical parity compares rate different groups receive positive prediction, irrespective true outcome. reports Positive Prediction Rate (PPR) group, differences, ratios, bootstrap-based confidence regions.","code":""},{"path":"/reference/eval_stats_parity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine Statistical Parity of a Model — eval_stats_parity","text":"","code":"eval_stats_parity(   data,   outcome,   group,   probs,   cutoff = 0.5,   confint = TRUE,   bootstraps = 2500,   alpha = 0.05,   digits = 2,   message = TRUE )"},{"path":"/reference/eval_stats_parity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine Statistical Parity of a Model — eval_stats_parity","text":"data Data frame containing outcome, predicted outcome, sensitive attribute outcome Name outcome variable, must binary group Name sensitive attribute probs Name predicted outcome variable cutoff Threshold predicted outcome, default 0.5 confint Whether compute 95% confidence interval, default TRUE bootstraps Number bootstrap samples, default 2500 alpha 1 - significance level confidence interval, default 0.05 digits Number digits round results , default 2 message Whether print results, default TRUE","code":""},{"path":"/reference/eval_stats_parity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine Statistical Parity of a Model — eval_stats_parity","text":"list containing following elements: PPR_Group1: Positive Prediction Rate first group PPR_Group2: Positive Prediction Rate second group PPR_Diff: Difference Positive Prediction Rate PPR_Ratio: ratio Positive Prediction Rate two groups. confidence intervals computed (confint = TRUE): PPR_Diff_CI: vector length 2 containing lower upper bounds 95% confidence interval difference Positive Prediction Rate PPR_Ratio_CI: vector length 2 containing lower upper bounds 95% confidence interval ratio Positive Prediction Rate","code":""},{"path":[]},{"path":"/reference/eval_stats_parity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Examine Statistical Parity of a Model — eval_stats_parity","text":"","code":"# \\donttest{ library(FairnessEval) library(dplyr) library(randomForest) data(\"mimic_preprocessed\") set.seed(123) train_data <- mimic_preprocessed |>   dplyr::filter(dplyr::row_number() <= 700) # Fit a random forest model rf_model <- randomForest::randomForest(factor(day_28_flg) ~ ., data = train_data, ntree = 1000) # Test the model on the remaining data test_data <- mimic_preprocessed |>   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\")) |>   dplyr::filter(dplyr::row_number() > 700)  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]  # Fairness evaluation # We will use sex as the sensitive attribute and day_28_flg as the outcome. # We choose threshold = 0.41 so that the overall FPR is around 5%.  # Evaluate Statistical Parity eval_stats_parity(   data = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = 0.41 ) #> There is evidence that model does not satisfy statistical parity. #>   Metric GroupFemale GroupMale Difference  95% Diff CI Ratio 95% Ratio CI #> 1    PPR        0.17      0.08       0.09 [0.05, 0.13]  2.12 [1.48, 3.05] # }"},{"path":"/reference/eval_treatment_equality.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine Treatment Equality of a Model — eval_treatment_equality","title":"Examine Treatment Equality of a Model — eval_treatment_equality","text":"function evaluates Treatment Equality, fairness measure checks whether model treats different groups (e.g., based gender race) equally terms False Negative False Positive ratio. Treatment equality ensures model unfairly treat one group harshly another assessing whether ratio false negatives false positives balanced across groups.","code":""},{"path":"/reference/eval_treatment_equality.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine Treatment Equality of a Model — eval_treatment_equality","text":"","code":"eval_treatment_equality(   data,   outcome,   group,   probs,   cutoff = 0.5,   confint = TRUE,   alpha = 0.05,   bootstraps = 2500,   digits = 2,   message = TRUE )"},{"path":"/reference/eval_treatment_equality.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine Treatment Equality of a Model — eval_treatment_equality","text":"data Data frame containing outcome, predicted outcome, sensitive attribute outcome Name outcome variable group Name sensitive attribute probs Predicted probabilities cutoff Cutoff value predicted probabilities confint Logical indicating whether calculate confidence intervals alpha 1 - significance level confidence interval, default 0.05 bootstraps Number bootstraps use confidence intervals digits Number digits round results , default 2 message Whether print results, default TRUE","code":""},{"path":"/reference/eval_treatment_equality.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine Treatment Equality of a Model — eval_treatment_equality","text":"list containing following elements: False Negative / False Positive ratio Group 1 False Negative / False Positive ratio Group 2 Difference False Negative / False Positive ratio Ratio False Negative / False Positive ratio confidence intervals computed (confint = TRUE): vector length 2 containing lower upper bounds 95% confidence interval difference False Negative / False Positive ratio vector length 2 containing lower upper bounds 95% confidence interval ratio False Negative / False Positive ratio","code":""},{"path":[]},{"path":"/reference/eval_treatment_equality.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Examine Treatment Equality of a Model — eval_treatment_equality","text":"","code":"# \\donttest{ library(FairnessEval) library(dplyr) library(randomForest) # Data for tests data(\"mimic_preprocessed\") set.seed(123) train_data <- mimic_preprocessed |>   dplyr::filter(dplyr::row_number() <= 700) # Fit a random forest model rf_model <- randomForest::randomForest(factor(day_28_flg) ~ ., data = train_data, ntree = 1000) # Test the model on the remaining data test_data <- mimic_preprocessed |>   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\")) |>   dplyr::filter(dplyr::row_number() > 700)  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]  # Fairness evaluation # We will use sex as the sensitive attribute and day_28_flg as the outcome.  # Evaluate Treatment Equality eval_treatment_equality(   data = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = 0.41,   confint = TRUE,   alpha = 0.05,   bootstraps = 2500,   digits = 2,   message = FALSE ) #>        Metric GroupFemale GroupMale Difference   95% Diff CI Ratio 95% Ratio CI #> 1 FN/FP Ratio        1.03      3.24      -2.21 [-4.44, 0.02]  0.32  [0.14, 0.7] # }"},{"path":"/reference/get_all_metrics.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the all metrics at once — get_all_metrics","title":"Calculate the all metrics at once — get_all_metrics","text":"function computes comprehensive set fairness-related performance metrics across levels sensitive attribute. includes standard classification metrics (e.g., TPR, FPR, PPV, NPV) well fairness-specific indicators like predicted positive rates error ratios.","code":""},{"path":"/reference/get_all_metrics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the all metrics at once — get_all_metrics","text":"","code":"get_all_metrics(data, outcome, group, probs, cutoff = 0.5, digits = 2)"},{"path":"/reference/get_all_metrics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the all metrics at once — get_all_metrics","text":"data Data frame containing outcome, predicted outcome, sensitive attribute outcome name outcome variable, must binary group name sensitive attribute probs name predicted outcome variable cutoff threshold predicted outcome, default 0.5 digits number digits round result , default 2","code":""},{"path":"/reference/get_all_metrics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate the all metrics at once — get_all_metrics","text":"Data frame metrics","code":""},{"path":"/reference/get_all_metrics.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate the all metrics at once — get_all_metrics","text":"useful quickly assessing multiple fairness dimensions binary classifier one step.","code":""},{"path":"/reference/get_all_metrics.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate the all metrics at once — get_all_metrics","text":"","code":"# \\donttest{ library(FairnessEval) library(dplyr) library(randomForest) data(\"mimic_preprocessed\") set.seed(123) train_data <- mimic_preprocessed |>   dplyr::filter(dplyr::row_number() <= 700) # Fit a random forest model rf_model <- randomForest::randomForest(factor(day_28_flg) ~ ., data = train_data, ntree = 1000) # Test the model on the remaining data test_data <- mimic_preprocessed |>   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\"))|>   dplyr::filter(dplyr::row_number() > 700)  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]  # Fairness evaluation # We will use sex as the sensitive attribute and day_28_flg as the outcome. # We choose threshold = 0.41 so that the overall FPR is around 5%.  # Calculate All Metrics get_all_metrics(   dat = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = 0.41 ) #>          Metric Group Female Group Male #> 1           TPR         0.62       0.38 #> 2           FPR         0.08       0.03 #> 3           PPR         0.17       0.08 #> 4           PPV         0.62       0.66 #> 5           NPV         0.92       0.90 #> 6           ACC         0.87       0.88 #> 7   Brier Score         0.09       0.08 #> 8   FN/FP Ratio         1.03       3.24 #> 9 Avg Pred Prob         0.21       0.14 # }"},{"path":"/reference/get_fairness_metrics.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute Fairness Metrics for Binary Classification — get_fairness_metrics","title":"Compute Fairness Metrics for Binary Classification — get_fairness_metrics","text":"function evaluates comprehensive set fairness metrics binary classification models across groups defined sensitive attribute (e.g., race, gender). returns unified data frame containing metric values, optionally bootstrap confidence intervals.","code":""},{"path":"/reference/get_fairness_metrics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute Fairness Metrics for Binary Classification — get_fairness_metrics","text":"","code":"get_fairness_metrics(   data,   outcome,   group,   group2 = NULL,   condition = NULL,   probs,   cutoff = 0.5,   bootstraps = 2500,   alpha = 0.05,   digits = 2 )"},{"path":"/reference/get_fairness_metrics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute Fairness Metrics for Binary Classification — get_fairness_metrics","text":"data data frame containing outcome, group, predicted probabilities. outcome name column containing true binary outcome. group name column representing sensitive attribute (e.g., race, gender). group2 Define conditional statistical parity desired. Name secondary group variable used conditional fairness analysis. condition Define conditional statistical parity desired. conditional group categorical, condition supplied must character levels condition . conditional group continuous, conditions supplied must character containing sign condition value threshold continuous variable (e.g. \"<50\", \">50\", \"<=50\", \">=50\"). probs name column predicted probabilities. cutoff Numeric threshold classification. Default 0.5. bootstraps Number bootstrap samples. Default 2500. alpha Significance level confidence intervals. Default 0.05. digits Number digits round metrics . Default 2.","code":""},{"path":"/reference/get_fairness_metrics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute Fairness Metrics for Binary Classification — get_fairness_metrics","text":"data frame evaluated fairness metrics.","code":""},{"path":[]},{"path":"/reference/get_fairness_metrics.html","id":"metrics-included-","dir":"Reference","previous_headings":"","what":"Metrics Included:","title":"Compute Fairness Metrics for Binary Classification — get_fairness_metrics","text":"Statistical Parity: Difference positive prediction rates across groups. Conditional Statistical Parity (group2 condition specified): Parity conditioned second group value. Equal Opportunity: Difference true positive rates (TPR) across groups. Predictive Equality: Difference false positive rates (FPR) across groups. Balance Positive Class: Checks whether predicted probability distributions positive outcomes similar across groups. Balance Negative Class: , negative outcomes. Predictive Parity: Difference positive predictive values (precision) across groups. Brier Score Parity: Difference Brier scores across groups. Overall Accuracy Parity: Difference overall accuracy across groups. Treatment Equality: Ratio false negatives false positives across groups.","code":""},{"path":"/reference/get_fairness_metrics.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute Fairness Metrics for Binary Classification — get_fairness_metrics","text":"","code":"# \\donttest{ library(FairnessEval) library(dplyr) library(randomForest) data(\"mimic_preprocessed\") set.seed(123) train_data <- mimic_preprocessed |>   dplyr::filter(dplyr::row_number() <= 700) # Fit a random forest model rf_model <- randomForest::randomForest(factor(day_28_flg) ~ ., data = train_data, ntree = 1000) # Test the model on the remaining data test_data <- mimic_preprocessed |>   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\"))|>   dplyr::filter(dplyr::row_number() > 700)  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]  # Fairness evaluation # We will use sex as the sensitive attribute and day_28_flg as the outcome. # We choose threshold = 0.41 so that the overall FPR is around 5%.  # Evaluate Accuracy Parity get_fairness_metrics(  data = test_data,  outcome = \"day_28_flg\",  group = \"gender\",  group2 = \"age\",  condition = \">=60\",  probs = \"pred\",  cutoff = 0.41 ) #>                                       Metric GroupFemale GroupMale Difference #> 1                         Statistical Parity        0.17      0.08       0.09 #> 2  Conditional Statistical Parity (age >=60)        0.34      0.21       0.13 #> 3                          Equal Opportunity        0.38      0.62      -0.24 #> 4                        Predictive Equality        0.08      0.03       0.05 #> 5                 Balance for Positive Class        0.46      0.37       0.09 #> 6                 Balance for Negative Class        0.15      0.10       0.05 #> 7                          Predictive Parity        0.62      0.66      -0.04 #> 8                         Brier Score Parity        0.09      0.08       0.01 #> 9                    Overall Accuracy Parity        0.87      0.88      -0.01 #> 10                        Treatment Equality        1.03      3.24      -2.21 #>       95% Diff CI Ratio 95% Ratio CI #> 1    [0.05, 0.13]  2.12 [1.48, 3.05] #> 2    [0.05, 0.21]  1.62 [1.18, 2.22] #> 3  [-0.39, -0.09]  0.61 [0.44, 0.86] #> 4    [0.02, 0.08]  2.67  [1.39, 5.1] #> 5    [0.04, 0.14]  1.24 [1.09, 1.41] #> 6    [0.03, 0.07]  1.50 [1.29, 1.74] #> 7   [-0.21, 0.13]  0.94 [0.72, 1.23] #> 8   [-0.01, 0.03]  1.12 [0.89, 1.43] #> 9   [-0.05, 0.03]  0.99 [0.94, 1.04] #> 10 [-4.35, -0.07]  0.32 [0.15, 0.68] # }"},{"path":"/reference/mimic.html","id":null,"dir":"Reference","previous_headings":"","what":"Clinical data from the MIMIC-II database for a case study on indwelling arterial catheters — mimic","title":"Clinical data from the MIMIC-II database for a case study on indwelling arterial catheters — mimic","text":"Indwelling Arterial Catheter Clinical dataset contains clinical data 1776 patients MIMIC-II clinical database. basis article: Hsu DJ, et al. association indwelling arterial catheters mortality hemodynamically stable patients respiratory failure: propensity score analysis. Chest, 148(6):1470–1476, Aug. 2015. dataset also used Raffa et al. Chapter 5 \"Data Analysis\" forthcoming book: Secondary Analysis Electronic Health Records, published Springer 2016.","code":""},{"path":"/reference/mimic.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clinical data from the MIMIC-II database for a case study on indwelling arterial catheters — mimic","text":"","code":"mimic"},{"path":"/reference/mimic.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Clinical data from the MIMIC-II database for a case study on indwelling arterial catheters — mimic","text":"data frame 1776 rows 46 variables: aline_flg Integer, indicates IAC used (1 = yes, 0 = ) icu_los_day Double, length stay ICU (days) hospital_los_day Integer, length stay hospital (days) age Double, age baseline (years) gender_num Integer, patient gender (1 = male; 0 = female) weight_first Double, first weight (kg) bmi Double, patient BMI sapsi_first Integer, first SAPS score sofa_first Integer, first SOFA score service_unit Character, type service unit (FICU, MICU, SICU) service_num Integer, service numeric value (0 = MICU FICU, 1 = SICU) day_icu_intime Character, day week ICU admission day_icu_intime_num Integer, day week ICU admission (numeric) hour_icu_intime Integer, hour ICU admission (24hr clock) hosp_exp_flg Integer, death hospital (1 = yes, 0 = ) icu_exp_flg Integer, death ICU (1 = yes, 0 = ) day_28_flg Integer, death within 28 days (1 = yes, 0 = ) mort_day_censored Double, day post ICU admission censoring death (days) censor_flg Integer, censored death (0 = death, 1 = censored) sepsis_flg Integer, sepsis present (0 = , 1 = yes) chf_flg Integer, congestive heart failure (0 = , 1 = yes) afib_flg Integer, atrial fibrillation (0 = , 1 = yes) renal_flg Integer, chronic renal disease (0 = , 1 = yes) liver_flg Integer, liver disease (0 = , 1 = yes) copd_flg Integer, chronic obstructive pulmonary disease (0 = , 1 = yes) cad_flg Integer, coronary artery disease (0 = , 1 = yes) stroke_flg Integer, stroke (0 = , 1 = yes) mal_flg Integer, malignancy (0 = , 1 = yes) resp_flg Integer, respiratory disease (non-COPD) (0 = , 1 = yes) map_1st Double, mean arterial pressure (mmHg) hr_1st Integer, heart rate temp_1st Double, temperature (F) spo2_1st Integer, S_pO_2 (percent) abg_count Integer, arterial blood gas count (number tests) wbc_first Double, first white blood cell count (K/uL) hgb_first Double, first hemoglobin (g/dL) platelet_first Integer, first platelets (K/u) sodium_first Integer, first sodium (mEq/L) potassium_first Double, first potassium (mEq/L) tco2_first Double, first bicarbonate (mEq/L) chloride_first Integer, first chloride (mEq/L) bun_first Integer, first blood urea nitrogen (mg/dL) creatinine_first Double, first creatinine (mg/dL) po2_first Integer, first PaO_2 (mmHg) pco2_first Integer, first PaCO_2 (mmHg) iv_day_1 Double, input fluids IV day 1 (mL)","code":""},{"path":"/reference/mimic.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Clinical data from the MIMIC-II database for a case study on indwelling arterial catheters — mimic","text":"https://physionet.org/content/mimic2-iaccd/1.0/","code":""},{"path":"/reference/mimic_preprocessed.html","id":null,"dir":"Reference","previous_headings":"","what":"Preprocessed Clinical Data from the MIMIC-II Database — mimic_preprocessed","title":"Preprocessed Clinical Data from the MIMIC-II Database — mimic_preprocessed","text":"version mimic dataset cleaned removing columns 10% missing data, imputing remaining missing values median, dropping columns highly correlated outcome. designed use fairness-aware machine learning tasks streamlined analysis.","code":""},{"path":"/reference/mimic_preprocessed.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preprocessed Clinical Data from the MIMIC-II Database — mimic_preprocessed","text":"","code":"mimic_preprocessed"},{"path":"/reference/mimic_preprocessed.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Preprocessed Clinical Data from the MIMIC-II Database — mimic_preprocessed","text":"data frame fewer rows variables original due preprocessing. Number rows: 1776.","code":""},{"path":"/reference/mimic_preprocessed.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Preprocessed Clinical Data from the MIMIC-II Database — mimic_preprocessed","text":"https://physionet.org/content/mimic2-iaccd/1.0/","code":""},{"path":[]}]
