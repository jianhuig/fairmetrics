[{"path":"/CONTRIBUTING.html","id":null,"dir":"","previous_headings":"","what":"Contributing to fairmetrics","title":"Contributing to fairmetrics","text":"Thank interest contributing fairmetrics package! welcome contributions, whether bug fixes, new functions/features, improvements documentation. contribute, please follow guidelines :","code":""},{"path":"/CONTRIBUTING.html","id":"how-to-contribute","dir":"","previous_headings":"","what":"How to Contribute","title":"Contributing to fairmetrics","text":"Open issue https://github.com/jianhuig/fairmetrics/issues discuss proposed changes additions development. sure tag Benjamin Smith (@benyamindsmith) Jianhui Gao (@jianhuig) issue description. proposed changes contributions approved. Fork repository create branch main. changes specific function existing file structure, please edit existing file(s). changes new function, please create new file R/ directory. Please ensure document function using roxygen2. relevant, please add tests changes tests/testthat/ directory. help ensure changes work expected. Make sure run devtools::document() update documentation. Run devtools::check() ensure changes introduce errors warnings. Commit push changes forked repository. Open pull request main branch original repository. Write brief description changes/features added. reviewing changes approval, changes merged main branch original repository. Thank contribution!","code":""},{"path":"/CONTRIBUTING.html","id":"need-help","dir":"","previous_headings":"","what":"Need Help?","title":"Contributing to fairmetrics","text":"Please open issue https://github.com/jianhuig/fairmetrics/issues tag Benjamin Smith (@benyamindsmith) Jianui Gao (@jianhuig) questions.","code":""},{"path":"/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 fairmetrics authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"/articles/fairmetrics.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Assessing Model Fairness Across Binary Protected Attributes","text":"vingette demonstrates obtain, report interpret model fairness metrics binary protected attributes fairmetrics package. illustrate case study based preprocessed version MIMIC II clinical database [1], previously studied explore relationship indwelling arterial catheters hemodynamically stable patients respiratory failure relation mortality outcomes [2]. original, unprocessed dataset publicly available PhysioNet [3]. preprocessed version dataset included fairmetrics package mimic_preprocessed used vingette.","code":""},{"path":"/articles/fairmetrics.html","id":"data-split-and-model-construction","dir":"Articles","previous_headings":"","what":"Data Split and Model Construction","title":"Assessing Model Fairness Across Binary Protected Attributes","text":"setting, construct model predict 28-day mortality (day_28_flg). , split dataset training testing sets fit random forest model. first 700 patients used training set remaining patients used testing set. model fit, used predict 28-day mortality. predicted probabilities saved new column testing data used assess model fairness.","code":"# Load required libraries library(dplyr) library(pROC) library(fairmetrics) # Set seed for reproducibility set.seed(1) # Load the MIMIC-II preprocessed data data(\"mimic_preprocessed\") # Use 700 labels to train the mimic train_data <- mimic_preprocessed %>%   filter(row_number() <= 700)  # Test the model on the remaining data test_data <- mimic %>%   filter(row_number() > 700)  # Fit a random forest model rf_model <- randomForest(factor(day_28_flg) ~ ., data = train_data, ntree = 1000) # Save model predcition test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]"},{"path":"/articles/fairmetrics.html","id":"fairness-evaluation","dir":"Articles","previous_headings":"","what":"Fairness Evaluation","title":"Assessing Model Fairness Across Binary Protected Attributes","text":"fairmetrics package used assess model fairness across binary protected attributes. means unique values protected attribute column need exactly two. evaluate fairness random forest model fit, examine patient gender binary protected attribute. Since many fairness metrics require binary predictions, threshold predicted probabilities using fixed cutoff. set threshold 0.41 maintain overall false positive rate (FPR) approximately 5%. evaluate specific fairness metrics, possible eval_* functions (list functions contained fairmetrics see ). example, interested calculating statistical parity model across gender (assumed binary), write: dataframe returned gives positive prediction rate groups defined binary protected attribute (GroupFemale GroupMale case), difference ratios groups bootstrap calculated confidence intervals estimated difference ratios. inference, can considered confidence interval contains 0 range difference 1 ratio insignificant. eval_* functions follow syntax exception eval_cond_stats_partiy() used evaluate conditional statistical parity, requires group condition . interested calculating statistical parity across male female patients aged 60 , write: calculate various fairness metrics model simultaneously, pass test data predicted results get_fairness_metrics function. result returned divides results model performance fairness criteria. fairmetrics focuses assessing fairness models accross binary protected attributes, possible work protected attributes consist two groups using “one-vs-” comparisons little bit data wrangling create appropriate columns.","code":"# Recode gender variable explicitly for readability:  test_data <- test_data %>%   mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\")) eval_stats_parity(   data = test_data,    outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = 0.41,   message = TRUE ) #> There is evidence that model does not satisfy statistical parity. #>                     Metric GroupFemale GroupMale Difference 95% Diff CI Ratio #> 1 Positive Prediction Rate        0.12      0.06       0.06 [0.02, 0.1]     2 #>   95% Ratio CI #> 1 [1.32, 3.03] eval_cond_stats_parity(   data = test_data,    outcome = \"day_28_flg\",   group = \"gender\",   group2 = \"age\",   condition = \">=60\",   probs = \"pred\",   cutoff = 0.41,   message = TRUE ) #> There is not enough evidence that the model does not satisfy #>             statistical parity. #>                     Metric GroupFemale GroupMale Difference 95% Diff CI Ratio #> 1 Positive Prediction Rate        0.24      0.17       0.07   [0, 0.14]  1.41 #>   95% Ratio CI #> 1 [0.97, 2.05] get_fairness_metrics(   data = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   group2 = \"age\",   condition = \">=60\",   probs = \"pred\",   cutoff = 0.41  ) #> $performance #>                                     Metric Group1 Group2 #> 1                 Positive Prediction Rate   0.12   0.06 #> 2     Conditional Positive Prediction Rate   0.24   0.17 #> 3                      False Negative Rate   0.56   0.70 #> 4                      False Positive Rate   0.06   0.03 #> 5            Avg. Predicted Positive Prob.   0.45   0.35 #> 6            Avg. Predicted Negative Prob.   0.15   0.11 #> 7                Positive Predictive Value   0.61   0.65 #> 8                Negative Predictive Value   0.92   0.91 #> 9                              Brier Score   0.09   0.08 #> 10                                Accuracy   0.74   0.73 #> 11 (False Negative)/(False Positive) Ratio   1.14   3.00 #>  #> $fairness #>                            Metric Difference       Diff_CI Ratio     Ratio_CI #> 1              Statistical Parity       0.06   [0.02, 0.1]  2.00 [1.33, 3.01] #> 2  Conditional Statistical Parity       0.07 [-0.01, 0.15]  1.41 [0.97, 2.06] #> 3               Equal Opportunity      -0.14 [-0.29, 0.01]  0.80 [0.63, 1.02] #> 4             Predictive Equality       0.03     [0, 0.06]  2.00 [0.95, 4.19] #> 5      Balance for Positive Class       0.10  [0.04, 0.16]  1.29   [1.1, 1.5] #> 6      Balance for Negative Class       0.04  [0.02, 0.06]  1.36  [1.16, 1.6] #> 7      Positive Predictive Parity      -0.04 [-0.24, 0.16]  0.94 [0.68, 1.29] #> 8      Negative Predictive Parity       0.01 [-0.03, 0.05]  1.01 [0.97, 1.05] #> 9              Brier Score Parity       0.01 [-0.01, 0.03]  1.12 [0.87, 1.46] #> 10        Overall Accuracy Parity       0.01 [-0.04, 0.06]  1.01 [0.94, 1.09] #> 11             Treatment Equality      -1.86 [-4.27, 0.55]  0.38  [0.16, 0.9]"},{"path":"/articles/fairmetrics.html","id":"appendix-a-confidence-interval-construction","dir":"Articles","previous_headings":"","what":"Appendix A: Confidence Interval Construction","title":"Assessing Model Fairness Across Binary Protected Attributes","text":"function get_fairness_metrics() computes Wald-type confidence intervals group-specific disparity metrics using nonparametric bootstrap. illustrate construction confidence intervals (CIs), use following example involving false positive rate (FPRFPR). Let FPR̂\\widehat{\\textrm{FPR}}_a FPRa\\textrm{FPR}_a denote estimated true FPR group =aA = . difference Δ̂FPR=FPR̂a1−FPR̂a0\\widehat{\\Delta}_{\\textrm{FPR}} = \\widehat{\\textrm{FPR}}_{a_1} - \\widehat{\\textrm{FPR}}_{a_0} satisfies (e.g., Gronsbell et al., 2018): n(Δ̂FPR−ΔFPR)→d𝒩(0,σ2) \\sqrt{n}(\\widehat{\\Delta}_{\\textrm{FPR}} - \\Delta_{\\textrm{FPR}}) \\overset{d}{\\} \\mathcal{N}(0, \\sigma^2) estimate standard error Δ̂FPR\\widehat{\\Delta}_{\\textrm{FPR}} using bootstrap resampling within groups, form Wald-style confidence interval: Δ̂FPR±z1−α/2⋅sê(Δ̂FPR) \\widehat{\\Delta}_{\\textrm{FPR}} \\pm z_{1-\\alpha/2} \\cdot \\widehat{\\textrm{se}}(\\widehat{\\Delta}_{\\textrm{FPR}}) ratios, ρ̂FPR=FPR̂a1/FPR̂a0\\widehat{\\rho}_{\\textrm{FPR}} = \\widehat{\\textrm{FPR}}_{a_1} / \\widehat{\\textrm{FPR}}_{a_0}, apply log transformation use delta method: log(ρ̂FPR)±z1−α/2⋅sê[log(ρ̂FPR)] \\log(\\widehat{\\rho}_{\\textrm{FPR}}) \\pm z_{1-\\alpha/2} \\cdot \\widehat{\\textrm{se}}\\left[\\log(\\widehat{\\rho}_{\\textrm{FPR}})\\right] Exponentiation bounds yields confidence interval ratio original scale: [exp{log(ρ̂FPR)−z1−α/2⋅sê[log(ρ̂FPR)]},exp{log(ρ̂FPR)+z1−α/2⋅sê[log(ρ̂FPR)]}]. \\left[  \\exp\\left\\{\\log(\\widehat{\\rho}_{\\textrm{FPR}}) - z_{1-\\alpha/2} \\cdot \\widehat{\\textrm{se}}\\left[\\log(\\widehat{\\rho}_{\\textrm{FPR}})\\right]\\right\\},\\  \\exp\\left\\{\\log(\\widehat{\\rho}_{\\textrm{FPR}}) + z_{1-\\alpha/2} \\cdot \\widehat{\\textrm{se}}\\left[\\log(\\widehat{\\rho}_{\\textrm{FPR}})\\right]\\right\\} \\right].","code":""},{"path":"/articles/fairmetrics.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Assessing Model Fairness Across Binary Protected Attributes","text":"Raffa, J. (2016). Clinical data MIMIC-II database case study indwelling arterial catheters (version 1.0). PhysioNet. https://doi.org/10.13026/C2NC7F. Raffa J.D., Ghassemi M., Naumann T., Feng M., Hsu D. (2016) Data Analysis. : Secondary Analysis Electronic Health Records. Springer, Cham Goldberger, ., Amaral, L., Glass, L., Hausdorff, J., Ivanov, P. C., Mark, R., … & Stanley, H. E. (2000). PhysioBank, PhysioToolkit, PhysioNet: Components new research resource complex physiologic signals. Circulation [Online]. 101 (23), pp. e215–e220. Gao, J. et al. Fair? Defining Fairness Machine Learning Health. arXiv.org https://arxiv.org/abs/2406.09307 (2024). Hort, M., Chen, Z., Zhang, J. M., Harman, M. & Sarro, F. Bias Mitigation Machine Learning Classifiers: Comprehensive survey. arXiv.org https://arxiv.org/abs/2207.07068 (2022). Hsu, D. J. et al. association indwelling arterial catheters mortality hemodynamically stable patients respiratory failure. CHEST Journal 148, 1470–1476 (2015). Gronsbell, J. L. & Cai, T. Semi-Supervised approaches efficient evaluation model prediction performance. Journal Royal Statistical Society Series B (Statistical Methodology) 80, 579–594 (2017). Efron, B. & Tibshirani, R. Bootstrap methods standard errors, confidence intervals, measures statistical accuracy. Statistical Science 1, (1986).","code":""},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Jianhui Gao. Author. Benjamin Smith. Author, maintainer. Benson Chou. Author. Jessica Gronsbell. Author.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Gao J, Smith B, Chou B, Gronsbell J (2025). fairmetrics: Fairness Evaluation Metrics Confidence Intervals Binary Protected Attributes. https://github.com/jianhuig/fairmetrics. Smith, Gao, Gronsbell (2025). fairmetrics: R package group fairness evaluation. arXiv:2506.06243. Gao et al. (2024). Fair? Defining Fairness Machine Learning Health. arXiv:2406.09307.","code":"@Manual{,   title = {fairmetrics: Fairness Evaluation Metrics with Confidence Intervals for Binary Protected Attributes},   author = {Jianhui Gao and Benjamin Smith and Benson Chou and Jessica Gronsbell},   year = {2025},   url = {https://github.com/jianhuig/fairmetrics}, } @Misc{Smith_Gao_Gronsbell_2025,   title = {fairmetrics: An R package for group fairness evaluation},   author = {Benjamin Smith and Jianhui Gao and Jessica Gronsbell},   year = {2025},   month = {jun},   note = {arXiv:2506.06243},   url = {https://arxiv.org/abs/2506.06243}, } @Misc{Gao_Chou_McCaw_Thurston_Varghese_Hong_Gronsbell_2024,   title = {What is Fair? Defining Fairness in Machine Learning for Health},   author = {Jianhui Gao and Benson Chou and Zachary R. McCaw and Hilary Thurston and Paul Varghese and Chuan Hong and Jessica Gronsbell},   year = {2024},   month = {jun},   note = {arXiv:2406.09307},   url = {https://arxiv.org/abs/2406.09307}, }"},{"path":"/index.html","id":"fairmetrics-fairness-evaluation-metrics-with-confidence-intervals-for-binary-protected-attributes-","dir":"","previous_headings":"","what":"Fairness Evaluation Metrics with Confidence Intervals for Binary Protected Attributes","title":"Fairness Evaluation Metrics with Confidence Intervals for Binary Protected Attributes","text":"collection functions computing fairness metrics machine learning statistical models, including confidence intervals metric. package supports evaluation group-level fairness criterion commonly used fairness research, particularly healthcare binary protected attributes. based overview fairness machine learning written Gao et al (2024) (https://arxiv.org/abs/2406.09307). Link online tutorial. Link preprint.","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Fairness Evaluation Metrics with Confidence Intervals for Binary Protected Attributes","text":"install latest CRAN release run: install package Github repository run:","code":"install.packages(\"fairmetrics\") devtools::install_github(\"jianhuig/fairmetrics\")"},{"path":"/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Fairness Evaluation Metrics with Confidence Intervals for Binary Protected Attributes","text":"cite package ‘fairmetrics’ publications use: Gao J, Smith B, Chou B, Gronsbell J (2025). fairmetrics: Fairness Evaluation Metrics Confidence Intervals. https://github.com/jianhuig/fairmetrics. Smith, Gao, Gronsbell (2025). fairmetrics: R package group fairness evaluation. arXiv:2506.06243. Gao et al. (2024). Fair? Defining Fairness Machine Learning Health. arXiv:2406.09307. BibTeX entry LaTeX users ","code":"bibentry(   bibtype = \"Manual\",   title = \"fairmetrics: Fairness Evaluation Metrics with Confidence Intervals for Binary Protected Attributes\",   author = c(     person(\"Jianhui\", \"Gao\"),     person(\"Benjamin\", \"Smith\"),     person(\"Benson\", \"Chou\"),     person(\"Jessica\", \"Gronsbell\")   ),   year = \"2025\",   url = \"https://github.com/jianhuig/fairmetrics\" )  bibentry(     bibtype = \"Misc\",     key = \"Smith_Gao_Gronsbell_2025\",     title = \"fairmetrics: An R package for group fairness evaluation\",     author = c(       person(\"Benjamin\", \"Smith\"),       person(\"Jianhui\", \"Gao\"),       person(\"Jessica\", \"Gronsbell\")     ),     year = \"2025\",     month = \"jun\",     note = \"arXiv:2506.06243\",     url = \"https://arxiv.org/abs/2506.06243\",     textVersion = \"Smith, Gao, and Gronsbell (2025). fairmetrics: An R package for group fairness evaluation. arXiv:2506.06243.\"    )  bibentry(   bibtype = \"Misc\",   key = \"Gao_Chou_McCaw_Thurston_Varghese_Hong_Gronsbell_2024\",   title = \"What is Fair? Defining Fairness in Machine Learning for Health\",   author = c(     person(\"Jianhui\", \"Gao\"),     person(\"Benson\", \"Chou\"),     person(\"Zachary R.\", \"McCaw\"),     person(\"Hilary\", \"Thurston\"),     person(\"Paul\", \"Varghese\"),     person(\"Chuan\", \"Hong\"),     person(\"Jessica\", \"Gronsbell\")   ),   year = \"2024\",   month = \"jun\",   note = \"arXiv:2406.09307\",   url = \"https://arxiv.org/abs/2406.09307\",   textVersion = \"Gao et al. (2024). What is Fair? Defining Fairness in Machine Learning for Health. arXiv:2406.09307.\" )"},{"path":"/index.html","id":"similar-works","dir":"","previous_headings":"","what":"Similar Works","title":"Fairness Evaluation Metrics with Confidence Intervals for Binary Protected Attributes","text":"fairness R package fairmodels R package fairlearn Python package","code":""},{"path":"/index.html","id":"references","dir":"","previous_headings":"","what":"References","title":"Fairness Evaluation Metrics with Confidence Intervals for Binary Protected Attributes","text":"Gao, J. et al. Fair? Defining Fairness Machine Learning Health. arXiv.org https://arxiv.org/abs/2406.09307 (2024).","code":""},{"path":"/reference/eval_acc_parity.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine Accuracy Parity of a Model — eval_acc_parity","title":"Examine Accuracy Parity of a Model — eval_acc_parity","text":"function assesses Accuracy Parity, fairness criterion evaluates whether overall accuracy predictive model consistent across two groups defined binary protected attribute.","code":""},{"path":"/reference/eval_acc_parity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine Accuracy Parity of a Model — eval_acc_parity","text":"","code":"eval_acc_parity(   data,   outcome,   group,   probs,   cutoff = 0.5,   confint = TRUE,   alpha = 0.05,   bootstraps = 2500,   digits = 2,   message = TRUE )"},{"path":"/reference/eval_acc_parity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine Accuracy Parity of a Model — eval_acc_parity","text":"data Data frame containing outcome, predicted outcome, binary protected attribute outcome Name outcome variable group Name binary protected attribute. Must consist two groups. probs Predicted probabilities cutoff Cutoff value predicted probabilities confint Logical indicating whether calculate confidence intervals alpha 1 - significance level confidence interval, default 0.05 bootstraps Number bootstraps use confidence intervals digits Number digits round results , default 2 message Logical; TRUE (default), prints textual summary fairness evaluation. works confint TRUE.","code":""},{"path":"/reference/eval_acc_parity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine Accuracy Parity of a Model — eval_acc_parity","text":"list containing following elements: Accuracy Group 1 Accuracy Group 2 Difference accuracy Ratio accuracy confidence intervals computed (confint = TRUE): vector length 2 containing lower upper bounds 95% confidence interval difference accuracy vector length 2 containing lower upper bounds 95% confidence interval ratio accurac","code":""},{"path":[]},{"path":"/reference/eval_acc_parity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Examine Accuracy Parity of a Model — eval_acc_parity","text":"","code":"# \\donttest{ library(fairmetrics) library(dplyr) #>  #> Attaching package: ‘dplyr’ #> The following objects are masked from ‘package:stats’: #>  #>     filter, lag #> The following objects are masked from ‘package:base’: #>  #>     intersect, setdiff, setequal, union library(magrittr) library(randomForest) #> randomForest 4.7-1.2 #> Type rfNews() to see new features/changes/bug fixes. #>  #> Attaching package: ‘randomForest’ #> The following object is masked from ‘package:dplyr’: #>  #>     combine data(\"mimic_preprocessed\") set.seed(123) train_data <- mimic_preprocessed %>%   dplyr::filter(dplyr::row_number() <= 700) # Fit a random forest model rf_model <- randomForest::randomForest(factor(day_28_flg) ~ ., data = train_data, ntree = 1000) # Test the model on the remaining data test_data <- mimic_preprocessed %>%   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\")) %>%   dplyr::filter(dplyr::row_number() > 700)  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]  # Fairness evaluation # We will use sex as the protected attribute and day_28_flg as the outcome. # We choose threshold = 0.41 so that the overall FPR is around 5%.  # Evaluate Accuracy Parity eval_acc_parity(   data = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = 0.41 ) #> There is not enough evidence that the model does not satisfy #>             accuracy parity. #>     Metric GroupFemale GroupMale Difference   95% Diff CI Ratio 95% Ratio CI #> 1 Accuracy        0.87      0.88      -0.01 [-0.05, 0.03]  0.99 [0.94, 1.04] # }"},{"path":"/reference/eval_bs_parity.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine Brier Score Parity of a Model — eval_bs_parity","title":"Examine Brier Score Parity of a Model — eval_bs_parity","text":"function evaluates Brier Score Parity, fairness measure checks whether Brier score (measure calibration probabilistic predictions) similar across across two groups defined binary protected attribute. Brier score parity ensures model's predicted probabilities equally well calibrated across subpopulations.","code":""},{"path":"/reference/eval_bs_parity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine Brier Score Parity of a Model — eval_bs_parity","text":"","code":"eval_bs_parity(   data,   outcome,   group,   probs,   confint = TRUE,   alpha = 0.05,   bootstraps = 2500,   digits = 2,   message = TRUE )"},{"path":"/reference/eval_bs_parity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine Brier Score Parity of a Model — eval_bs_parity","text":"data Data frame containing outcome, predicted outcome, binary protected attribute outcome Name outcome variable group Name binary protected attribute. Must consist two groups. probs Predicted probabilities confint Logical indicating whether calculate confidence intervals alpha 1 - significance level confidence interval, default 0.05 bootstraps Number bootstraps use confidence intervals digits Number digits round results , default 2 message Logical; TRUE (default), prints textual summary fairness evaluation. works confint TRUE.","code":""},{"path":"/reference/eval_bs_parity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine Brier Score Parity of a Model — eval_bs_parity","text":"list containing following elements: Brier Score Group 1 Brier Score Group 2 Difference Brier Score Ratio Brier Score confidence intervals computed (confint = TRUE): vector length 2 containing lower upper bounds 95% confidence interval difference Brier Score vector length 2 containing lower upper bounds 95% confidence interval ratio Brier Score","code":""},{"path":[]},{"path":"/reference/eval_bs_parity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Examine Brier Score Parity of a Model — eval_bs_parity","text":"","code":"# \\donttest{ library(fairmetrics) library(dplyr) library(magrittr) library(randomForest) data(\"mimic_preprocessed\") set.seed(123) train_data <- mimic_preprocessed %>%   dplyr::filter(dplyr::row_number() <= 700) # Fit a random forest model rf_model <- randomForest::randomForest(factor(day_28_flg) ~ ., data = train_data, ntree = 1000) # Test the model on the remaining data test_data <- mimic_preprocessed %>%   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\")) %>%   dplyr::filter(dplyr::row_number() > 700)  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]  # Fairness evaluation # We will use sex as the protected attribute and day_28_flg as the outcome.  # Evaluate Brier Score Parity eval_bs_parity(   data = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\" ) #> There is not enough evidence that the model does not satisfy #>             Brier Score parity. #>        Metric GroupFemale GroupMale Difference   95% Diff CI Ratio 95% Ratio CI #> 1 Brier Score        0.09      0.08       0.01 [-0.01, 0.03]  1.12 [0.89, 1.43] # }"},{"path":"/reference/eval_cond_acc_equality.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine Conditional Use Accuracy Equality of a Model — eval_cond_acc_equality","title":"Examine Conditional Use Accuracy Equality of a Model — eval_cond_acc_equality","text":"function evaluates Conditional Use Accuracy Equality, fairness criterion requires predictive performance similar across across two groups - defined binary protected attribute - model makes positive negative predictions.","code":""},{"path":"/reference/eval_cond_acc_equality.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine Conditional Use Accuracy Equality of a Model — eval_cond_acc_equality","text":"","code":"eval_cond_acc_equality(   data,   outcome,   group,   probs,   cutoff = 0.5,   confint = TRUE,   alpha = 0.05,   bootstraps = 2500,   digits = 2,   message = TRUE )"},{"path":"/reference/eval_cond_acc_equality.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine Conditional Use Accuracy Equality of a Model — eval_cond_acc_equality","text":"data Data frame containing outcome, predicted outcome, binary protected attribute outcome Name outcome variable, must binary group Name binary protected attribute. Must consist two groups. probs Name predicted outcome variable cutoff Threshold predicted outcome, default 0.5 confint Whether compute 95% confidence interval, default TRUE alpha 1 - significance level confidence interval, default 0.05 bootstraps Number bootstrap samples, default 2500 digits Number digits round results , default 2 message Logical; TRUE (default), prints textual summary fairness evaluation. works confint TRUE.","code":""},{"path":"/reference/eval_cond_acc_equality.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine Conditional Use Accuracy Equality of a Model — eval_cond_acc_equality","text":"list containing following elements: PPV_Group1: Positive Predictive Value first group PPV_Group2: Positive Predictive Value second group PPV_Diff: Difference Positive Predictive Value NPV_Group1: Negative Predictive Value first group NPV_Group2: Negative Predictive Value second group NPV_Diff: Difference Negative Predictive Value confidence intervals computed (confint = TRUE): PPV_Diff_CI: vector length 2 containing lower upper bounds 95% confidence interval difference Positive Predictive Value NPV_Diff_CI: vector length 2 containing lower upper bounds 95% confidence interval difference Negative Predictive Value","code":""},{"path":[]},{"path":"/reference/eval_cond_acc_equality.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Examine Conditional Use Accuracy Equality of a Model — eval_cond_acc_equality","text":"","code":"# \\donttest{ library(fairmetrics) library(dplyr) library(magrittr) library(randomForest) data(\"mimic_preprocessed\") set.seed(123) train_data <- mimic_preprocessed %>%   dplyr::filter(dplyr::row_number() <= 700) # Fit a random forest model rf_model <- randomForest::randomForest(factor(day_28_flg) ~ ., data = train_data, ntree = 1000) # Test the model on the remaining data test_data <- mimic_preprocessed %>%   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\")) %>%   dplyr::filter(dplyr::row_number() > 700)  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]  # Fairness evaluation # We will use sex as the protected attribute and day_28_flg as the outcome. # We choose threshold = 0.41 so that the overall FPR is around 5%.  # Evaluate Conditional Use Accuracy Equality eval_cond_acc_equality(   data = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = 0.41 ) #> There is not enough evidence that the model does not satisfy #>             conditional use accuracy equality. #>     Metric GroupFemale GroupMale  Difference                       95% CI #> 1 PPV; NPV  0.62; 0.92 0.66; 0.9 -0.04; 0.02 [-0.24, 0.16]; [-0.02, 0.06] # }"},{"path":"/reference/eval_cond_stats_parity.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine Conditional Statistical Parity of a Model — eval_cond_stats_parity","title":"Examine Conditional Statistical Parity of a Model — eval_cond_stats_parity","text":"function evaluates conditional statistical parity, measures fairness comparing positive prediction rates across two groups defined binary protected attribute within defined subgroup population. useful scenarios fairness evaluated context-specific way—e.g., within particular hospital unit age bracket. Conditional statistical parity refinement standard statistical parity. Instead comparing prediction rates across groups entire dataset, restricts comparison specified subset population, defined conditioning variable.","code":""},{"path":"/reference/eval_cond_stats_parity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine Conditional Statistical Parity of a Model — eval_cond_stats_parity","text":"","code":"eval_cond_stats_parity(   data,   outcome,   group,   group2,   condition,   probs,   confint = TRUE,   cutoff = 0.5,   bootstraps = 2500,   alpha = 0.05,   message = TRUE,   digits = 2 )"},{"path":"/reference/eval_cond_stats_parity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine Conditional Statistical Parity of a Model — eval_cond_stats_parity","text":"data Data frame containing outcome, predicted outcome, binary protected attribute outcome Name outcome variable, must binary group Name binary protected attribute. Must consist two groups. group2 Name group condition condition conditional group categorical, condition supplied must character levels condition . conditional group continuous, conditions supplied must character containing sign condition value threshold continuous variable (e.g. \"<50\", \">50\", \"<=50\", \">=50\"). probs Name predicted outcome variable confint Whether compute 95% confidence interval, default TRUE cutoff Threshold predicted outcome, default 0.5 bootstraps Number bootstrap samples, default 2500 alpha 1 - significance level confidence interval, default 0.05 message Logical; TRUE (default), prints textual summary fairness evaluation. works confint TRUE. digits Number digits round results , default 2","code":""},{"path":"/reference/eval_cond_stats_parity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine Conditional Statistical Parity of a Model — eval_cond_stats_parity","text":"list containing following elements: Conditions: conditions used calculate conditional PPR PPR_Group1: Positive Prediction Rate first group PPR_Group2: Positive Prediction Rate second group PPR_Diff: Difference Positive Prediction Rate PPR_Ratio: Ratio Positive Prediction Rate confidence intervals computed (confint = TRUE): PPR_Diff_CI: vector length 2 containing lower upper bounds 95% confidence interval difference Positive Prediction Rate PPR_Ratio_CI: vector length 2 containing lower upper bounds 95% confidence interval ratio Positive Prediction Rate","code":""},{"path":"/reference/eval_cond_stats_parity.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Examine Conditional Statistical Parity of a Model — eval_cond_stats_parity","text":"function supports categorical continuous conditioning variables. continuous variables, can supply threshold expression like \"<50\" \">=75\" condition parameter.","code":""},{"path":[]},{"path":"/reference/eval_cond_stats_parity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Examine Conditional Statistical Parity of a Model — eval_cond_stats_parity","text":"","code":"# \\donttest{ library(fairmetrics) library(dplyr) library(magrittr) library(randomForest) data(\"mimic_preprocessed\") set.seed(123) train_data <- mimic_preprocessed %>%   dplyr::filter(dplyr::row_number() <= 700) # Fit a random forest model rf_model <- randomForest::randomForest(factor(day_28_flg) ~ ., data = train_data, ntree = 1000) # Test the model on the remaining data test_data <- mimic_preprocessed %>%   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\")) %>%   dplyr::filter(dplyr::row_number() > 700)  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]  # Fairness evaluation # We will use sex as the protected attribute and day_28_flg as the outcome. # We choose threshold = 0.41 so that the overall FPR is around 5%.  # Evaluate Conditional Statistical Parity  eval_cond_stats_parity(   data = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   group2 = \"service_unit\",   condition = \"MICU\",   probs = \"pred\",   cutoff = 0.41 ) #> There is not enough evidence that the model does not satisfy #>             statistical parity. #>                     Metric GroupFemale GroupMale Difference   95% Diff CI Ratio #> 1 Positive Prediction Rate        0.15       0.1       0.05 [-0.01, 0.11]   1.5 #>   95% Ratio CI #> 1  [0.87, 2.6] # }"},{"path":"/reference/eval_eq_odds.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine Equalized Odds of a Predictive Model — eval_eq_odds","title":"Examine Equalized Odds of a Predictive Model — eval_eq_odds","text":"function evaluates whether predictive model satisfies Equalized Odds criterion comparing False Negative Rates (FNR) False Positive Rates (FPR) across two groups defined binary protected attribute. reports rate group, differences, ratios, bootstrap-based confidence regions. Bonferroni-corrected union test used test whether model violates Equalized Odds criterion.","code":""},{"path":"/reference/eval_eq_odds.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine Equalized Odds of a Predictive Model — eval_eq_odds","text":"","code":"eval_eq_odds(   data,   outcome,   group,   probs,   cutoff = 0.5,   confint = TRUE,   bootstraps = 2500,   alpha = 0.05,   digits = 2,   message = TRUE )"},{"path":"/reference/eval_eq_odds.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine Equalized Odds of a Predictive Model — eval_eq_odds","text":"data data frame containing true binary outcomes, predicted probabilities, binary protected attribute. outcome string specifying name binary outcome variable data. group Name binary protected attribute. Must consist two groups. probs string specifying name variable containing predicted probabilities risk scores. cutoff numeric value used threshold predicted probabilities binary predictions; defaults 0.5. confint Whether compute 95% confidence interval, default TRUE. bootstraps integer specifying number bootstrap resamples constructing confidence intervals; vdefaults 2500. alpha Significance level (1 - alpha) confidence interval; defaults 0.05. digits Number decimal places round numeric results; defaults 2. message Logical; TRUE (default), prints textual summary fairness evaluation. works confint TRUE.","code":""},{"path":"/reference/eval_eq_odds.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine Equalized Odds of a Predictive Model — eval_eq_odds","text":"data frame summarizing group disparities FNR FPR following columns: Metric: reported metrics (\"FNR; FPR\"). Group1: Estimated FNR FPR first group. Group2: Estimated FNR FPR second group. Difference: Differences FNR FPR, computed Group1 - Group2. 95% CR: Bonferroni-adjusted confidence regions differences. Ratio: Ratios FNR FPR, computed Group1 / Group2. 95% CR: Bonferroni-adjusted confidence regions ratios.","code":""},{"path":"/reference/eval_eq_odds.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Examine Equalized Odds of a Predictive Model — eval_eq_odds","text":"","code":"# \\donttest{ library(fairmetrics) library(dplyr) library(magrittr) library(randomForest) data(\"mimic_preprocessed\") set.seed(123) train_data <- mimic_preprocessed %>%   dplyr::filter(dplyr::row_number() <= 700) # Fit a random forest model rf_model <- randomForest::randomForest(factor(day_28_flg) ~ .,   data = train_data, ntree = 1000 ) # Test the model on the remaining data test_data <- mimic_preprocessed %>%   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\")) %>%   dplyr::filter(dplyr::row_number() > 700)  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]  # Fairness evaluation # We will use sex as the protected attribute and day_28_flg as the outcome. # We choose threshold = 0.41 so that the overall FPR is around 5%.  # Evaluate Equalized Odds eval_eq_odds(   data = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = 0.41 ) #> There is evidence that model does not satisfy equalized odds. #>     Metric Group Female Group Male  Difference                  95% Diff CI #> 1 FNR; FPR   0.38; 0.08 0.62; 0.03 -0.24; 0.05 [-0.41, -0.07]; [0.01, 0.09] #>        Ratio              95% Ratio CI #> 1 0.61; 2.67 [0.42, 0.9]; [1.26, 5.66] # }"},{"path":"/reference/eval_eq_opp.html","id":null,"dir":"Reference","previous_headings":"","what":"Evaluate Equal Opportunity Compliance of a Predictive Model — eval_eq_opp","title":"Evaluate Equal Opportunity Compliance of a Predictive Model — eval_eq_opp","text":"function evaluates fairness predictive model respect Equal Opportunity criterion, requires False Negative Rate (FNR) comparable across groups defined binary protected attribute. function quantifies disparities FNR two groups provides absolute difference ratio, along confidence intervals obtained via bootstrapping.","code":""},{"path":"/reference/eval_eq_opp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Evaluate Equal Opportunity Compliance of a Predictive Model — eval_eq_opp","text":"","code":"eval_eq_opp(   data,   outcome,   group,   probs,   cutoff = 0.5,   confint = TRUE,   bootstraps = 2500,   alpha = 0.05,   digits = 2,   message = TRUE )"},{"path":"/reference/eval_eq_opp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Evaluate Equal Opportunity Compliance of a Predictive Model — eval_eq_opp","text":"data data frame containing true binary outcomes, predicted probabilities, binary protected attribute. outcome string specifying name binary outcome variable data. group group Name binary protected attribute. Must consist two groups. probs string specifying name variable containing predicted probabilities risk scores. cutoff numeric value used threshold predicted probabilities binary decisions; defaults 0.5. confint Whether compute 95% confidence interval, default TRUE. bootstraps integer specifying number bootstrap resamples constructing confidence intervals; defaults 2500. alpha Significance level constructing (1 - alpha) confidence interval; defaults 0.05. digits Integer indicating number decimal places round results ; defaults 2. message Logical; TRUE (default), prints textual summary fairness evaluation. works confint TRUE.","code":""},{"path":"/reference/eval_eq_opp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Evaluate Equal Opportunity Compliance of a Predictive Model — eval_eq_opp","text":"data frame summarizing FNR-based group disparity metrics following columns: Metric label indicating reported fairness criterion. Group1 Estimated FNR FPR first group. Group2 Estimated FNR FPR second group. Difference difference FNR two groups, computed FNR Group1 minus FNR Group2. 95% Diff CI (1 - alpha) confidence interval FNR difference. Ratio ratio FNRs Group1 Group2, computed FNR Group1 divided FNR Group2. 95% Ratio CI corresponding confidence interval FNR ratio.","code":""},{"path":"/reference/eval_eq_opp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Evaluate Equal Opportunity Compliance of a Predictive Model — eval_eq_opp","text":"","code":"# \\donttest{ library(fairmetrics) library(dplyr) library(magrittr) library(randomForest) data(\"mimic_preprocessed\") set.seed(123) train_data <- mimic_preprocessed %>%   dplyr::filter(dplyr::row_number() <= 700) # Fit a random forest model rf_model <- randomForest::randomForest(factor(day_28_flg) ~ .,   data =     train_data, ntree = 1000 ) # Test the model on the remaining data test_data <- mimic_preprocessed %>%   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\")) %>%   dplyr::filter(dplyr::row_number() > 700)  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]  # Fairness evaluation # We will use sex as the protected attribute and day_28_flg as the outcome. # We choose threshold = 0.41 so that the overall FPR is around 5%.  # Evaluate Equal Opportunity Compliance eval_eq_opp(   data = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = 0.41 ) #> There is evidence that model does not satisfy equal opportunity. #>                Metric GroupFemale GroupMale Difference    95% Diff CI Ratio #> 1 False Negative Rate        0.38      0.62      -0.24 [-0.39, -0.09]  0.61 #>   95% Ratio CI #> 1 [0.44, 0.86] # }"},{"path":"/reference/eval_neg_class_bal.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine Balance for Negative Class of a Model — eval_neg_class_bal","title":"Examine Balance for Negative Class of a Model — eval_neg_class_bal","text":"function evaluates Balance Negative Class, fairness criterion checks whether model assigns similar predicted probabilities among individuals whose true outcome negative (.e. \\(Y = 0\\)) accross groups defined binary protected attribute.","code":""},{"path":"/reference/eval_neg_class_bal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine Balance for Negative Class of a Model — eval_neg_class_bal","text":"","code":"eval_neg_class_bal(   data,   outcome,   group,   probs,   confint = TRUE,   alpha = 0.05,   bootstraps = 2500,   digits = 2,   message = TRUE )"},{"path":"/reference/eval_neg_class_bal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine Balance for Negative Class of a Model — eval_neg_class_bal","text":"data Data frame containing outcome, predicted outcome, binary protected attribute attribute outcome Name outcome variable group Name protected attribute. Must consist two groups. probs Predicted probabilities confint Logical indicating whether calculate confidence intervals alpha 1 - significance level confidence interval, default 0.05 bootstraps Number bootstraps use confidence intervals digits Number digits round results , default 2 message Logical; TRUE (default), prints textual summary fairness evaluation. works confint TRUE.","code":""},{"path":"/reference/eval_neg_class_bal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine Balance for Negative Class of a Model — eval_neg_class_bal","text":"list containing following elements: Average predicted probability Group 1 Average predicted probability Group 2 Difference average predicted probability Ratio average predicted probability confidence intervals computed (confint = TRUE): vector length 2 containing lower upper bounds 95% confidence interval difference average predicted probability vector length 2 containing lower upper bounds 95% confidence interval ratio average predicted probability","code":""},{"path":[]},{"path":"/reference/eval_neg_class_bal.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Examine Balance for Negative Class of a Model — eval_neg_class_bal","text":"","code":"# \\donttest{ library(fairmetrics) library(dplyr) library(magrittr) library(randomForest) data(\"mimic_preprocessed\") set.seed(123) train_data <- mimic_preprocessed %>%   dplyr::filter(dplyr::row_number() <= 700) # Fit a random forest model rf_model <- randomForest::randomForest(factor(day_28_flg) ~ ., data = train_data, ntree = 1000) # Test the model on the remaining data test_data <- mimic_preprocessed %>%   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\")) %>%   dplyr::filter(dplyr::row_number() > 700)  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]  # Fairness evaluation # We will use sex as the protected attribute and day_28_flg as the outcome.  # Evaluate Balance for Negative Class eval_neg_class_bal(   data = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\" ) #> There is enough evidence that the model does not satisfy #>             balance for negative class. #>                 Metric GroupFemale GroupMale Difference  95% Diff CI Ratio #> 1 Avg. Predicted Prob.        0.15       0.1       0.05 [0.03, 0.07]   1.5 #>   95% Ratio CI #> 1 [1.29, 1.75] # }"},{"path":"/reference/eval_neg_pred_parity.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine Negative Predictive Parity of a Model — eval_neg_pred_parity","title":"Examine Negative Predictive Parity of a Model — eval_neg_pred_parity","text":"function evaluates negative predictive predictive parity, key fairness criterion compares Negative Predictive Value (NPV) groups defined binary protected attribute. words, assesses whether, among individuals predicted negative, probability truly negative equal across subgroups.","code":""},{"path":"/reference/eval_neg_pred_parity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine Negative Predictive Parity of a Model — eval_neg_pred_parity","text":"","code":"eval_neg_pred_parity(   data,   outcome,   group,   probs,   cutoff = 0.5,   confint = TRUE,   bootstraps = 2500,   alpha = 0.05,   digits = 2,   message = TRUE )"},{"path":"/reference/eval_neg_pred_parity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine Negative Predictive Parity of a Model — eval_neg_pred_parity","text":"data Data frame containing outcome, predicted outcome, protected attribute outcome Name outcome variable, must binary group Name protected attribute. Must consist two groups. probs Name predicted outcome variable cutoff Threshold predicted outcome, default 0.5 confint Whether compute 95% confidence interval, default TRUE bootstraps Number bootstrap samples, default 2500 alpha 1 - significance level confidence interval, default 0.05 digits Number digits round results , default 2 message Logical; TRUE (default), prints textual summary fairness evaluation. works confint TRUE.","code":""},{"path":"/reference/eval_neg_pred_parity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine Negative Predictive Parity of a Model — eval_neg_pred_parity","text":"list containing following elements: NPV_Group1: Negative Predictive Value first group NPV_Group2: Negative Predictive Value second group NPV_Diff: Difference Negative Predictive Value NPV_Ratio: Ratio Negative Predictive Value confidence intervals computed (confint = TRUE): NPV_Diff_CI: vector length 2 containing lower upper bounds 95% confidence interval difference Negative Predictive Value NPV_Ratio_CI: vector length 2 containing lower upper bounds 95% confidence interval ratio Negative Predictive Value","code":""},{"path":[]},{"path":"/reference/eval_neg_pred_parity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Examine Negative Predictive Parity of a Model — eval_neg_pred_parity","text":"","code":"# \\donttest{ library(fairmetrics) library(dplyr) library(magrittr) library(randomForest) data(\"mimic_preprocessed\") set.seed(123) train_data <- mimic_preprocessed %>%   dplyr::filter(dplyr::row_number() <= 700) # Fit a random forest model rf_model <- randomForest::randomForest(factor(day_28_flg) ~ ., data = train_data, ntree = 1000) # Test the model on the remaining data test_data <- mimic_preprocessed %>%   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\")) %>%   dplyr::filter(dplyr::row_number() > 700)  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]  # Fairness evaluation # We will use sex as the protected attribute and day_28_flg as the outcome. # We choose threshold = 0.41 so that the overall FPR is around 5%.  # Evaluate Negative Predictive Parity eval_neg_pred_parity(   data = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = 0.41 ) #> There is not enough evidence that the model does not satisfy negative predictive parity. #>                      Metric GroupFemale GroupMale Difference   95% Diff CI #> 1 Negative Predictive Value        0.92       0.9       0.02 [-0.15, 0.19] #>   Ratio 95% Ratio CI #> 1  1.02 [0.78, 1.34] # }"},{"path":"/reference/eval_pos_class_bal.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine Balance for the Positive Class of a Model — eval_pos_class_bal","title":"Examine Balance for the Positive Class of a Model — eval_pos_class_bal","text":"function evaluates Balance Positive Class, fairness criterion checks whether model assigns similar predicted probabilities among individuals whose true outcome positive (.e. \\(Y = 1\\)) accross groups defined binary protected attribute.","code":""},{"path":"/reference/eval_pos_class_bal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine Balance for the Positive Class of a Model — eval_pos_class_bal","text":"","code":"eval_pos_class_bal(   data,   outcome,   group,   probs,   confint = TRUE,   alpha = 0.05,   bootstraps = 2500,   digits = 2,   message = TRUE )"},{"path":"/reference/eval_pos_class_bal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine Balance for the Positive Class of a Model — eval_pos_class_bal","text":"data Data frame containing outcome, predicted outcome, binary protected attribute outcome Name outcome variable group Name protected attribute. Must consist two groups. probs Predicted probabilities confint Logical indicating whether calculate confidence intervals alpha 1 - significance level confidence interval, default 0.05 bootstraps Number bootstraps use confidence intervals digits Number digits round results , default 2 message Logical; TRUE (default), prints textual summary fairness evaluation. works confint TRUE.","code":""},{"path":"/reference/eval_pos_class_bal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine Balance for the Positive Class of a Model — eval_pos_class_bal","text":"list containing following elements: Average predicted probability Group 1 Average predicted probability Group 2 Difference average predicted probability Ratio average predicted probability confidence intervals computed (confint = TRUE): vector length 2 containing lower upper bounds 95% confidence interval difference average predicted probability vector length 2 containing lower upper bounds 95% confidence interval ratio average predicted probability","code":""},{"path":[]},{"path":"/reference/eval_pos_class_bal.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Examine Balance for the Positive Class of a Model — eval_pos_class_bal","text":"","code":"# \\donttest{ library(fairmetrics) library(dplyr) library(magrittr) library(randomForest) data(\"mimic_preprocessed\") set.seed(123) train_data <- mimic_preprocessed %>%   dplyr::filter(dplyr::row_number() <= 700) # Fit a random forest model rf_model <- randomForest::randomForest(factor(day_28_flg) ~ ., data = train_data, ntree = 1000) # Test the model on the remaining data test_data <- mimic_preprocessed %>%   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\")) %>%   dplyr::filter(dplyr::row_number() > 700)  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]  # Fairness evaluation # We will use sex as the protected attribute and day_28_flg as the outcome.  # Evaluate Balance for Positive Class eval_pos_class_bal(   data = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\" ) #> There is evidence that the model does not satisfy #>             balance for positive class. #>                 Metric GroupFemale GroupMale Difference  95% Diff CI Ratio #> 1 Avg. Predicted Prob.        0.46      0.37       0.09 [0.04, 0.14]  1.24 #>   95% Ratio CI #> 1 [1.09, 1.42] # }"},{"path":"/reference/eval_pos_pred_parity.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine Positive Predictive Parity of a Model — eval_pos_pred_parity","title":"Examine Positive Predictive Parity of a Model — eval_pos_pred_parity","text":"function evaluates positive predictive predictive parity, key fairness criterion compares Positive Predictive Value (PPV) groups defined binary protected attribute. words, assesses whether, among individuals predicted positive, probability truly positive equal across subgroups.","code":""},{"path":"/reference/eval_pos_pred_parity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine Positive Predictive Parity of a Model — eval_pos_pred_parity","text":"","code":"eval_pos_pred_parity(   data,   outcome,   group,   probs,   cutoff = 0.5,   confint = TRUE,   bootstraps = 2500,   alpha = 0.05,   digits = 2,   message = TRUE )"},{"path":"/reference/eval_pos_pred_parity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine Positive Predictive Parity of a Model — eval_pos_pred_parity","text":"data Data frame containing outcome, predicted outcome, binary protected attribute outcome Name outcome variable, must binary group Name protected attribute. Must consist two groups. probs Name predicted outcome variable cutoff Threshold predicted outcome, default 0.5 confint Whether compute 95% confidence interval, default TRUE bootstraps Number bootstrap samples, default 2500 alpha 1 - significance level confidence interval, default 0.05 digits Number digits round results , default 2 message Logical; TRUE (default), prints textual summary fairness evaluation. works confint TRUE.","code":""},{"path":"/reference/eval_pos_pred_parity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine Positive Predictive Parity of a Model — eval_pos_pred_parity","text":"list containing following elements: PPV_Group1: Positive Predictive Value first group PPV_Group2: Positive Predictive Value second group PPV_Diff: Difference Positive Predictive Value PPV_Ratio: Ratio Positive Predictive Value confidence intervals computed (confint = TRUE): PPV_Diff_CI: vector length 2 containing lower upper bounds 95% confidence interval difference Positive Predictive Value PPV_Ratio_CI: vector length 2 containing lower upper bounds 95% confidence interval ratio Positive Predictive Value","code":""},{"path":[]},{"path":"/reference/eval_pos_pred_parity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Examine Positive Predictive Parity of a Model — eval_pos_pred_parity","text":"","code":"# \\donttest{ library(fairmetrics) library(dplyr) library(magrittr) library(randomForest) data(\"mimic_preprocessed\") set.seed(123) train_data <- mimic_preprocessed %>%   dplyr::filter(dplyr::row_number() <= 700) # Fit a random forest model rf_model <- randomForest::randomForest(factor(day_28_flg) ~ ., data = train_data, ntree = 1000) # Test the model on the remaining data test_data <- mimic_preprocessed %>%   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\")) %>%   dplyr::filter(dplyr::row_number() > 700)  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]  # Fairness evaluation # We will use sex as the protected attribute and day_28_flg as the outcome. # We choose threshold = 0.41 so that the overall FPR is around 5%.  # Evaluate Positive Predictive Parity eval_pos_pred_parity(   data = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = 0.41 ) #> There is not enough evidence that the model does not satisfy positive predictive parity. #>                      Metric GroupFemale GroupMale Difference   95% Diff CI #> 1 Positive Predictive Value        0.62      0.66      -0.04 [-0.21, 0.13] #>   Ratio 95% Ratio CI #> 1  0.94 [0.72, 1.23] # }"},{"path":"/reference/eval_pred_equality.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine Predictive Equality of a Model — eval_pred_equality","title":"Examine Predictive Equality of a Model — eval_pred_equality","text":"function evaluates predictive equality, fairness metric compares False Positive Rate (FPR) groups defined binary protected attribute. assesses whether individuals different groups equally likely incorrectly flagged positive , fact, negative.","code":""},{"path":"/reference/eval_pred_equality.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine Predictive Equality of a Model — eval_pred_equality","text":"","code":"eval_pred_equality(   data,   outcome,   group,   probs,   cutoff = 0.5,   confint = TRUE,   alpha = 0.05,   bootstraps = 2500,   digits = 2,   message = TRUE )"},{"path":"/reference/eval_pred_equality.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine Predictive Equality of a Model — eval_pred_equality","text":"data Data frame containing outcome, predicted outcome, binary protected attribute outcome Name outcome variable, must binary group Name protected attribute. Must consist two groups. probs Name predicted outcome variable cutoff Threshold predicted outcome, default 0.5 confint Whether compute 95% confidence interval, default TRUE alpha 1 - significance level confidence interval, default 0.05 bootstraps Number bootstrap samples, default 2500 digits Number digits round results , default 2 message Logical; TRUE (default), prints textual summary fairness evaluation. works confint TRUE.","code":""},{"path":"/reference/eval_pred_equality.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine Predictive Equality of a Model — eval_pred_equality","text":"list containing following elements: FPR_Group1: False Positive Rate first group FPR_Group2: False Positive Rate second group FPR_Diff: Difference False Positive Rate FPR_Ratio: Ratio False Positive Rate confidence intervals computed (confint = TRUE): FPR_Diff_CI: vector length 2 containing lower upper bounds 95% confidence interval difference False Positive Rate FPR_Ratio_CI: vector length 2 containing lower upper bounds 95% confidence interval ratio False Positive Rate","code":""},{"path":[]},{"path":"/reference/eval_pred_equality.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Examine Predictive Equality of a Model — eval_pred_equality","text":"","code":"# \\donttest{ library(fairmetrics) library(dplyr) library(magrittr) library(randomForest) data(\"mimic_preprocessed\") set.seed(123) train_data <- mimic_preprocessed %>%   dplyr::filter(dplyr::row_number() <= 700) # Fit a random forest model rf_model <- randomForest::randomForest(factor(day_28_flg) ~ ., data = train_data, ntree = 1000) # Test the model on the remaining data test_data <- mimic_preprocessed %>%   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\")) %>%   dplyr::filter(dplyr::row_number() > 700)  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]  # Fairness evaluation # We will use sex as the protectedR attribute and day_28_flg as the outcome. # We choose threshold = 0.41 so that the overall FPR is around 5%.  # Evaluate Predictive Equality eval_pred_equality(   data = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = 0.41 ) #> There is evidence that model does not satisfy predictive #>             equality. #>                Metric GroupFemale GroupMale Difference  95% Diff CI Ratio #> 1 False Positive Rate        0.08      0.03       0.05 [0.02, 0.08]  2.67 #>   95% Ratio CI #> 1 [1.38, 5.15] # }"},{"path":"/reference/eval_stats_parity.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine Statistical Parity of a Model — eval_stats_parity","title":"Examine Statistical Parity of a Model — eval_stats_parity","text":"function assesses statistical parity - also known demographic parity - predictions binary classifier across two groups defined protected attribute. Statistical parity compares rate different groups receive positive prediction, irrespective true outcome. reports Positive Prediction Rate (PPR) group, differences, ratios, bootstrap-based confidence regions.","code":""},{"path":"/reference/eval_stats_parity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine Statistical Parity of a Model — eval_stats_parity","text":"","code":"eval_stats_parity(   data,   outcome,   group,   probs,   cutoff = 0.5,   confint = TRUE,   bootstraps = 2500,   alpha = 0.05,   digits = 2,   message = TRUE )"},{"path":"/reference/eval_stats_parity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine Statistical Parity of a Model — eval_stats_parity","text":"data Data frame containing outcome, predicted outcome, protected attribute outcome Name outcome variable, must binary group Name protected attribute. Must consist two groups. probs Name predicted outcome variable cutoff Threshold predicted outcome, default 0.5 confint Whether compute 95% confidence interval, default TRUE bootstraps Number bootstrap samples, default 2500 alpha 1 - significance level confidence interval, default 0.05 digits Number digits round results , default 2 message Logical; TRUE (default), prints textual summary fairness evaluation. works confint TRUE.","code":""},{"path":"/reference/eval_stats_parity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine Statistical Parity of a Model — eval_stats_parity","text":"list containing following elements: PPR_Group1: Positive Prediction Rate first group PPR_Group2: Positive Prediction Rate second group PPR_Diff: Difference Positive Prediction Rate PPR_Ratio: ratio Positive Prediction Rate two groups. confidence intervals computed (confint = TRUE): PPR_Diff_CI: vector length 2 containing lower upper bounds 95% confidence interval difference Positive Prediction Rate PPR_Ratio_CI: vector length 2 containing lower upper bounds 95% confidence interval ratio Positive Prediction Rate","code":""},{"path":[]},{"path":"/reference/eval_stats_parity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Examine Statistical Parity of a Model — eval_stats_parity","text":"","code":"# \\donttest{ library(fairmetrics) library(dplyr) library(magrittr) library(randomForest) data(\"mimic_preprocessed\") set.seed(123) train_data <- mimic_preprocessed %>%   dplyr::filter(dplyr::row_number() <= 700) # Fit a random forest model rf_model <- randomForest::randomForest(factor(day_28_flg) ~ ., data = train_data, ntree = 1000) # Test the model on the remaining data test_data <- mimic_preprocessed %>%   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\")) %>%   dplyr::filter(dplyr::row_number() > 700)  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]  # Fairness evaluation # We will use sex as the protected attribute and day_28_flg as the outcome. # We choose threshold = 0.41 so that the overall FPR is around 5%.  # Evaluate Statistical Parity eval_stats_parity(   data = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = 0.41 ) #> There is evidence that model does not satisfy statistical parity. #>                     Metric GroupFemale GroupMale Difference  95% Diff CI Ratio #> 1 Positive Prediction Rate        0.17      0.08       0.09 [0.05, 0.13]  2.12 #>   95% Ratio CI #> 1 [1.48, 3.05] # }"},{"path":"/reference/eval_treatment_equality.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine Treatment Equality of a Model — eval_treatment_equality","title":"Examine Treatment Equality of a Model — eval_treatment_equality","text":"function evaluates Treatment Equality, fairness criterion assesses whether ratio false negatives false positives similar across groups defined binary protected attribute. Treatment Equality ensures model disproportionately favor disadvantage group terms relative frequency missed detections (false negatives) versus false alarms (false positives).","code":""},{"path":"/reference/eval_treatment_equality.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine Treatment Equality of a Model — eval_treatment_equality","text":"","code":"eval_treatment_equality(   data,   outcome,   group,   probs,   cutoff = 0.5,   confint = TRUE,   alpha = 0.05,   bootstraps = 2500,   digits = 2,   message = TRUE )"},{"path":"/reference/eval_treatment_equality.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine Treatment Equality of a Model — eval_treatment_equality","text":"data Data frame containing outcome, predicted outcome, binary protected attribute outcome Name outcome variable group group Name binary protected attribute. Must consist two groups. probs Predicted probabilities cutoff Cutoff value predicted probabilities confint Logical indicating whether calculate confidence intervals alpha 1 - significance level confidence interval, default 0.05 bootstraps Number bootstraps use confidence intervals digits Number digits round results , default 2 message Logical; TRUE (default), prints textual summary fairness evaluation. works confint TRUE.","code":""},{"path":"/reference/eval_treatment_equality.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine Treatment Equality of a Model — eval_treatment_equality","text":"list containing following elements: False Negative / False Positive ratio Group 1 False Negative / False Positive ratio Group 2 Difference False Negative / False Positive ratio Ratio False Negative / False Positive ratio confidence intervals computed (confint = TRUE): vector length 2 containing lower upper bounds 95% confidence interval difference False Negative / False Positive ratio vector length 2 containing lower upper bounds 95% confidence interval ratio False Negative / False Positive ratio","code":""},{"path":[]},{"path":"/reference/eval_treatment_equality.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Examine Treatment Equality of a Model — eval_treatment_equality","text":"","code":"# \\donttest{ library(fairmetrics) library(dplyr) library(magrittr) library(randomForest) # Data for tests data(\"mimic_preprocessed\") set.seed(123) train_data <- mimic_preprocessed %>%   dplyr::filter(dplyr::row_number() <= 700) # Fit a random forest model rf_model <- randomForest::randomForest(factor(day_28_flg) ~ ., data = train_data, ntree = 1000) # Test the model on the remaining data test_data <- mimic_preprocessed %>%   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\")) %>%   dplyr::filter(dplyr::row_number() > 700)  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]  # Fairness evaluation # We will use sex as the protected attribute and day_28_flg as the outcome.  # Evaluate Treatment Equality eval_treatment_equality(   data = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = 0.41,   confint = TRUE,   alpha = 0.05,   bootstraps = 2500,   digits = 2,   message = FALSE ) #>                                    Metric GroupFemale GroupMale Difference #> 1 (False Negative)/(False Positive) Ratio        1.03      3.24      -2.21 #>     95% Diff CI Ratio 95% Ratio CI #> 1 [-4.44, 0.02]  0.32  [0.14, 0.7] # }"},{"path":"/reference/get_fairness_metrics.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute Fairness Metrics for Binary Classification — get_fairness_metrics","title":"Compute Fairness Metrics for Binary Classification — get_fairness_metrics","text":"Computes comprehensive set fairness metrics binary classification models, disaggregated binary protected attribute (e.g., race, gender). Optionally, conditional fairness can evaluated using second attribute specified condition. function also computes corresponding performance metrics used fairness calculations.","code":""},{"path":"/reference/get_fairness_metrics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute Fairness Metrics for Binary Classification — get_fairness_metrics","text":"","code":"get_fairness_metrics(   data,   outcome,   group,   group2 = NULL,   condition = NULL,   probs,   confint = TRUE,   cutoff = 0.5,   bootstraps = 2500,   alpha = 0.05,   digits = 2 )"},{"path":"/reference/get_fairness_metrics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute Fairness Metrics for Binary Classification — get_fairness_metrics","text":"data data frame containing outcome, group, predicted probabilities. outcome name column containing true binary outcome. group name column representing binary protected attribute (e.g., race, gender). group2 Define conditional statistical parity desired. Name secondary group variable used conditional fairness analysis. condition Define conditional statistical parity desired. conditional group categorical, condition supplied must character levels condition . conditional group continuous, conditions supplied must character containing sign condition value threshold continuous variable (e.g. \"<50\", \">50\", \"<=50\", \">=50\"). probs name column predicted probabilities. confint Logical indicating whether calculate confidence intervals. cutoff Numeric threshold classification. Default 0.5. bootstraps Number bootstrap samples. Default 2500. alpha Significance level confidence intervals. Default 0.05. digits Number digits round metrics . Default 2.","code":""},{"path":"/reference/get_fairness_metrics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute Fairness Metrics for Binary Classification — get_fairness_metrics","text":"list containing: performance Data frame performance metrics group. fairness Data frame computed fairness metrics optional confidence intervals.","code":""},{"path":"/reference/get_fairness_metrics.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute Fairness Metrics for Binary Classification — get_fairness_metrics","text":"results returned list two data frames: performance: Contains performance metrics (e.g., TPR, FPR, PPV) group. fairness: Contains group-level fairness metrics (e.g., disparities ratios), confidence intervals (specified).","code":""},{"path":"/reference/get_fairness_metrics.html","id":"fairness-metrics-included-","dir":"Reference","previous_headings":"","what":"Fairness Metrics Included:","title":"Compute Fairness Metrics for Binary Classification — get_fairness_metrics","text":"Statistical Parity: Difference positive prediction rates across groups. Conditional Statistical Parity (group2 condition specified): Parity conditioned second group value. Equal Opportunity: Difference true positive rates (TPR) across groups. Predictive Equality: Difference false positive rates (FPR) across groups. Balance Positive Class: Checks whether predicted probability distributions positive outcomes similar across groups. Balance Negative Class: , negative outcomes. Positive Predictive Parity: Difference positive predictive values (precision) across groups. Negative Predictive Parity: Difference negative predictive values across groups. Brier Score Parity: Difference Brier scores across groups. Overall Accuracy Parity: Difference overall accuracy across groups. Treatment Equality: Ratio false negatives false positives across groups.","code":""},{"path":"/reference/get_fairness_metrics.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute Fairness Metrics for Binary Classification — get_fairness_metrics","text":"","code":"# \\donttest{ library(fairmetrics) library(dplyr) library(randomForest) library(magrittr) data(\"mimic_preprocessed\") set.seed(123) train_data <- mimic_preprocessed %>%   dplyr::filter(dplyr::row_number() <= 700) # Fit a random forest model rf_model <- randomForest::randomForest(factor(day_28_flg) ~ ., data = train_data, ntree = 1000) # Test the model on the remaining data test_data <- mimic_preprocessed %>%   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\"))%>%   dplyr::filter(dplyr::row_number() > 700)  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]  # Fairness evaluation # We will use sex as the protected attribute and day_28_flg as the outcome. # We choose threshold = 0.41 so that the overall FPR is around 5%.  # Get Fairness Metrics get_fairness_metrics(  data = test_data,  outcome = \"day_28_flg\",  group = \"gender\",  group2 = \"age\",  condition = \">=60\",  probs = \"pred\",  confint = TRUE,  cutoff = 0.41,  alpha = 0.05 ) #> $performance #>                                     Metric Group1 Group2 #> 1                 Positive Prediction Rate   0.17   0.08 #> 2     Conditional Positive Prediction Rate   0.34   0.21 #> 3                      False Negative Rate   0.38   0.62 #> 4                      False Positive Rate   0.08   0.03 #> 5            Avg. Predicted Positive Prob.   0.46   0.37 #> 6            Avg. Predicted Negative Prob.   0.15   0.10 #> 7                Positive Predictive Value   0.62   0.66 #> 8                Negative Predictive Value   0.92   0.90 #> 9                              Brier Score   0.09   0.08 #> 10                                Accuracy   0.87   0.88 #> 11 (False Negative)/(False Positive) Ratio   1.03   3.24 #>  #> $fairness #>                            Metric Difference        Diff_CI Ratio     Ratio_CI #> 1              Statistical Parity       0.09   [0.05, 0.13]  2.12 [1.48, 3.05] #> 2  Conditional Statistical Parity       0.13   [0.05, 0.21]  1.62 [1.17, 2.24] #> 3               Equal Opportunity      -0.24 [-0.39, -0.09]  0.61 [0.44, 0.86] #> 4             Predictive Equality       0.05   [0.02, 0.08]  2.67 [1.38, 5.15] #> 5      Balance for Positive Class       0.09   [0.04, 0.14]  1.24 [1.09, 1.42] #> 6      Balance for Negative Class       0.05   [0.03, 0.07]  1.50 [1.29, 1.75] #> 7      Positive Predictive Parity      -0.04  [-0.21, 0.13]  0.94 [0.71, 1.24] #> 8      Negative Predictive Parity       0.02  [-0.02, 0.06]  1.02 [0.98, 1.07] #> 9              Brier Score Parity       0.01  [-0.01, 0.03]  1.12 [0.89, 1.43] #> 10        Overall Accuracy Parity      -0.01  [-0.05, 0.03]  0.99 [0.94, 1.04] #> 11             Treatment Equality      -2.21  [-4.44, 0.02]  0.32  [0.14, 0.7] #>  # }"},{"path":"/reference/mimic.html","id":null,"dir":"Reference","previous_headings":"","what":"Clinical data from the MIMIC-II database for a case study on indwelling arterial catheters — mimic","title":"Clinical data from the MIMIC-II database for a case study on indwelling arterial catheters — mimic","text":"Indwelling Arterial Catheter Clinical dataset contains clinical data 1776 patients MIMIC-II clinical database. basis article: Hsu DJ, et al. association indwelling arterial catheters mortality hemodynamically stable patients respiratory failure: propensity score analysis. Chest, 148(6):1470–1476, Aug. 2015. dataset also used Raffa et al. Chapter 5 \"Data Analysis\" forthcoming book: Secondary Analysis Electronic Health Records, published Springer 2016.","code":""},{"path":"/reference/mimic.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clinical data from the MIMIC-II database for a case study on indwelling arterial catheters — mimic","text":"","code":"mimic"},{"path":"/reference/mimic.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Clinical data from the MIMIC-II database for a case study on indwelling arterial catheters — mimic","text":"data frame 1776 rows 46 variables: aline_flg Integer, indicates IAC used (1 = yes, 0 = ) icu_los_day Double, length stay ICU (days) hospital_los_day Integer, length stay hospital (days) age Double, age baseline (years) gender_num Integer, patient gender (1 = male; 0 = female) weight_first Double, first weight (kg) bmi Double, patient BMI sapsi_first Integer, first SAPS score sofa_first Integer, first SOFA score service_unit Character, type service unit (FICU, MICU, SICU) service_num Integer, service numeric value (0 = MICU FICU, 1 = SICU) day_icu_intime Character, day week ICU admission day_icu_intime_num Integer, day week ICU admission (numeric) hour_icu_intime Integer, hour ICU admission (24hr clock) hosp_exp_flg Integer, death hospital (1 = yes, 0 = ) icu_exp_flg Integer, death ICU (1 = yes, 0 = ) day_28_flg Integer, death within 28 days (1 = yes, 0 = ) mort_day_censored Double, day post ICU admission censoring death (days) censor_flg Integer, censored death (0 = death, 1 = censored) sepsis_flg Integer, sepsis present (0 = , 1 = yes) chf_flg Integer, congestive heart failure (0 = , 1 = yes) afib_flg Integer, atrial fibrillation (0 = , 1 = yes) renal_flg Integer, chronic renal disease (0 = , 1 = yes) liver_flg Integer, liver disease (0 = , 1 = yes) copd_flg Integer, chronic obstructive pulmonary disease (0 = , 1 = yes) cad_flg Integer, coronary artery disease (0 = , 1 = yes) stroke_flg Integer, stroke (0 = , 1 = yes) mal_flg Integer, malignancy (0 = , 1 = yes) resp_flg Integer, respiratory disease (non-COPD) (0 = , 1 = yes) map_1st Double, mean arterial pressure (mmHg) hr_1st Integer, heart rate temp_1st Double, temperature (F) spo2_1st Integer, S_pO_2 (percent) abg_count Integer, arterial blood gas count (number tests) wbc_first Double, first white blood cell count (K/uL) hgb_first Double, first hemoglobin (g/dL) platelet_first Integer, first platelets (K/u) sodium_first Integer, first sodium (mEq/L) potassium_first Double, first potassium (mEq/L) tco2_first Double, first bicarbonate (mEq/L) chloride_first Integer, first chloride (mEq/L) bun_first Integer, first blood urea nitrogen (mg/dL) creatinine_first Double, first creatinine (mg/dL) po2_first Integer, first PaO_2 (mmHg) pco2_first Integer, first PaCO_2 (mmHg) iv_day_1 Double, input fluids IV day 1 (mL)","code":""},{"path":"/reference/mimic.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Clinical data from the MIMIC-II database for a case study on indwelling arterial catheters — mimic","text":"https://physionet.org/content/mimic2-iaccd/1.0/","code":""},{"path":"/reference/mimic_preprocessed.html","id":null,"dir":"Reference","previous_headings":"","what":"Preprocessed Clinical Data from the MIMIC-II Database — mimic_preprocessed","title":"Preprocessed Clinical Data from the MIMIC-II Database — mimic_preprocessed","text":"version mimic dataset cleaned removing columns 10% missing data, imputing remaining missing values median, dropping columns highly correlated outcome. designed use fairness-aware machine learning tasks streamlined analysis.","code":""},{"path":"/reference/mimic_preprocessed.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preprocessed Clinical Data from the MIMIC-II Database — mimic_preprocessed","text":"","code":"mimic_preprocessed"},{"path":"/reference/mimic_preprocessed.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Preprocessed Clinical Data from the MIMIC-II Database — mimic_preprocessed","text":"data frame fewer variables original due preprocessing. Number rows: 1776.","code":""},{"path":"/reference/mimic_preprocessed.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Preprocessed Clinical Data from the MIMIC-II Database — mimic_preprocessed","text":"https://physionet.org/content/mimic2-iaccd/1.0/","code":""},{"path":[]}]
