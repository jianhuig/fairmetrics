[{"path":"/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 jianhuig Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"/articles/FairnessEval.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Binary Protected Attributes","text":"illustrate usage FairnessEval package case study using publicly available dataset 1,776 ICU patients MIMIC-II clinical database, focusing predicting 28-day mortality evaluating disparities model performance across sex. following packages used analysis along FairnessEval package: “Data Preprocessing” section discusses dataset, handling missing data, model construction standard predictive model evaluation train-test splitting binary classification. “Fairness Evaluation” section shows evaluate model’s fairness toward binary protected attributes FairnessEval package. Finally, “Practical Considerations” section discusses inherent trade-offs different fairness definitions clinical settings highlights challenges enforcing fairness outcome prevalence differs across groups. Details regarding construction 95% confidence intervals provided appendix.","code":"# Packages we are using for the analysis library(dplyr) library(corrplot) library(randomForest) library(pROC) library(SpecsVerification) library(kableExtra) library(naniar) # Our package library(FairnessEval)"},{"path":"/articles/FairnessEval.html","id":"data-preprocessing","dir":"Articles","previous_headings":"","what":"Data Preprocessing","title":"Binary Protected Attributes","text":"dataset used analysis MIMIC II clinical database, previously studied explore relationship indwelling arterial catheters hemodynamically stable patients respiratory failure relation mortality outcomes. includes 46 variables cover demographics clinical characteristics (including white blood cell count, heart rate ICU stays others) along 28-day mortality indicator (day_28_flg) 1,776 patients. data made publicly available PhysioNet available FairnessEval package mimic dataset.","code":""},{"path":"/articles/FairnessEval.html","id":"handling-missing-data","dir":"Articles","previous_headings":"Data Preprocessing","what":"Handling Missing Data","title":"Binary Protected Attributes","text":"first assess extent missingness dataset. variable, calculate total number percentage missing values naniar::miss_var_summary(). ensure data quality, following procedure applied handle missing data: Removal variables 10% missing values. Three variables 10% missing values: body mass index (bmi; 26.2%), first partial pressure oxygen (po2_first; 10.5%), first partial pressure carbon dioxide (pco2_first; 10.5%). Remaining missing values imputed using median value variable belong . additionally remove sepsis_flg column dataset contains single unique value across observations. Since provide useful information model training, excluded.","code":"# Loading mimic dataset  # (available in FairnessEval) data(\"mimic\")   missing_data_summary<- naniar::miss_var_summary(mimic, digits= 3)  kableExtra::kable(missing_data_summary, booktabs = TRUE, escape = FALSE) %>%   kableExtra::kable_styling(     latex_options = \"hold_position\"   ) # Remove columns with more than 10% missing values columns_to_remove <- missing_data_summary %>%   dplyr::filter(pct_miss > 10) %>%   dplyr::pull(variable)    mimic <- dplyr::select(mimic,                         -dplyr::one_of(columns_to_remove)                        )  # Impute remaining missing values with median mimic <- mimic %>%    dplyr::mutate(     dplyr::across(       dplyr::where(~any(is.na(.))),                    ~ifelse(is.na(.), median(., na.rm = TRUE), .)                   )     ) # Identify columns that have only one unique value cols_with_one_value <- sapply(mimic, function(x) length(unique(x)) == 1) # Subset the dataframe to remove these columns mimic <- mimic[, !cols_with_one_value]"},{"path":"/articles/FairnessEval.html","id":"model-building","dir":"Articles","previous_headings":"Data Preprocessing","what":"Model Building","title":"Binary Protected Attributes","text":"training model, remove variables directly correlated patient outcomes prevent data leakage. particular, inspect correlation matrix numeric features exclude variables hospital expiration flag (hosp_exp_flg), ICU expiration flag (icu_exp_flg), mortality censoring day (mort_day_censored), censoring flag (censor_flg), strongly associated patient outcomes.  split dataset training testing sets. first 700 patients used training set remaining patients used testing set. hyperparameters random forest (RF) model set use 1000 trees random sampling 6 variables split, determined square root number predictors. training, overall area receiver operating characteristic curve (AUC) model test set 0.90 overall accuracy model test set 0.88.","code":"# Remove columns that are highly correlated with the outcome variable corrplot::corrplot(cor(select_if(mimic, is.numeric)), method = \"color\", tl.cex = 0.5) mimic <- mimic %>%    dplyr::select(-c(\"hosp_exp_flg\", \"icu_exp_flg\", \"mort_day_censored\", \"censor_flg\")) # Use 700 labels to train the mimic train_data <- mimic %>%    dplyr::filter(     dplyr::row_number() <= 700     )  # Fit a random forest model set.seed(123) rf_model <- randomForest::randomForest(factor(day_28_flg) ~ ., data = train_data, ntree = 1000)  # Test the model on the remaining data test_data <- mimic %>%    dplyr::filter(     dplyr::row_number() > 700     )  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[,2]  # Check the AUC roc_obj <- pROC::roc(test_data$day_28_flg, test_data$pred) #> Setting levels: control = 0, case = 1 #> Setting direction: controls < cases roc_auc <- pROC::auc(roc_obj) roc_auc #> Area under the curve: 0.8971"},{"path":"/articles/FairnessEval.html","id":"fairness-evaluation","dir":"Articles","previous_headings":"","what":"Fairness Evaluation","title":"Binary Protected Attributes","text":"fairness evaluation, use results testing set focus patient gender binary protected attribute relationship 28-day mortality (day_28_flg) outcome. Since many fairness metrics require binary predictions, threshold predicted probabilities using fixed cutoff. set threshold 0.41 maintain overall false positive rate (FPR) approximately 5% calculate various fairness metrics model, pass test data predicted results get_fairness_metrics function. outputted fairness metrics note: Independence likely violated, evidenced statistical parity metric shows 9% difference (95% CI: [5%, 13%]) ratio 2.12 (95% CI: [1.49, 3.04]) females males. measures independence category indicate model predicts significantly higher mortality rate females, even conditioning age. respect separation, observe metrics show significant disparities. instance, equal opportunity shows -24% difference (95% CI: [-39%, -9%]) false negative rate (FNR) females males. indicates model less likely detect males risk mortality compared females. hand, sufficiency criterion satisfied predictive parity shows significant difference males females (difference: -4%, 95% CI: [-21%, 13%]; ratio: 0.94, 95% CI: [0.72, 1.23]). suggests given prediction, actual mortality rates similar males females. Among additional metrics assess calibration discrimination, Brier Score Parity Overall Accuracy Equality show significant disparities. However, Treatment Equality shows statistically significant difference (difference: –2.21, 95% CI: [–4.35, –0.07]; ratio: 0.32, 95% CI: [0.15, 0.68]), indicating males substantially higher false negative false positive ratio compared females. suggests male patients likely missed model relative incorrectly flagged.","code":"# Recode gender variable explicitly for readability:   test_data <- test_data %>%   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\")) # control the overall false positive rate (FPR) at 5% by setting a threshold.  cut_off <- 0.41  test_data %>%   dplyr::mutate(pred = ifelse(pred > cut_off, 1, 0)) %>%   dplyr::filter(day_28_flg == 0) %>%   dplyr::summarise(fpr = mean(pred)) #>          fpr #> 1 0.05054945 fairness_result <- FairnessEval::get_fairness_metrics(   data = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   group2 = \"age\",   condition = \">=60\",   probs = \"pred\",   cutoff = cut_off  )  kableExtra::kable(fairness_result, booktabs = TRUE, escape = FALSE) %>%   kableExtra::kable_styling(full_width = FALSE) %>%   kableExtra::pack_rows(\"Independence-based criteria\", 1, 2) %>%   kableExtra::pack_rows(\"Separation-based criteria\", 3, 6) %>%   kableExtra::pack_rows(\"Sufficiency-based criteria\", 7, 7) %>%   kableExtra::pack_rows(\"Other criteria\", 8, 10) %>%   kableExtra::kable_styling(     full_width = FALSE,     font_size = 10,         # control font size manually     latex_options = \"hold_position\"   )"},{"path":"/articles/FairnessEval.html","id":"practical-considerations","dir":"Articles","previous_headings":"","what":"Practical Considerations","title":"Binary Protected Attributes","text":"example, separation sufficiency-based metrics important. Separation ensures patients true risk mortality accurately identified across protected groups. reduces risk systematically -detecting individuals may require immediate intervention particular subpopulation. hand, sufficiency also valuable, ensures immediate intervention based solely predicted medical needs rather protected attributes. However, 28-day-mortality unlikely marginally independent sex estimated mortality rates differ females (19%) males (14%). consequence, possible model simultaneously satisfy one fairness category due previously discussed incompatibilities. points complexity fairness considerations clinical settings, one must prioritize criteria relevant. Given different mortality rates males females, enforcing independence likely advisable, blind model true mortality rate differences. model used example violates separation criteria, result higher rates undetected mortality risk among male patients potential delays interventions care. main text, refer user recent survey bias mitigation strategies can potentially help reduce observed disparities separation-based metrics.","code":""},{"path":"/articles/FairnessEval.html","id":"appendix","dir":"Articles","previous_headings":"","what":"Appendix","title":"Binary Protected Attributes","text":"illustrate construction confidence intervals (CIs), use following example involving false positive rate (FPRFPR).","code":""},{"path":"/articles/FairnessEval.html","id":"difference-ci","dir":"Articles","previous_headings":"Appendix","what":"Difference CI","title":"Binary Protected Attributes","text":"Let FPR̂\\widehat{\\textrm{FPR}}_a FPRa\\textrm{FPR}_a represent estimated true FPRs group =aA = . construct Wald-type CIs fact n(FPR̂−FPRa)\\sqrt{n} \\left(\\widehat{\\textrm{FPR}}_a - \\textrm{FPR}_a \\right) asymptotic normal mean 0 =aA = (see Gronsbell et al., 2018). Let Δ̂FPR=FPR̂a1−FPR̂a0\\widehat{\\Delta}_{\\textrm{FPR}} = \\widehat{\\textrm{FPR}}_{a_1} - \\widehat{\\textrm{FPR}}_{a_0} represent estimated difference group a1a_1 a0a_0. follows n(Δ̂FPR−ΔFPR)\\sqrt{n}\\left(\\widehat{\\Delta}_{\\textrm{FPR}} - \\Delta_{\\textrm{FPR}}\\right) asymptotic normal mean 0. estimate standard error Δ̂FPR\\widehat{\\Delta}_{\\textrm{FPR}}, utilize non-parametric bootstrap (Efron Tibshirani, 1986). bootstrap iteration bb {1,…,B}\\left\\{1,\\dots, B\\right\\}, sample data replacement within group calculate Δ̂FPR(b)\\widehat{\\Delta}_{\\textrm{FPR}}^{(b)}, assuming resampling within groups fixed group sizes. estimated standard error empirical standard error difference: sê[Δ̂FPR]=1B−1∑b=1B(Δ̂FPR(b)−1B∑b=1BΔ̂FPR(b))2. \\widehat{\\textrm{se}}\\left[\\widehat{\\Delta}_{\\textrm{FPR}}\\right] = \\sqrt{\\frac{1}{B-1}\\sum_{b=1}^B \\left(\\widehat{\\Delta}_{\\textrm{FPR}}^{(b)} - \\frac{1}{B} \\sum_{b=1}^B \\widehat{\\Delta}_{\\textrm{FPR}}^{(b)}\\right)^2}. corresponding Wald-based 100(1−α)%100(1-\\alpha)\\% CI given Δ̂FPR±z1−α/2*sê[Δ̂FPR]\\widehat{\\Delta}_{\\textrm{FPR}} \\pm z_{1-\\alpha/2} * \\widehat{\\textrm{se}}\\left[\\widehat{\\Delta}_{\\textrm{FPR}}\\right], z1−α/2z_{1-\\alpha/2} 100(1−α/2)100(1-\\alpha/2) percentile standard normal distribution.","code":""},{"path":"/articles/FairnessEval.html","id":"ratio-ci","dir":"Articles","previous_headings":"Appendix","what":"Ratio CI","title":"Binary Protected Attributes","text":"construct 100(1−α)%100(1-\\alpha)\\% Wald-type CI ratio ρ̂FPR=FPR̂a1FPR̂a0\\widehat{\\rho}_{\\textrm{FPR}} = \\frac{\\widehat{\\textrm{FPR}}_{a_1}}{\\widehat{\\textrm{FPR}}_{a_0}}, note n[log(ρ̂FPR)−log(ρFPR)]\\sqrt{n}\\left[\\log\\left(\\widehat{\\rho}_{\\textrm{FPR}}\\right)- \\log\\left(\\rho_{\\textrm{FPR}}\\right)\\right] asymptotic normal mean 0. bootstrap iteration bb {1,…,B}\\left\\{1,\\dots, B\\right\\}, sample data replacement within group calculate log[ρ̂FPR(b)]\\log\\left[\\widehat{\\rho}_{\\textrm{FPR}}^{(b)}\\right]. estimated standard error log ratio : sê[log(ρ̂FPR)]=1B−1∑b=1B{log[ρ̂FPR(b)]−1B∑b=1Blog[ρ̂FPR(b)]}2. \\widehat{\\textrm{se}}\\left[\\log\\left(\\widehat{\\rho}_{\\textrm{FPR}}\\right)\\right] = \\sqrt{\\frac{1}{B-1}\\sum_{b=1}^B \\left\\{\\log\\left[\\widehat{\\rho}_{\\textrm{FPR}}^{(b)}\\right] - \\frac{1}{B} \\sum_{b=1}^B \\log\\left[\\widehat{\\rho}_{\\textrm{FPR}}^{(b)}\\right]\\right\\}^2}. corresponding 100(1−α)%100(1-\\alpha)\\% CI given exp{log[ρ̂FPR]±z1−α/2*sê[log(ρ̂FPR)]}\\exp \\left\\{\\log\\left[\\widehat{\\rho}_{\\textrm{FPR}}\\right] \\pm z_{1-\\alpha/2} * \\widehat{\\textrm{se}}\\left[\\log\\left(\\widehat{\\rho}_{\\textrm{FPR}}\\right)\\right]\\right\\}, z1−α/2z_{1-\\alpha/2} 100(1−α/2)100(1-\\alpha/2) percentile standard normal distribution.","code":""},{"path":"/articles/FairnessEval.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Binary Protected Attributes","text":"Gao, J. et al. Fair? Defining Fairness Machine Learning Health. arXiv.org https://arxiv.org/abs/2406.09307 (2024). Clinical data MIMIC-II database case study indwelling arterial catheters v1.0. https://physionet.org/content/mimic2-iaccd/1.0/ (2016). Gronsbell, J. L. & Cai, T. Semi-Supervised approaches efficient evaluation model prediction performance. Journal Royal Statistical Society Series B (Statistical Methodology) 80, 579–594 (2017). Efron, B. & Tibshirani, R. Bootstrap methods standard errors, confidence intervals, measures statistical accuracy. Statistical Science 1, (1986).","code":""},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Jianhui Gao. Author, maintainer. Benjamin Smith. Author. Benson Chou. Author. Jessica Gronsbell. Author.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Gao J, Smith B, Chou B, Gronsbell J (2025). FairnessEval: Fairness evaluation metrics confidence intervals. R package version 1.0.0, https://jianhuig.github.io/FairnessTutorial/.","code":"@Manual{,   title = {FairnessEval: Fairness evaluation metrics with confidence intervals},   author = {Jianhui Gao and Benjamin Smith and Benson Chou and Jessica Gronsbell},   year = {2025},   note = {R package version 1.0.0},   url = {https://jianhuig.github.io/FairnessTutorial/}, }"},{"path":"/index.html","id":"fairnesseval-fairness-evaluation-metrics-with-confidence-intervals-","dir":"","previous_headings":"","what":"Fairness evaluation metrics with confidence intervals","title":"Fairness evaluation metrics with confidence intervals","text":"collection functions computing fairness metrics machine learning statistical models, including confidence intervals metric. package supports evaluation group-level fairness criteron commonly used fairness research, particularly healthcare. based overview fairness machine learning written Gao et al (2024) (https://arxiv.org/abs/2406.09307). Link online tutorial. Link preprint.","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Fairness evaluation metrics with confidence intervals","text":"","code":"devtools::install_github(repo = \"https://github.com/jianhuig/FairnessTutorial\")"},{"path":"/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Fairness evaluation metrics with confidence intervals","text":"cite package ‘FairnessEval’ publications use: Gao J, Smith B, Chou B, Gronsbell J (2025). FairnessEval: Fairness evaluation metrics confidence intervals. R package version 1.0.0, https://jianhuig.github.io/FairnessEval/. BibTeX entry LaTeX users ","code":"@Manual{,     title = {FairnessEval: Fairness evaluation metrics with confidence intervals},     author = {Jianhui Gao and Benjamin Smith and Benson Chou and Jessica Gronsbell},     year = {2025},     note = {R package version 1.0.0},     url = {https://jianhuig.github.io/FairnessTutorial/},   }"},{"path":"/index.html","id":"similar-works","dir":"","previous_headings":"","what":"Similar Works","title":"Fairness evaluation metrics with confidence intervals","text":"fairness R package","code":""},{"path":"/index.html","id":"references","dir":"","previous_headings":"","what":"References","title":"Fairness evaluation metrics with confidence intervals","text":"Gao, J. et al. Fair? Defining Fairness Machine Learning Health. arXiv.org https://arxiv.org/abs/2406.09307 (2024).","code":""},{"path":"/reference/eval_acc_parity.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine Accuracy Parity of a Model — eval_acc_parity","title":"Examine Accuracy Parity of a Model — eval_acc_parity","text":"function assesses Accuracy Parity, fairness criterion evaluates whether overall accuracy predictive model consistent across different groups.","code":""},{"path":"/reference/eval_acc_parity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine Accuracy Parity of a Model — eval_acc_parity","text":"","code":"eval_acc_parity(   data,   outcome,   group,   probs,   cutoff = 0.5,   confint = TRUE,   alpha = 0.05,   bootstraps = 2500,   digits = 2,   message = TRUE )"},{"path":"/reference/eval_acc_parity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine Accuracy Parity of a Model — eval_acc_parity","text":"data Data frame containing outcome, predicted outcome, sensitive attribute outcome Name outcome variable group Name sensitive attribute probs Predicted probabilities cutoff Cutoff value predicted probabilities confint Logical indicating whether calculate confidence intervals alpha 1 - significance level confidence interval, default 0.05 bootstraps Number bootstraps use confidence intervals digits Number digits round results , default 2 message Whether print results, default TRUE","code":""},{"path":"/reference/eval_acc_parity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine Accuracy Parity of a Model — eval_acc_parity","text":"list containing following elements: Accuracy Group 1 Accuracy Group 2 Difference accuracy Ratio accuracy confidence intervals computed (confint = TRUE): vector length 2 containing lower upper bounds 95% confidence interval difference accuracy vector length 2 containing lower upper bounds 95% confidence interval ratio accurac","code":""},{"path":[]},{"path":"/reference/eval_acc_parity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Examine Accuracy Parity of a Model — eval_acc_parity","text":"","code":"# \\donttest{ library(FairnessEval) library(dplyr) #>  #> Attaching package: ‘dplyr’ #> The following objects are masked from ‘package:stats’: #>  #>     filter, lag #> The following objects are masked from ‘package:base’: #>  #>     intersect, setdiff, setequal, union library(randomForest) #> randomForest 4.7-1.2 #> Type rfNews() to see new features/changes/bug fixes. #>  #> Attaching package: ‘randomForest’ #> The following object is masked from ‘package:dplyr’: #>  #>     combine data(\"mimic_preprocessed\") set.seed(123) train_data <- mimic_preprocessed |>   dplyr::filter(dplyr::row_number() <= 700) # Fit a random forest model rf_model <- randomForest::randomForest(factor(day_28_flg) ~ ., data = train_data, ntree = 1000) # Test the model on the remaining data test_data <- mimic_preprocessed |>   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\")) |>   dplyr::filter(dplyr::row_number() > 700)  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]  # Fairness evaluation # We will use sex as the sensitive attribute and day_28_flg as the outcome. # We choose threshold = 0.41 so that the overall FPR is around 5%.  # Evaluate Accuracy Parity eval_acc_parity(   data = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = 0.41 ) #> There is not enough evidence that the model does not satisfy #>             accuracy parity. #>     Metric GroupFemale GroupMale Difference   95% Diff CI Ratio 95% Ratio CI #> 1 Accuracy        0.87      0.88      -0.01 [-0.05, 0.03]  0.99 [0.94, 1.04] # }"},{"path":"/reference/eval_bs_parity.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine Brier Score Parity of a Model — eval_bs_parity","title":"Examine Brier Score Parity of a Model — eval_bs_parity","text":"function evaluates Brier Score Parity, fairness measure checks whether Brier score (measure calibration probabilistic predictions) similar across different groups. Brier score parity ensures model's predicted probabilities equally well calibrated across subpopulations.","code":""},{"path":"/reference/eval_bs_parity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine Brier Score Parity of a Model — eval_bs_parity","text":"","code":"eval_bs_parity(   data,   outcome,   group,   probs,   confint = TRUE,   alpha = 0.05,   bootstraps = 2500,   digits = 2,   message = TRUE )"},{"path":"/reference/eval_bs_parity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine Brier Score Parity of a Model — eval_bs_parity","text":"data Data frame containing outcome, predicted outcome, sensitive attribute outcome Name outcome variable group Name sensitive attribute probs Predicted probabilities confint Logical indicating whether calculate confidence intervals alpha 1 - significance level confidence interval, default 0.05 bootstraps Number bootstraps use confidence intervals digits Number digits round results , default 2 message Whether print results, default TRUE","code":""},{"path":"/reference/eval_bs_parity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine Brier Score Parity of a Model — eval_bs_parity","text":"list containing following elements: Brier Score Group 1 Brier Score Group 2 Difference Brier Score Ratio Brier Score confidence intervals computed (confint = TRUE): vector length 2 containing lower upper bounds 95% confidence interval difference Brier Score vector length 2 containing lower upper bounds 95% confidence interval ratio Brier Score","code":""},{"path":[]},{"path":"/reference/eval_bs_parity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Examine Brier Score Parity of a Model — eval_bs_parity","text":"","code":"# \\donttest{ library(FairnessEval) library(dplyr) library(randomForest) data(\"mimic_preprocessed\") set.seed(123) train_data <- mimic_preprocessed |>   dplyr::filter(dplyr::row_number() <= 700) # Fit a random forest model rf_model <- randomForest::randomForest(factor(day_28_flg) ~ ., data = train_data, ntree = 1000) # Test the model on the remaining data test_data <- mimic_preprocessed |>   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\")) |>   dplyr::filter(dplyr::row_number() > 700)  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]  # Fairness evaluation # We will use sex as the sensitive attribute and day_28_flg as the outcome.  # Evaluate Brier Score Parity eval_bs_parity(   data = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\" ) #> There is not enough evidence that the model does not satisfy #>             Brier Score parity. #>        Metric GroupFemale GroupMale Difference   95% Diff CI Ratio 95% Ratio CI #> 1 Brier Score        0.09      0.08       0.01 [-0.01, 0.03]  1.12 [0.89, 1.43] # }"},{"path":"/reference/eval_cond_acc_equality.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine Conditional Use Accuracy Equality of a Model — eval_cond_acc_equality","title":"Examine Conditional Use Accuracy Equality of a Model — eval_cond_acc_equality","text":"function evaluates Conditional Use Accuracy Equality, fairness criterion requires predictive performance similar across groups model makes positive negative predictions.","code":""},{"path":"/reference/eval_cond_acc_equality.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine Conditional Use Accuracy Equality of a Model — eval_cond_acc_equality","text":"","code":"eval_cond_acc_equality(   data,   outcome,   group,   probs,   cutoff = 0.5,   confint = TRUE,   alpha = 0.05,   bootstraps = 2500,   digits = 2,   message = TRUE )"},{"path":"/reference/eval_cond_acc_equality.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine Conditional Use Accuracy Equality of a Model — eval_cond_acc_equality","text":"data Data frame containing outcome, predicted outcome, sensitive attribute outcome Name outcome variable, must binary group Name sensitive attribute probs Name predicted outcome variable cutoff Threshold predicted outcome, default 0.5 confint Whether compute 95% confidence interval, default TRUE alpha 1 - significance level confidence interval, default 0.05 bootstraps Number bootstrap samples, default 2500 digits Number digits round results , default 2 message Whether print results, default TRUE","code":""},{"path":"/reference/eval_cond_acc_equality.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine Conditional Use Accuracy Equality of a Model — eval_cond_acc_equality","text":"list containing following elements: PPV_Group1: Positive Predictive Value first group PPV_Group2: Positive Predictive Value second group PPV_Diff: Difference Positive Predictive Value NPV_Group1: Negative Predictive Value first group NPV_Group2: Negative Predictive Value second group NPV_Diff: Difference Negative Predictive Value confidence intervals computed (confint = TRUE): PPV_Diff_CI: vector length 2 containing lower upper bounds 95% confidence interval difference Positive Predictive Value NPV_Diff_CI: vector length 2 containing lower upper bounds 95% confidence interval difference Negative Predictive Value","code":""},{"path":[]},{"path":"/reference/eval_cond_acc_equality.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Examine Conditional Use Accuracy Equality of a Model — eval_cond_acc_equality","text":"","code":"# \\donttest{ library(FairnessEval) library(dplyr) library(randomForest) data(\"mimic_preprocessed\") set.seed(123) train_data <- mimic_preprocessed |>   dplyr::filter(dplyr::row_number() <= 700) # Fit a random forest model rf_model <- randomForest::randomForest(factor(day_28_flg) ~ ., data = train_data, ntree = 1000) # Test the model on the remaining data test_data <- mimic_preprocessed |>   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\")) |>   dplyr::filter(dplyr::row_number() > 700)  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]  # Fairness evaluation # We will use sex as the sensitive attribute and day_28_flg as the outcome. # We choose threshold = 0.41 so that the overall FPR is around 5%.  # Evaluate Conditional Use Accuracy Equality eval_cond_acc_equality(   data = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = 0.41 ) #> There is not enough evidence that the model does not satisfy #>             conditional use accuracy equality. #>     Metric GroupFemale GroupMale  Difference                       95% CI #> 1 PPV; NPV  0.62; 0.92 0.66; 0.9 -0.04; 0.02 [-0.24, 0.16]; [-0.02, 0.06] # }"},{"path":"/reference/eval_cond_stats_parity.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine Conditional Statistical Parity of a Model — eval_cond_stats_parity","title":"Examine Conditional Statistical Parity of a Model — eval_cond_stats_parity","text":"function evaluates conditional statistical parity, measures fairness comparing positive prediction rates across sensitive groups within defined subgroup population. useful scenarios fairness evaluated context-specific way—e.g., within particular hospital unit age bracket. Conditional statistical parity refinement standard statistical parity. Instead comparing prediction rates across groups entire dataset, restricts comparison specified subset population, defined conditioning variable.","code":""},{"path":"/reference/eval_cond_stats_parity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine Conditional Statistical Parity of a Model — eval_cond_stats_parity","text":"","code":"eval_cond_stats_parity(   data,   outcome,   group,   group2,   condition,   probs,   cutoff = 0.5,   bootstraps = 2500,   alpha = 0.05,   message = TRUE,   digits = 2 )"},{"path":"/reference/eval_cond_stats_parity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine Conditional Statistical Parity of a Model — eval_cond_stats_parity","text":"data Data frame containing outcome, predicted outcome, sensitive attribute outcome Name outcome variable, must binary group Name sensitive attribute group2 Name group condition condition conditional group categorical, condition supplied must character levels condition . conditional group continuous, conditions supplied must character containing sign condition value threshold continuous variable (e.g. \"<50\", \">50\", \"<=50\", \">=50\"). probs Name predicted outcome variable cutoff Threshold predicted outcome, default 0.5 bootstraps Number bootstrap samples, default 2500 alpha 1 - significance level confidence interval, default 0.05 message Whether print results, default TRUE digits Number digits round results , default 2","code":""},{"path":"/reference/eval_cond_stats_parity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine Conditional Statistical Parity of a Model — eval_cond_stats_parity","text":"list containing following elements: Conditions: conditions used calculate conditional PPR PPR_Group1: Positive Prediction Rate first group PPR_Group2: Positive Prediction Rate second group PPR_Diff: Difference Positive Prediction Rate PPR_Ratio: Ratio Positive Prediction Rate confidence intervals computed (confint = TRUE): PPR_Diff_CI: vector length 2 containing lower upper bounds 95% confidence interval difference Positive Prediction Rate PPR_Ratio_CI: vector length 2 containing lower upper bounds 95% confidence interval ratio Positive Prediction Rate","code":""},{"path":"/reference/eval_cond_stats_parity.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Examine Conditional Statistical Parity of a Model — eval_cond_stats_parity","text":"function supports categorical continuous conditioning variables. continuous variables, can supply threshold expression like \"<50\" \">=75\" condition parameter.","code":""},{"path":[]},{"path":"/reference/eval_cond_stats_parity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Examine Conditional Statistical Parity of a Model — eval_cond_stats_parity","text":"","code":"# \\donttest{ #' library(FairnessEval) library(dplyr) library(randomForest) data(\"mimic_preprocessed\") set.seed(123) train_data <- mimic_preprocessed |>   dplyr::filter(dplyr::row_number() <= 700) # Fit a random forest model rf_model <- randomForest::randomForest(factor(day_28_flg) ~ ., data = train_data, ntree = 1000) # Test the model on the remaining data test_data <- mimic_preprocessed |>   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\")) |>   dplyr::filter(dplyr::row_number() > 700)  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]  # Fairness evaluation # We will use sex as the sensitive attribute and day_28_flg as the outcome. # We choose threshold = 0.41 so that the overall FPR is around 5%.  # Evaluate Conditional Statistical Parity  eval_cond_stats_parity(   data = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   group2 = \"service_unit\",   condition = \"MICU\",   probs = \"pred\",   cutoff = 0.41 ) #> There is not enough evidence that the model does not satisfy #>             statistical parity. #>   Metric GroupFemale GroupMale Difference   95% Diff CI Ratio 95% Ratio CI #> 1    PPR        0.15       0.1       0.05 [-0.01, 0.11]   1.5  [0.87, 2.6] # }"},{"path":"/reference/eval_eq_odds.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine Equalized Odds of a Predictive Model — eval_eq_odds","title":"Examine Equalized Odds of a Predictive Model — eval_eq_odds","text":"function evaluates whether predictive model satisfies Equalized Odds criterion comparing False Negative Rates (FNR) False Positive Rates (FPR) across two groups defined binary sensitive attribute. reports rate group, differences, ratios, bootstrap-based confidence regions. Bonferroni-corrected union test used test whether model violates Equalized Odds criterion.","code":""},{"path":"/reference/eval_eq_odds.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine Equalized Odds of a Predictive Model — eval_eq_odds","text":"","code":"eval_eq_odds(   data,   outcome,   group,   probs,   cutoff = 0.5,   bootstraps = 2500,   alpha = 0.05,   digits = 2,   message = TRUE )"},{"path":"/reference/eval_eq_odds.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine Equalized Odds of a Predictive Model — eval_eq_odds","text":"data data frame containing true binary outcomes, predicted probabilities, sensitive group membership. outcome string specifying name binary outcome variable data. group string specifying name binary sensitive attribute variable (e.g., race, gender) used define comparison groups. probs string specifying name variable containing predicted probabilities risk scores. cutoff numeric value used threshold predicted probabilities binary predictions; defaults 0.5. bootstraps integer specifying number bootstrap resamples constructing confidence intervals; vdefaults 2500. alpha Significance level (1 - alpha) confidence interval; defaults 0.05. digits Number decimal places round numeric results; defaults 2. message Logical; TRUE (default), prints textual summary fairness evaluation.","code":""},{"path":"/reference/eval_eq_odds.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine Equalized Odds of a Predictive Model — eval_eq_odds","text":"data frame summarizing group disparities FNR FPR following columns: Metric: reported metrics (\"FNR; FPR\"). Group1: Estimated FNR FPR first group. Group2: Estimated FNR FPR second group. Difference: Differences FNR FPR, computed Group1 - Group2. 95% CR: Bonferroni-adjusted confidence regions differences. Ratio: Ratios FNR FPR, computed Group1 / Group2. 95% CR: Bonferroni-adjusted confidence regions ratios.","code":""},{"path":"/reference/eval_eq_odds.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Examine Equalized Odds of a Predictive Model — eval_eq_odds","text":"","code":"# \\donttest{ library(FairnessEval) library(dplyr) library(randomForest) data(\"mimic_preprocessed\") set.seed(123) train_data <- mimic_preprocessed |>   dplyr::filter(dplyr::row_number() <= 700) # Fit a random forest model rf_model <- randomForest::randomForest(factor(day_28_flg) ~ .,   data = train_data, ntree = 1000 ) # Test the model on the remaining data test_data <- mimic_preprocessed |>   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\")) |>   dplyr::filter(dplyr::row_number() > 700)  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]  # Fairness evaluation # We will use sex as the sensitive attribute and day_28_flg as the outcome. # We choose threshold = 0.41 so that the overall FPR is around 5%.  # Evaluate Equalized Odds eval_eq_odds(   data = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = 0.41 ) #> There is evidence that model does not satisfy equalized odds. #>     Metric Group Female Group Male  Difference                       95% CR #> 1 FNR; FPR   0.38; 0.08 0.62; 0.03 -0.24; 0.05 [-0.41, -0.07]; [0.01, 0.09] #>        Ratio                    95% CR #> 1 0.61; 2.67 [0.42, 0.9]; [1.26, 5.66] # }"},{"path":"/reference/eval_eq_opp.html","id":null,"dir":"Reference","previous_headings":"","what":"Evaluate Equal Opportunity Compliance of a Predictive Model — eval_eq_opp","title":"Evaluate Equal Opportunity Compliance of a Predictive Model — eval_eq_opp","text":"function evaluates fairness predictive model respect Equal Opportunity criterion, requires False Negative Rate (FNR) comparable across groups defined sensitive attribute. function quantifies disparities FNR two groups provides absolute difference ratio, along confidence intervals obtained via bootstrapping.","code":""},{"path":"/reference/eval_eq_opp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Evaluate Equal Opportunity Compliance of a Predictive Model — eval_eq_opp","text":"","code":"eval_eq_opp(   data,   outcome,   group,   probs,   cutoff = 0.5,   bootstraps = 2500,   alpha = 0.05,   digits = 2,   message = TRUE )"},{"path":"/reference/eval_eq_opp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Evaluate Equal Opportunity Compliance of a Predictive Model — eval_eq_opp","text":"data data frame containing true binary outcomes, predicted probabilities, sensitive group membership. outcome string specifying name binary outcome variable data. group string specifying name sensitive attribute variable (e.g., race, gender). probs string specifying name variable containing predicted probabilities risk scores. cutoff numeric value used threshold predicted probabilities binary decisions; defaults 0.5. bootstraps integer specifying number bootstrap resamples constructing confidence intervals; defaults 2500. alpha Significance level constructing (1 - alpha) confidence interval; defaults 0.05. digits Integer indicating number decimal places round results ; defaults 2. message Logical; TRUE (default), prints textual summary fairness evaluation.","code":""},{"path":"/reference/eval_eq_opp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Evaluate Equal Opportunity Compliance of a Predictive Model — eval_eq_opp","text":"data frame summarizing FNR-based group disparity metrics following columns: Metric label indicating reported fairness criterion. Group1 Estimated FNR FPR first group. Group2 Estimated FNR FPR second group. Difference difference FNR two groups, computed FNR Group1 minus FNR Group2. 95% Diff CI (1 - alpha) confidence interval FNR difference. Ratio ratio FNRs Group1 Group2, computed FNR Group1 divided FNR Group2. 95% Ratio CI corresponding confidence interval FNR ratio.","code":""},{"path":"/reference/eval_eq_opp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Evaluate Equal Opportunity Compliance of a Predictive Model — eval_eq_opp","text":"","code":"# \\donttest{ library(FairnessEval) library(dplyr) library(randomForest) data(\"mimic_preprocessed\") set.seed(123) train_data <- mimic_preprocessed |>   dplyr::filter(dplyr::row_number() <= 700) # Fit a random forest model rf_model <- randomForest::randomForest(factor(day_28_flg) ~ .,   data =     train_data, ntree = 1000 ) # Test the model on the remaining data test_data <- mimic_preprocessed |>   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\")) |>   dplyr::filter(dplyr::row_number() > 700)  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]  # Fairness evaluation # We will use sex as the sensitive attribute and day_28_flg as the outcome. # We choose threshold = 0.41 so that the overall FPR is around 5%.  # Evaluate Equal Opportunity Compliance eval_eq_opp(   data = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = 0.41 ) #> There is evidence that model does not satisfy equal opportunity. #>   Metric GroupFemale GroupMale Difference    95% Diff CI Ratio 95% Ratio CI #> 1    FNR        0.38      0.62      -0.24 [-0.39, -0.09]  0.61 [0.44, 0.86] # }"},{"path":"/reference/eval_neg_class_bal.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine Balance for Negative Class of a Model — eval_neg_class_bal","title":"Examine Balance for Negative Class of a Model — eval_neg_class_bal","text":"function evaluates Balance Negative Class, fairness criterion checks whether model assigns similar predicted probabilities across groups among individuals whose true outcome negative (.e., \\(Y = 0\\)).","code":""},{"path":"/reference/eval_neg_class_bal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine Balance for Negative Class of a Model — eval_neg_class_bal","text":"","code":"eval_neg_class_bal(   data,   outcome,   group,   probs,   confint = TRUE,   alpha = 0.05,   bootstraps = 2500,   digits = 2,   message = TRUE )"},{"path":"/reference/eval_neg_class_bal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine Balance for Negative Class of a Model — eval_neg_class_bal","text":"data Data frame containing outcome, predicted outcome, sensitive attribute outcome Name outcome variable group Name sensitive attribute probs Predicted probabilities confint Logical indicating whether calculate confidence intervals alpha 1 - significance level confidence interval, default 0.05 bootstraps Number bootstraps use confidence intervals digits Number digits round results , default 2 message Whether print results, default TRUE","code":""},{"path":"/reference/eval_neg_class_bal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine Balance for Negative Class of a Model — eval_neg_class_bal","text":"list containing following elements: Average predicted probability Group 1 Average predicted probability Group 2 Difference average predicted probability Ratio average predicted probability confidence intervals computed (confint = TRUE): vector length 2 containing lower upper bounds 95% confidence interval difference average predicted probability vector length 2 containing lower upper bounds 95% confidence interval ratio average predicted probability","code":""},{"path":[]},{"path":"/reference/eval_neg_class_bal.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Examine Balance for Negative Class of a Model — eval_neg_class_bal","text":"","code":"# \\donttest{ library(FairnessEval) library(dplyr) library(randomForest) data(\"mimic_preprocessed\") set.seed(123) train_data <- mimic_preprocessed |>   dplyr::filter(dplyr::row_number() <= 700) # Fit a random forest model rf_model <- randomForest::randomForest(factor(day_28_flg) ~ ., data = train_data, ntree = 1000) # Test the model on the remaining data test_data <- mimic_preprocessed |>   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\")) |>   dplyr::filter(dplyr::row_number() > 700)  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]  # Fairness evaluation # We will use sex as the sensitive attribute and day_28_flg as the outcome.  # Evaluate Balance for Negative Class eval_neg_class_bal(   data = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\" ) #> There is enough evidence that the model does not satisfy #>             balance for negative class. #>                 Metric GroupFemale GroupMale Difference  95% Diff CI Ratio #> 1 Avg. Predicted Prob.        0.15       0.1       0.05 [0.03, 0.07]   1.5 #>   95% Ratio CI #> 1 [1.29, 1.75] # }"},{"path":"/reference/eval_pos_class_bal.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine Balance for the Positive Class of a Model — eval_pos_class_bal","title":"Examine Balance for the Positive Class of a Model — eval_pos_class_bal","text":"function evaluates Balance Positive Class, fairness criterion checks whether model assigns similar predicted probabilities across groups among individuals whose true outcome positive (.e., \\(Y = 1\\)).","code":""},{"path":"/reference/eval_pos_class_bal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine Balance for the Positive Class of a Model — eval_pos_class_bal","text":"","code":"eval_pos_class_bal(   data,   outcome,   group,   probs,   confint = TRUE,   alpha = 0.05,   bootstraps = 2500,   digits = 2,   message = TRUE )"},{"path":"/reference/eval_pos_class_bal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine Balance for the Positive Class of a Model — eval_pos_class_bal","text":"data Data frame containing outcome, predicted outcome, sensitive attribute outcome Name outcome variable group Name sensitive attribute probs Predicted probabilities confint Logical indicating whether calculate confidence intervals alpha 1 - significance level confidence interval, default 0.05 bootstraps Number bootstraps use confidence intervals digits Number digits round results , default 2 message Whether print results, default TRUE","code":""},{"path":"/reference/eval_pos_class_bal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine Balance for the Positive Class of a Model — eval_pos_class_bal","text":"list containing following elements: Average predicted probability Group 1 Average predicted probability Group 2 Difference average predicted probability Ratio average predicted probability confidence intervals computed (confint = TRUE): vector length 2 containing lower upper bounds 95% confidence interval difference average predicted probability vector length 2 containing lower upper bounds 95% confidence interval ratio average predicted probability","code":""},{"path":[]},{"path":"/reference/eval_pos_class_bal.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Examine Balance for the Positive Class of a Model — eval_pos_class_bal","text":"","code":"# \\donttest{ library(FairnessEval) library(dplyr) library(randomForest) data(\"mimic_preprocessed\") set.seed(123) train_data <- mimic_preprocessed |>   dplyr::filter(dplyr::row_number() <= 700) # Fit a random forest model rf_model <- randomForest::randomForest(factor(day_28_flg) ~ ., data = train_data, ntree = 1000) # Test the model on the remaining data test_data <- mimic_preprocessed |>   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\")) |>   dplyr::filter(dplyr::row_number() > 700)  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]  # Fairness evaluation # We will use sex as the sensitive attribute and day_28_flg as the outcome.  # Evaluate Balance for Positive Class eval_pos_class_bal(   data = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\" ) #> There is evidence that the model does not satisfy #>             balance for positive class. #>                 Metric GroupFemale GroupMale Difference  95% Diff CI Ratio #> 1 Avg. Predicted Prob.        0.46      0.37       0.09 [0.04, 0.14]  1.24 #>   95% Ratio CI #> 1 [1.09, 1.42] # }"},{"path":"/reference/eval_pred_equality.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine Predictive Equality of a Model — eval_pred_equality","title":"Examine Predictive Equality of a Model — eval_pred_equality","text":"function evaluates predictive equality, fairness metric compares False Positive Rate (FPR) groups defined sensitive attribute. assesses whether individuals different groups equally likely incorrectly flagged positive , fact, negative.","code":""},{"path":"/reference/eval_pred_equality.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine Predictive Equality of a Model — eval_pred_equality","text":"","code":"eval_pred_equality(   data,   outcome,   group,   probs,   cutoff = 0.5,   confint = TRUE,   alpha = 0.05,   bootstraps = 2500,   digits = 2,   message = TRUE )"},{"path":"/reference/eval_pred_equality.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine Predictive Equality of a Model — eval_pred_equality","text":"data Data frame containing outcome, predicted outcome, sensitive attribute outcome Name outcome variable, must binary group Name sensitive attribute probs Name predicted outcome variable cutoff Threshold predicted outcome, default 0.5 confint Whether compute 95% confidence interval, default TRUE alpha 1 - significance level confidence interval, default 0.05 bootstraps Number bootstrap samples, default 2500 digits Number digits round results , default 2 message Whether print results, default TRUE","code":""},{"path":"/reference/eval_pred_equality.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine Predictive Equality of a Model — eval_pred_equality","text":"list containing following elements: FPR_Group1: False Positive Rate first group FPR_Group2: False Positive Rate second group FPR_Diff: Difference False Positive Rate FPR_Ratio: Ratio False Positive Rate confidence intervals computed (confint = TRUE): FPR_Diff_CI: vector length 2 containing lower upper bounds 95% confidence interval difference False Positive Rate FPR_Ratio_CI: vector length 2 containing lower upper bounds 95% confidence interval ratio False Positive Rate","code":""},{"path":[]},{"path":"/reference/eval_pred_equality.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Examine Predictive Equality of a Model — eval_pred_equality","text":"","code":"# \\donttest{ library(FairnessEval) library(dplyr) library(randomForest) data(\"mimic_preprocessed\") set.seed(123) train_data <- mimic_preprocessed |>   dplyr::filter(dplyr::row_number() <= 700) # Fit a random forest model rf_model <- randomForest::randomForest(factor(day_28_flg) ~ ., data = train_data, ntree = 1000) # Test the model on the remaining data test_data <- mimic_preprocessed |>   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\")) |>   dplyr::filter(dplyr::row_number() > 700)  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]  # Fairness evaluation # We will use sex as the sensitive attribute and day_28_flg as the outcome. # We choose threshold = 0.41 so that the overall FPR is around 5%.  # Evaluate Predictive Equality eval_pred_equality(   data = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = 0.41 ) #> There is evidence that model does not satisfy predictive #>             equality. #>   Metric GroupFemale GroupMale Difference  95% Diff CI Ratio 95% Ratio CI #> 1    FPR        0.08      0.03       0.05 [0.02, 0.08]  2.67 [1.38, 5.15] # }"},{"path":"/reference/eval_pred_parity.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine Predictive Parity of a Model — eval_pred_parity","title":"Examine Predictive Parity of a Model — eval_pred_parity","text":"function evaluates predictive parity (PP), key fairness criterion compares Positive Predictive Value (PPV) groups defined sensitive attribute. words, assesses whether, among individuals predicted positive, probability truly positive equal across subgroups.","code":""},{"path":"/reference/eval_pred_parity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine Predictive Parity of a Model — eval_pred_parity","text":"","code":"eval_pred_parity(   data,   outcome,   group,   probs,   cutoff = 0.5,   confint = TRUE,   bootstraps = 2500,   alpha = 0.05,   digits = 2,   message = TRUE )"},{"path":"/reference/eval_pred_parity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine Predictive Parity of a Model — eval_pred_parity","text":"data Data frame containing outcome, predicted outcome, sensitive attribute outcome Name outcome variable, must binary group Name sensitive attribute probs Name predicted outcome variable cutoff Threshold predicted outcome, default 0.5 confint Whether compute 95% confidence interval, default TRUE bootstraps Number bootstrap samples, default 2500 alpha 1 - significance level confidence interval, default 0.05 digits Number digits round results , default 2 message Whether print results, default TRUE","code":""},{"path":"/reference/eval_pred_parity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine Predictive Parity of a Model — eval_pred_parity","text":"list containing following elements: PPV_Group1: Positive Predictive Value first group PPV_Group2: Positive Predictive Value second group PPV_Diff: Difference Positive Predictive Value PPV_Ratio: Ratio Positive Predictive Value confidence intervals computed (confint = TRUE): PPV_Diff_CI: vector length 2 containing lower upper bounds 95% confidence interval difference Positive Predictive Value PPV_Ratio_CI: vector length 2 containing lower upper bounds 95% confidence interval ratio Positive Predictive Value","code":""},{"path":"/reference/eval_pred_parity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Examine Predictive Parity of a Model — eval_pred_parity","text":"","code":"# \\donttest{ library(FairnessEval) library(dplyr) library(randomForest) data(\"mimic_preprocessed\") set.seed(123) train_data <- mimic_preprocessed |>   dplyr::filter(dplyr::row_number() <= 700) # Fit a random forest model rf_model <- randomForest::randomForest(factor(day_28_flg) ~ ., data = train_data, ntree = 1000) # Test the model on the remaining data test_data <- mimic_preprocessed |>   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\")) |>   dplyr::filter(dplyr::row_number() > 700)  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]  # Fairness evaluation # We will use sex as the sensitive attribute and day_28_flg as the outcome. # We choose threshold = 0.41 so that the overall FPR is around 5%.  # Evaluate Predictive Parity eval_pred_parity(   data = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = 0.41 ) #> There is not enough evidence that the model does not satisfy #>             predictive parity. #>   Metric GroupFemale GroupMale Difference   95% Diff CI Ratio 95% Ratio CI #> 1    PPV        0.62      0.66      -0.04 [-0.21, 0.13]  0.94 [0.72, 1.23] # }"},{"path":"/reference/eval_stats_parity.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine Statistical Parity of a Model — eval_stats_parity","title":"Examine Statistical Parity of a Model — eval_stats_parity","text":"function assesses statistical parity - also known demographic parity - predictions binary classifier across two groups defined sensitive attribute. Statistical parity compares rate different groups receive positive prediction, irrespective true outcome. reports Positive Prediction Rate (PPR) group, differences, ratios, bootstrap-based confidence regions.","code":""},{"path":"/reference/eval_stats_parity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine Statistical Parity of a Model — eval_stats_parity","text":"","code":"eval_stats_parity(   data,   outcome,   group,   probs,   cutoff = 0.5,   confint = TRUE,   bootstraps = 2500,   alpha = 0.05,   digits = 2,   message = TRUE )"},{"path":"/reference/eval_stats_parity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine Statistical Parity of a Model — eval_stats_parity","text":"data Data frame containing outcome, predicted outcome, sensitive attribute outcome Name outcome variable, must binary group Name sensitive attribute probs Name predicted outcome variable cutoff Threshold predicted outcome, default 0.5 confint Whether compute 95% confidence interval, default TRUE bootstraps Number bootstrap samples, default 2500 alpha 1 - significance level confidence interval, default 0.05 digits Number digits round results , default 2 message Whether print results, default TRUE","code":""},{"path":"/reference/eval_stats_parity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine Statistical Parity of a Model — eval_stats_parity","text":"list containing following elements: PPR_Group1: Positive Prediction Rate first group PPR_Group2: Positive Prediction Rate second group PPR_Diff: Difference Positive Prediction Rate PPR_Ratio: ratio Positive Prediction Rate two groups. confidence intervals computed (confint = TRUE): PPR_Diff_CI: vector length 2 containing lower upper bounds 95% confidence interval difference Positive Prediction Rate PPR_Ratio_CI: vector length 2 containing lower upper bounds 95% confidence interval ratio Positive Prediction Rate","code":""},{"path":[]},{"path":"/reference/eval_stats_parity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Examine Statistical Parity of a Model — eval_stats_parity","text":"","code":"# \\donttest{ library(FairnessEval) library(dplyr) library(randomForest) data(\"mimic_preprocessed\") set.seed(123) train_data <- mimic_preprocessed |>   dplyr::filter(dplyr::row_number() <= 700) # Fit a random forest model rf_model <- randomForest::randomForest(factor(day_28_flg) ~ ., data = train_data, ntree = 1000) # Test the model on the remaining data test_data <- mimic_preprocessed |>   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\")) |>   dplyr::filter(dplyr::row_number() > 700)  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]  # Fairness evaluation # We will use sex as the sensitive attribute and day_28_flg as the outcome. # We choose threshold = 0.41 so that the overall FPR is around 5%.  # Evaluate Statistical Parity eval_stats_parity(   data = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = 0.41 ) #> There is evidence that model does not satisfy statistical parity. #>   Metric GroupFemale GroupMale Difference  95% Diff CI Ratio 95% Ratio CI #> 1    PPR        0.17      0.08       0.09 [0.05, 0.13]  2.12 [1.48, 3.05] # }"},{"path":"/reference/eval_treatment_equality.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine Treatment Equality of a Model — eval_treatment_equality","title":"Examine Treatment Equality of a Model — eval_treatment_equality","text":"function evaluates Treatment Equality, fairness criterion assesses whether ratio false negatives false positives similar across groups (e.g., based gender race). Treatment Equality ensures model disproportionately favor disadvantage group terms relative frequency missed detections (false negatives) versus false alarms (false positives).","code":""},{"path":"/reference/eval_treatment_equality.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine Treatment Equality of a Model — eval_treatment_equality","text":"","code":"eval_treatment_equality(   data,   outcome,   group,   probs,   cutoff = 0.5,   confint = TRUE,   alpha = 0.05,   bootstraps = 2500,   digits = 2,   message = TRUE )"},{"path":"/reference/eval_treatment_equality.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine Treatment Equality of a Model — eval_treatment_equality","text":"data Data frame containing outcome, predicted outcome, sensitive attribute outcome Name outcome variable group Name sensitive attribute probs Predicted probabilities cutoff Cutoff value predicted probabilities confint Logical indicating whether calculate confidence intervals alpha 1 - significance level confidence interval, default 0.05 bootstraps Number bootstraps use confidence intervals digits Number digits round results , default 2 message Whether print results, default TRUE","code":""},{"path":"/reference/eval_treatment_equality.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine Treatment Equality of a Model — eval_treatment_equality","text":"list containing following elements: False Negative / False Positive ratio Group 1 False Negative / False Positive ratio Group 2 Difference False Negative / False Positive ratio Ratio False Negative / False Positive ratio confidence intervals computed (confint = TRUE): vector length 2 containing lower upper bounds 95% confidence interval difference False Negative / False Positive ratio vector length 2 containing lower upper bounds 95% confidence interval ratio False Negative / False Positive ratio","code":""},{"path":[]},{"path":"/reference/eval_treatment_equality.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Examine Treatment Equality of a Model — eval_treatment_equality","text":"","code":"# \\donttest{ library(FairnessEval) library(dplyr) library(randomForest) # Data for tests data(\"mimic_preprocessed\") set.seed(123) train_data <- mimic_preprocessed |>   dplyr::filter(dplyr::row_number() <= 700) # Fit a random forest model rf_model <- randomForest::randomForest(factor(day_28_flg) ~ ., data = train_data, ntree = 1000) # Test the model on the remaining data test_data <- mimic_preprocessed |>   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\")) |>   dplyr::filter(dplyr::row_number() > 700)  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]  # Fairness evaluation # We will use sex as the sensitive attribute and day_28_flg as the outcome.  # Evaluate Treatment Equality eval_treatment_equality(   data = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = 0.41,   confint = TRUE,   alpha = 0.05,   bootstraps = 2500,   digits = 2,   message = FALSE ) #>        Metric GroupFemale GroupMale Difference   95% Diff CI Ratio 95% Ratio CI #> 1 FN/FP Ratio        1.03      3.24      -2.21 [-4.44, 0.02]  0.32  [0.14, 0.7] # }"},{"path":"/reference/get_all_metrics.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the all metrics at once — get_all_metrics","title":"Calculate the all metrics at once — get_all_metrics","text":"function computes comprehensive set fairness-related performance metrics across levels sensitive attribute. includes standard classification metrics (e.g., TPR, FPR, PPV, NPV) well fairness-specific indicators like predicted positive rates error ratios.","code":""},{"path":"/reference/get_all_metrics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the all metrics at once — get_all_metrics","text":"","code":"get_all_metrics(data, outcome, group, probs, cutoff = 0.5, digits = 2)"},{"path":"/reference/get_all_metrics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the all metrics at once — get_all_metrics","text":"data Data frame containing outcome, predicted outcome, sensitive attribute outcome name outcome variable, must binary group name sensitive attribute probs name predicted outcome variable cutoff threshold predicted outcome, default 0.5 digits number digits round result , default 2","code":""},{"path":"/reference/get_all_metrics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate the all metrics at once — get_all_metrics","text":"Data frame metrics","code":""},{"path":"/reference/get_all_metrics.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate the all metrics at once — get_all_metrics","text":"useful quickly assessing multiple fairness dimensions binary classifier one step.","code":""},{"path":"/reference/get_all_metrics.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate the all metrics at once — get_all_metrics","text":"","code":"# \\donttest{ library(FairnessEval) library(dplyr) library(randomForest) data(\"mimic_preprocessed\") set.seed(123) train_data <- mimic_preprocessed |>   dplyr::filter(dplyr::row_number() <= 700) # Fit a random forest model rf_model <- randomForest::randomForest(factor(day_28_flg) ~ ., data = train_data, ntree = 1000) # Test the model on the remaining data test_data <- mimic_preprocessed |>   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\"))|>   dplyr::filter(dplyr::row_number() > 700)  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]  # Fairness evaluation # We will use sex as the sensitive attribute and day_28_flg as the outcome. # We choose threshold = 0.41 so that the overall FPR is around 5%.  # Calculate All Metrics get_all_metrics(   dat = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = 0.41 ) #>          Metric Group Female Group Male #> 1           TPR         0.62       0.38 #> 2           FPR         0.08       0.03 #> 3           PPR         0.17       0.08 #> 4           PPV         0.62       0.66 #> 5           NPV         0.92       0.90 #> 6           ACC         0.87       0.88 #> 7   Brier Score         0.09       0.08 #> 8   FN/FP Ratio         1.03       3.24 #> 9 Avg Pred Prob         0.21       0.14 # }"},{"path":"/reference/get_fairness_metrics.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute Fairness Metrics for Binary Classification — get_fairness_metrics","title":"Compute Fairness Metrics for Binary Classification — get_fairness_metrics","text":"function evaluates comprehensive set fairness metrics binary classification models across groups defined sensitive attribute (e.g., race, gender). returns unified data frame containing metric values, optionally bootstrap confidence intervals.","code":""},{"path":"/reference/get_fairness_metrics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute Fairness Metrics for Binary Classification — get_fairness_metrics","text":"","code":"get_fairness_metrics(   data,   outcome,   group,   group2 = NULL,   condition = NULL,   probs,   cutoff = 0.5,   bootstraps = 2500,   alpha = 0.05,   digits = 2 )"},{"path":"/reference/get_fairness_metrics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute Fairness Metrics for Binary Classification — get_fairness_metrics","text":"data data frame containing outcome, group, predicted probabilities. outcome name column containing true binary outcome. group name column representing sensitive attribute (e.g., race, gender). group2 Define conditional statistical parity desired. Name secondary group variable used conditional fairness analysis. condition Define conditional statistical parity desired. conditional group categorical, condition supplied must character levels condition . conditional group continuous, conditions supplied must character containing sign condition value threshold continuous variable (e.g. \"<50\", \">50\", \"<=50\", \">=50\"). probs name column predicted probabilities. cutoff Numeric threshold classification. Default 0.5. bootstraps Number bootstrap samples. Default 2500. alpha Significance level confidence intervals. Default 0.05. digits Number digits round metrics . Default 2.","code":""},{"path":"/reference/get_fairness_metrics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute Fairness Metrics for Binary Classification — get_fairness_metrics","text":"data frame evaluated fairness metrics.","code":""},{"path":[]},{"path":"/reference/get_fairness_metrics.html","id":"metrics-included-","dir":"Reference","previous_headings":"","what":"Metrics Included:","title":"Compute Fairness Metrics for Binary Classification — get_fairness_metrics","text":"Statistical Parity: Difference positive prediction rates across groups. Conditional Statistical Parity (group2 condition specified): Parity conditioned second group value. Equal Opportunity: Difference true positive rates (TPR) across groups. Predictive Equality: Difference false positive rates (FPR) across groups. Balance Positive Class: Checks whether predicted probability distributions positive outcomes similar across groups. Balance Negative Class: , negative outcomes. Predictive Parity: Difference positive predictive values (precision) across groups. Brier Score Parity: Difference Brier scores across groups. Overall Accuracy Parity: Difference overall accuracy across groups. Treatment Equality: Ratio false negatives false positives across groups.","code":""},{"path":"/reference/get_fairness_metrics.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute Fairness Metrics for Binary Classification — get_fairness_metrics","text":"","code":"# \\donttest{ library(FairnessEval) library(dplyr) library(randomForest) data(\"mimic_preprocessed\") set.seed(123) train_data <- mimic_preprocessed |>   dplyr::filter(dplyr::row_number() <= 700) # Fit a random forest model rf_model <- randomForest::randomForest(factor(day_28_flg) ~ ., data = train_data, ntree = 1000) # Test the model on the remaining data test_data <- mimic_preprocessed |>   dplyr::mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\"))|>   dplyr::filter(dplyr::row_number() > 700)  test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[, 2]  # Fairness evaluation # We will use sex as the sensitive attribute and day_28_flg as the outcome. # We choose threshold = 0.41 so that the overall FPR is around 5%.  # Evaluate Accuracy Parity get_fairness_metrics(  data = test_data,  outcome = \"day_28_flg\",  group = \"gender\",  group2 = \"age\",  condition = \">=60\",  probs = \"pred\",  cutoff = 0.41 ) #>                                       Metric GroupFemale GroupMale Difference #> 1                         Statistical Parity        0.17      0.08       0.09 #> 2  Conditional Statistical Parity (age >=60)        0.34      0.21       0.13 #> 3                          Equal Opportunity        0.38      0.62      -0.24 #> 4                        Predictive Equality        0.08      0.03       0.05 #> 5                 Balance for Positive Class        0.46      0.37       0.09 #> 6                 Balance for Negative Class        0.15      0.10       0.05 #> 7                          Predictive Parity        0.62      0.66      -0.04 #> 8                         Brier Score Parity        0.09      0.08       0.01 #> 9                    Overall Accuracy Parity        0.87      0.88      -0.01 #> 10                        Treatment Equality        1.03      3.24      -2.21 #>       95% Diff CI Ratio 95% Ratio CI #> 1    [0.05, 0.13]  2.12 [1.48, 3.05] #> 2    [0.05, 0.21]  1.62 [1.18, 2.22] #> 3  [-0.39, -0.09]  0.61 [0.44, 0.86] #> 4    [0.02, 0.08]  2.67  [1.39, 5.1] #> 5    [0.04, 0.14]  1.24 [1.09, 1.41] #> 6    [0.03, 0.07]  1.50 [1.29, 1.74] #> 7   [-0.21, 0.13]  0.94 [0.72, 1.23] #> 8   [-0.01, 0.03]  1.12 [0.89, 1.43] #> 9   [-0.05, 0.03]  0.99 [0.94, 1.04] #> 10 [-4.35, -0.07]  0.32 [0.15, 0.68] # }"},{"path":"/reference/mimic.html","id":null,"dir":"Reference","previous_headings":"","what":"Clinical data from the MIMIC-II database for a case study on indwelling arterial catheters — mimic","title":"Clinical data from the MIMIC-II database for a case study on indwelling arterial catheters — mimic","text":"Indwelling Arterial Catheter Clinical dataset contains clinical data 1776 patients MIMIC-II clinical database. basis article: Hsu DJ, et al. association indwelling arterial catheters mortality hemodynamically stable patients respiratory failure: propensity score analysis. Chest, 148(6):1470–1476, Aug. 2015. dataset also used Raffa et al. Chapter 5 \"Data Analysis\" forthcoming book: Secondary Analysis Electronic Health Records, published Springer 2016.","code":""},{"path":"/reference/mimic.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clinical data from the MIMIC-II database for a case study on indwelling arterial catheters — mimic","text":"","code":"mimic"},{"path":"/reference/mimic.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Clinical data from the MIMIC-II database for a case study on indwelling arterial catheters — mimic","text":"data frame 1776 rows 46 variables: aline_flg Integer, indicates IAC used (1 = yes, 0 = ) icu_los_day Double, length stay ICU (days) hospital_los_day Integer, length stay hospital (days) age Double, age baseline (years) gender_num Integer, patient gender (1 = male; 0 = female) weight_first Double, first weight (kg) bmi Double, patient BMI sapsi_first Integer, first SAPS score sofa_first Integer, first SOFA score service_unit Character, type service unit (FICU, MICU, SICU) service_num Integer, service numeric value (0 = MICU FICU, 1 = SICU) day_icu_intime Character, day week ICU admission day_icu_intime_num Integer, day week ICU admission (numeric) hour_icu_intime Integer, hour ICU admission (24hr clock) hosp_exp_flg Integer, death hospital (1 = yes, 0 = ) icu_exp_flg Integer, death ICU (1 = yes, 0 = ) day_28_flg Integer, death within 28 days (1 = yes, 0 = ) mort_day_censored Double, day post ICU admission censoring death (days) censor_flg Integer, censored death (0 = death, 1 = censored) sepsis_flg Integer, sepsis present (0 = , 1 = yes) chf_flg Integer, congestive heart failure (0 = , 1 = yes) afib_flg Integer, atrial fibrillation (0 = , 1 = yes) renal_flg Integer, chronic renal disease (0 = , 1 = yes) liver_flg Integer, liver disease (0 = , 1 = yes) copd_flg Integer, chronic obstructive pulmonary disease (0 = , 1 = yes) cad_flg Integer, coronary artery disease (0 = , 1 = yes) stroke_flg Integer, stroke (0 = , 1 = yes) mal_flg Integer, malignancy (0 = , 1 = yes) resp_flg Integer, respiratory disease (non-COPD) (0 = , 1 = yes) map_1st Double, mean arterial pressure (mmHg) hr_1st Integer, heart rate temp_1st Double, temperature (F) spo2_1st Integer, S_pO_2 (percent) abg_count Integer, arterial blood gas count (number tests) wbc_first Double, first white blood cell count (K/uL) hgb_first Double, first hemoglobin (g/dL) platelet_first Integer, first platelets (K/u) sodium_first Integer, first sodium (mEq/L) potassium_first Double, first potassium (mEq/L) tco2_first Double, first bicarbonate (mEq/L) chloride_first Integer, first chloride (mEq/L) bun_first Integer, first blood urea nitrogen (mg/dL) creatinine_first Double, first creatinine (mg/dL) po2_first Integer, first PaO_2 (mmHg) pco2_first Integer, first PaCO_2 (mmHg) iv_day_1 Double, input fluids IV day 1 (mL)","code":""},{"path":"/reference/mimic.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Clinical data from the MIMIC-II database for a case study on indwelling arterial catheters — mimic","text":"https://physionet.org/content/mimic2-iaccd/1.0/","code":""},{"path":"/reference/mimic_preprocessed.html","id":null,"dir":"Reference","previous_headings":"","what":"Preprocessed Clinical Data from the MIMIC-II Database — mimic_preprocessed","title":"Preprocessed Clinical Data from the MIMIC-II Database — mimic_preprocessed","text":"version mimic dataset cleaned removing columns 10% missing data, imputing remaining missing values median, dropping columns highly correlated outcome. designed use fairness-aware machine learning tasks streamlined analysis.","code":""},{"path":"/reference/mimic_preprocessed.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preprocessed Clinical Data from the MIMIC-II Database — mimic_preprocessed","text":"","code":"mimic_preprocessed"},{"path":"/reference/mimic_preprocessed.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Preprocessed Clinical Data from the MIMIC-II Database — mimic_preprocessed","text":"data frame fewer variables original due preprocessing. Number rows: 1776.","code":""},{"path":"/reference/mimic_preprocessed.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Preprocessed Clinical Data from the MIMIC-II Database — mimic_preprocessed","text":"https://physionet.org/content/mimic2-iaccd/1.0/","code":""},{"path":[]}]
