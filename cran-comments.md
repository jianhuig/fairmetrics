## Version 1.0.3

* Changing abbreviations to full names for `eval_*` functions. 
* Updating `get_fairness_metrics` to return two separate dataframes - `performance` and `fairness` for separating model performance metrics from fairness metrics. Re-documented as well. 

## Version 1.0.2

* Allowing for bootstrap CIs to be an optional parameter. 
* Removing `get_all_metrics` - `get_fairness_metrics` covers it with `confint = FALSE`.


## Version 1.0.1

0 errors | 0 warnings | 0 notes

* Renamed `eval_pred_parity` -> `eval_pos_pred_parity`
* Added function for evaluating negative predictive parity (neg_pred_parity)
* Edited linking in documentation.
* Minor edits to code in vignette. 


## Version 1.0.0
0 errors | 0 warnings | 1 note

* This is a new release.
* Addressed submission issues listed in automated response.
* "Gao", "et" and "al" are not typos

