<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Summary • fairmetrics</title><script src="deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"><link href="deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"><script src="deps/headroom-0.11.0/headroom.min.js"></script><script src="deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="deps/search-1.0.0/fuse.min.js"></script><script src="deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="pkgdown.js"></script><meta property="og:title" content="Summary"></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="index.html">fairmetrics</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.0.2</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="nav-item"><a class="nav-link" href="articles/fairmetrics.html">Get started</a></li>
<li class="nav-item"><a class="nav-link" href="reference/index.html">Reference</a></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="search.json"></form></li>
      </ul></div>


  </div>
</nav><div class="container template-title-body">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Summary</h1>

    </div>


<div id="summary" class="section level1">

<p>Fairness is a growing area of machine learning (ML) that focuses on ensuring models do not produce systematically biased outcomes for specific groups, particularly those defined by protected attributes such as race, gender, or age. Evaluating fairness is a critical aspect of ML model development, as biased models can perpetuate structural inequalities. The {fairmetrics} R package offers a user-friendly framework for rigorously evaluating numerous group-based fairness criteria, including metrics based on independence (e.g., statistical parity), separation (e.g., equalized odds), and sufficiency (e.g., predictive parity). Group-based fairness criteria assess whether a model is equally accurate or well-calibrated across a set of predefined groups so that appropriate bias mitigation strategies can be implemented. {fairmetrics} provides both point and interval estimates for multiple metrics through a convenient wrapper function and includes an example dataset derived from the Medical Information Mart for Intensive Care, version II (MIMIC-II) database [@goldberger2000physiobank; @raffa2016clinical].</p>
</div>
<div class="section level1">
<h1 id="statement-of-need">Statement of Need<a class="anchor" aria-label="anchor" href="#statement-of-need"></a></h1>
<p>ML models are increasingly integrated into high-stakes domains to support decision making that significantly impacts individuals and society more broadly, including criminal justice, healthcare, finance, employment, and education [@mehrabi_survey_21]. Mounting evidence suggest that these models often exhibit bias across groups defined by protected attributes. For example, within criminal justice, the Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) software, a tool used by U.S. courts to evaluate the risk of defendants becoming recidivists, was found to incorrectly classify Black defendants as high-risk at nearly twice the rate of white defendants [@mattuMachineBias]. This bias impacted Black defendants by potentially leading to harsher bail decisions, longer sentences, and reduced parole opportunities compared to white defendants with similar risk profiles. Similarly, within healthcare, a commercial risk-prediction algorithm deployed in the U.S. to identify patients with complex health needs for high-risk care management programs was shown to be significantly less calibrated for Black patients relative to white patients [@obermeyerDissectingRacialBias2019]. This caused Black patients with equivalent health conditions to be under-referred for essential care services compared to white patients. These examples illustrate that there is an urgent need for practitioners and researchers to ensure that ML models support fair decision making before they are deployed in real-world applications.</p>
<p>While existing software can compute group fairness criteria, they only provide point estimates and/or visualizations without quantifying the uncertainty around the criteria. This limitation prevents users from determining whether observed disparities between groups are statistically significant or merely the result of random variation due to finite sample size, potentially leading to incorrect conclusions about fairness violations. The {fairmetrics} R package addresses this gap by providing bootstrap-based confidence intervals (CIs) for both difference-based and ratio-based group fairness metrics, empowering users to make statistically grounded decisions about the fairness of their models, which is inconsistently done in practice.</p>
</div>
<div class="section level1">
<h1 id="scope">Scope<a class="anchor" aria-label="anchor" href="#scope"></a></h1>
<p>The <a href="https://jianhuig.github.io/fairmetrics/" class="external-link">fairmetrics</a> package is designed to evaluate group fairness in the setting of binary classification with a binary protected attribute. This restriction reflects standard practice in the fairness literature and is motivated by several considerations. First, binary classification remains prevalent in many high-stakes applications, such as loan approval, hiring decisions, and disease screening, where outcomes are typically framed as accept/reject or positive/negative [@mehrabi_survey_21]. Second, group fairness is the most widely used framework for binary classification tasks [@mehrabi_survey_21]. Third, when protected attributes have more than two categories, there is no clear consensus on how to evaluate group fairness [@lum_debias_22]. This focus enables {fairmetrics} to provide statistically grounded uncertainty quantification for group fairness metrics commonly applied in binary classification tasks across diverse application domains.</p>
</div>
<div class="section level1">
<h1 id="fairness-criteria">Fairness Criteria<a class="anchor" aria-label="anchor" href="#fairness-criteria"></a></h1>
<p>Group fairness criteria are primarily classified into three main categories: independence, separation, and sufficiency [@barocas2023fairness; @Berk_Heidari_Jabbari_Kearns_Roth_2018; @Castelnovo_Crupi_Greco_Regoli_Penco_Cosentini_2022, @Gao_Chou_McCaw_Thurston_Varghese_Hong_Gronsbell_2024]. Independence requires that the model’s classifications be statistically independent of the protected attribute, meaning the likelihood of receiving a positive prediction is the same across protected groups. Separation requires independence between the classifications and the protected attribute conditional on the true outcome, so that the probability of a positive prediction is equal across protected groups within the positive (or negative) outcome class. Sufficiency requires independence between the outcome and the protected attribute conditional on the prediction, implying that once the model’s prediction is known, the protected attribute provides no additional information about the true outcome. Below we summarize the fairness metrics that are available within the {fairmetrics} package.</p>
<div class="section level2">
<h2 id="independence">Independence<a class="anchor" aria-label="anchor" href="#independence"></a></h2>
<ul><li><p><strong>Statistical Parity:</strong> Compares the overall rate of positive predictions between groups.</p></li>
<li><p><strong>Conditional Statistical Parity:</strong> Restricts the comparison of positive prediction rates to a specific subgroup (e.g., within a hospital unit or age bracket), offering a more context-specific fairness assessment.</p></li>
</ul></div>
<div class="section level2">
<h2 id="separation">Separation<a class="anchor" aria-label="anchor" href="#separation"></a></h2>
<ul><li><p><strong>Equal Opportunity:</strong> Compares disparities in the false negative rates between groups, quantifying differences in the likelihood of missing positive outcomes.</p></li>
<li><p><strong>Predictive Equality:</strong> Compares the false positive rates (FPR) between groups, quantifying differences in the likelihood of incorrectly labeling negative outcomes as positive.</p></li>
<li><p><strong>Balance for Positive Class:</strong> Compares the average of the predicted probabilities among individuals whose true outcome is positive across groups.</p></li>
<li><p><strong>Balance for Negative Class:</strong> Compares the average of the predicted probabilities among individuals whose true outcome is negative across groups.</p></li>
</ul></div>
<div class="section level2">
<h2 id="sufficiency">Sufficiency<a class="anchor" aria-label="anchor" href="#sufficiency"></a></h2>
<ul><li><p><strong>Positive Predictive Parity:</strong> Compares the positive predictive values across groups, assessing differences in the precision of positive predictions.</p></li>
<li><p><strong>Negative Predictive Parity:</strong> Compares the negative predictive values across groups, assessing differences in the precision of negative predictions.</p></li>
</ul></div>
<div class="section level2">
<h2 id="other-criteria">Other Criteria<a class="anchor" aria-label="anchor" href="#other-criteria"></a></h2>
<ul><li><p><strong>Brier Score Parity:</strong> Compares the Brier score (i.e., the mean squared error of the predicted probabilities) across groups, evaluating differences in calibration.</p></li>
<li><p><strong>Accuracy Parity:</strong> Compares the overall accuracy of a predictive model across groups.</p></li>
<li><p><strong>Treatment Equality:</strong> Compares the ratio of false negatives to false positives across groups, evaluating whether the trade-off between missed detections of positive outcomes and false alarms of negative outcomes is balanced.</p></li>
</ul></div>
</div>
<div class="section level1">
<h1 id="evaluating-fairness-criteria">Evaluating Fairness Criteria<a class="anchor" aria-label="anchor" href="#evaluating-fairness-criteria"></a></h1>
<p>The input to the {fairmetrics} package is a data frame or tibble containing the model’s predicted probabilities, the true outcomes, and the protected attribute of interest.  shows the workflow for using {fairmetrics}. Users can evaluate a model for a specific criterion or multiple group fairness criteria using the combined metrics function.</p>
<div class="float">
<img src="fairmetrics-workflow.png" alt="Workflow for using {fairmetrics} to evaluate model fairness across multiple criteria. "><div class="figcaption">Workflow for using {fairmetrics} to evaluate model fairness across multiple criteria. </div>
</div>
<p>A simple example of how to use the {fairmetrics} package is illustrated below. The example makes use of the <code>mimic_preprocessed</code> dataset, a pre-processed version of the the Indwelling Arterial Catheter (IAC) Clinical Dataset, from the MIMIC-II clinical database<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content="&lt;p&gt;The raw version of this data is made available by PhysioNet [@goldberger2000physiobank] and can be accessed in the {fairmetrics} package by loading the &lt;code&gt;mimic&lt;/code&gt; dataset.&lt;/p&gt;"><sup>1</sup></a> [@raffa2016clinical; @raffa2016data]. The dataset consists of 1,776 hemodynamically stable patients with respiratory failure and includes demographic information (patient age and gender), vital signs, laboratory results, whether an IAC was used, and a binary outcome indicating whether the patient died within 28 days of hospital admission.</p>
<p>While the choice of fairness criteria used is context dependent, we show all criteria available with the <code><a href="reference/get_fairness_metrics.html">get_fairness_metrics()</a></code> function for the purposes of illustration. In this example, we evaluate the model’s fairness with respect to the protected attribute <code>gender</code>. For conditional statistical parity, we condition on patients older than 60 years old. The model is trained on a subset of the data and the predictions are made and evaluated on a test set. The <code><a href="reference/get_fairness_metrics.html">get_fairness_metrics()</a></code> function outputs difference and ratio-based metrics as well as their corresponding confidence intervals. A statistically significant difference across groups at a given level of significance is indicated when the confidence interval for a difference-based metric does not include zero or when the interval for a ratio-based metric does not include one.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://jianhuig.github.io/fairmetrics/" class="external-link">fairmetrics</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://dplyr.tidyverse.org" class="external-link">dplyr</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://magrittr.tidyverse.org" class="external-link">magrittr</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://www.stat.berkeley.edu/~breiman/RandomForests/" class="external-link">randomForest</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Load the example dataset</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="st">"mimic_preprocessed"</span><span class="op">)</span>  </span>
<span></span>
<span><span class="co"># Split the data into training and test sets</span></span>
<span><span class="va">train_data</span> <span class="op">&lt;-</span> <span class="va">mimic_preprocessed</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html" class="external-link">%&gt;%</a></span></span>
<span>  <span class="fu">dplyr</span><span class="fu">::</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html" class="external-link">filter</a></span><span class="op">(</span><span class="fu">dplyr</span><span class="fu">::</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/row_number.html" class="external-link">row_number</a></span><span class="op">(</span><span class="op">)</span> <span class="op">&lt;=</span> <span class="fl">700</span><span class="op">)</span></span>
<span></span>
<span><span class="va">test_data</span> <span class="op">&lt;-</span> <span class="va">mimic_preprocessed</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html" class="external-link">%&gt;%</a></span></span>
<span>  <span class="fu">dplyr</span><span class="fu">::</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html" class="external-link">mutate</a></span><span class="op">(</span>gender <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html" class="external-link">ifelse</a></span><span class="op">(</span><span class="va">gender_num</span> <span class="op">==</span> <span class="fl">1</span>, <span class="st">"Male"</span>, <span class="st">"Female"</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html" class="external-link">%&gt;%</a></span></span>
<span>  <span class="fu">dplyr</span><span class="fu">::</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html" class="external-link">filter</a></span><span class="op">(</span><span class="fu">dplyr</span><span class="fu">::</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/row_number.html" class="external-link">row_number</a></span><span class="op">(</span><span class="op">)</span> <span class="op">&gt;</span> <span class="fl">700</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Train a random forest model</span></span>
<span><span class="va">rf_model</span> <span class="op">&lt;-</span> <span class="fu">randomForest</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/randomForest/man/randomForest.html" class="external-link">randomForest</a></span><span class="op">(</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/factor.html" class="external-link">factor</a></span><span class="op">(</span><span class="va">day_28_flg</span><span class="op">)</span> <span class="op">~</span> <span class="va">.</span>, </span>
<span>  data <span class="op">=</span> <span class="va">train_data</span>, </span>
<span>  ntree <span class="op">=</span> <span class="fl">1000</span></span>
<span>  <span class="op">)</span></span>
<span>  </span>
<span><span class="co"># Make predictions on the test set</span></span>
<span><span class="va">test_data</span><span class="op">$</span><span class="va">pred</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">rf_model</span>, newdata <span class="op">=</span> <span class="va">test_data</span>, type <span class="op">=</span> <span class="st">"prob"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Get fairness metrics</span></span>
<span><span class="co"># Setting alpha=0.05 for 95% confidence intervals</span></span>
<span><span class="fu"><a href="reference/get_fairness_metrics.html">get_fairness_metrics</a></span><span class="op">(</span></span>
<span> data <span class="op">=</span> <span class="va">test_data</span>,</span>
<span> outcome <span class="op">=</span> <span class="st">"day_28_flg"</span>,</span>
<span> group <span class="op">=</span> <span class="st">"gender"</span>,</span>
<span> group2 <span class="op">=</span> <span class="st">"age"</span>,</span>
<span> condition <span class="op">=</span> <span class="st">"&gt;=60"</span>,</span>
<span> probs <span class="op">=</span> <span class="st">"pred"</span>,</span>
<span> cutoff <span class="op">=</span> <span class="fl">0.41</span>, </span>
<span> alpha <span class="op">=</span> <span class="fl">0.05</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co">#&gt;                  Metric                          Full Metric Name GroupFemale GroupMale Difference    95% Diff CI Ratio 95% Ratio CI</span></span>
<span><span class="co">#&gt; 1                   PPR                        Statistical Parity        1.12      1.06       0.06    [0.02, 0.1]  1.06 [1.02, 1.09]</span></span>
<span><span class="co">#&gt; 2                   PPR Conditional Statistical Parity (age &gt;=60)        1.23      1.16       0.07      [0, 0.14]  1.06    [1, 1.13]</span></span>
<span><span class="co">#&gt; 3                   FNR                         Equal Opportunity       -0.35     -0.23      -0.12  [-0.26, 0.02]  1.52  [0.9, 2.58]</span></span>
<span><span class="co">#&gt; 4                   FPR                       Predictive Equality        1.07      1.03       0.04   [0.01, 0.07]  1.04 [1.01, 1.07]</span></span>
<span><span class="co">#&gt; 5  Avg. Predicted Prob.                Balance for Positive Class        0.50      0.50       0.00         [0, 0]  1.00       [1, 1]</span></span>
<span><span class="co">#&gt; 6  Avg. Predicted Prob.                Balance for Negative Class        0.50      0.50       0.00         [0, 0]  1.00       [1, 1]</span></span>
<span><span class="co">#&gt; 7                   PPV                Positive Predictive Parity        0.21      0.16       0.05       [0, 0.1]  1.31 [0.99, 1.74]</span></span>
<span><span class="co">#&gt; 8                   NPV                Negative Predictive Parity        0.87      0.88      -0.01  [-0.06, 0.04]  0.99 [0.74, 1.32]</span></span>
<span><span class="co">#&gt; 9           Brier Score                        Brier Score Parity        0.37      0.40      -0.03 [-0.04, -0.02]  0.92 [0.89, 0.96]</span></span>
<span><span class="co">#&gt; 10             Accuracy                   Overall Accuracy Parity        1.00      1.01      -0.01  [-0.05, 0.03]  0.99 [0.95, 1.03]</span></span>
<span><span class="co">#&gt; 11          FN/FP Ratio                        Treatment Equality        0.13      0.12       0.01  [-0.04, 0.06]  1.08  [0.73, 1.6]</span></span></code></pre></div>
<p>Should the user wish to calculate an individual criteria, it is possible to use any of the <code>eval_*</code> functions. For example, to calculate equal opportunity, the user can call the <code>eval_equal_opportunity()</code> function.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="reference/eval_eq_opp.html">eval_eq_opp</a></span><span class="op">(</span></span>
<span>  data <span class="op">=</span> <span class="va">test_data</span>,</span>
<span>  outcome <span class="op">=</span> <span class="st">"day_28_flg"</span>,</span>
<span>  group <span class="op">=</span> <span class="st">"gender"</span>,</span>
<span>  probs <span class="op">=</span> <span class="st">"pred"</span>,</span>
<span>  confint <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  cutoff <span class="op">=</span> <span class="fl">0.41</span>,</span>
<span>  alpha <span class="op">=</span> <span class="fl">0.05</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co">#&gt;There is evidence that model does not satisfy equal opportunity.</span></span>
<span><span class="co">#&gt;  Metric GroupFemale GroupMale Difference    95% Diff CI Ratio 95% Ratio CI</span></span>
<span><span class="co">#&gt;1    FNR        0.38      0.62      -0.24 [-0.39, -0.09]  0.61 [0.44, 0.85]</span></span></code></pre></div>
</div>
<div class="section level1">
<h1 id="related-work">Related Work<a class="anchor" aria-label="anchor" href="#related-work"></a></h1>
<p>Other R packages similar to {fairmetrics} include {fairness} [@fairness_package], {fairmodels} [@wisniewski2022fairmodels] and {mlr3fairness} [@mlr3fairness_package]. The differences between {fairmetrics} and these other packages is twofold. The primary difference between is that {fairmetrics} calculates the ratio and difference between group fairness criterion and their corresponding confidence intervals of fairness metrics via bootstrap, allowing for more meaningful inferences about the fairness criteria. Additionally, in contrast to the {fairmodels}, {fairness} and {mlr3fairness} packages, the {fairmetrics} package does not posses any external dependencies and has a lower memory footprint, resulting in an environment agnostic tool that can be used with modest hardware and older systems.  shows the comparison of memory used and dependencies required when loading each library.</p>
<p>For python users, the {fairlearn} library [@fairlearn_paper] provides additional fairness metrics and algorithms. The {fairmetrics} package is designed for seemless integration with R workflows, making it a more convenient choice for R-based ML applications.</p>
</div>
<div class="section level1">
<h1 id="licensing-and-availability">Licensing and Availability<a class="anchor" aria-label="anchor" href="#licensing-and-availability"></a></h1>
<p>The {fairmetrics} package is under the MIT license. It is available on CRAN and can be installed by using <code>install.packages("fairmetrics")</code>. A more in-depth tutorial can be accessed at: <a href="https://jianhuig.github.io/fairmetrics/articles/fairmetrics.html" class="external-link uri">https://jianhuig.github.io/fairmetrics/articles/fairmetrics.html</a>. All code is open-source and hosted on GitHub. All bugs and inquiries can be reported at <a href="https://github.com/jianhuig/fairmetrics/issues/" class="external-link uri">https://github.com/jianhuig/fairmetrics/issues/</a>.</p>
</div>
<div class="section level1">
<h1 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a></h1>
</div>



  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Jianhui Gao, Benjamin Smith, Benson Chou, Jessica Gronsbell.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer></div>





  </body></html>

